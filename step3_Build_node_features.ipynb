{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import sys, os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from apex import amp\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# from Classes import Node, Adjacency_sp, Question_Paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question_Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Question_Paragraph(object):\n",
    "    '''Q-P pair and label. for BERT and node init.\n",
    "    返回q-p对和q-s对, 还要确保能够初始化Node类.\n",
    "    每个问句有10个paragraph,即10个此类.'''\n",
    "    def __init__(self, ques_id, para_id, question_tokens, para_title_tokens, para_label, sents_in_para, sentences_label):\n",
    "        self.question_tokens = question_tokens\n",
    "        self.para_title_tokens = para_title_tokens\n",
    "        self.sents_in_para = sents_in_para\n",
    "        self.sentences_offsets = self.cal_offsets(self.sents_in_para)\n",
    "        self.sentences_label = sentences_label\n",
    "\n",
    "        self.ques_id = ques_id\n",
    "        self.para_id = para_id \n",
    "        self.para_label = para_label # 段落label\n",
    "\n",
    "        self.question_features = None # [N, dim]\n",
    "        self.para_features = None\n",
    "#         self.paragraph_features = None\n",
    "\n",
    "    @classmethod\n",
    "    def build(cls, ques_id, para_id, question, para_title_tokens, para_label, node_list):\n",
    "        Snodes = [n for n in node_list if n.paragraph_id == para_id and n.node_type == 'Sentence']\n",
    "        question_tokens = tokensizer_in_Model(question)\n",
    "        # para_title_tokens = tokensizer_in_Model(para_title)\n",
    "        sents_in_para = [n.content_tokens for n in Snodes]\n",
    "        sentences_label = [int(n.is_support) for n in Snodes]\n",
    "        return cls(ques_id, para_id, question_tokens, para_title_tokens, para_label, sents_in_para, sentences_label)\n",
    "\n",
    "    @staticmethod\n",
    "    def cal_offsets(sents_list):\n",
    "        cursor = 0\n",
    "        offsets = []\n",
    "        for sent_tokens in sents_list:\n",
    "            len_sent = len(sent_tokens)\n",
    "            offsets.append((cursor, cursor+len_sent))\n",
    "            cursor += len_sent\n",
    "        return offsets\n",
    "    \n",
    "    # content tokens\n",
    "    def get_para_tokens(self, contain_title = False):\n",
    "        para_token = self.para_title_tokens if contain_title else []\n",
    "        for i in self.sents_in_para: para_token.extend(i)\n",
    "        return para_token\n",
    "\n",
    "    def get_ques_para_label_tuple(self, contain_title = False):\n",
    "        '''问句-段落对'''\n",
    "        return (self.question_tokens, self.get_para_tokens(contain_title), self.para_label)\n",
    "\n",
    "    def get_ques_sent_label_list(self, contain_title = False):\n",
    "        '''问句-句子对'''\n",
    "        if contain_title:\n",
    "            return [(self.question_tokens, self.para_title_tokens.extend(sent_tokens), sent_label)\\\n",
    "                for sent_tokens,sent_label in zip(self.sents_in_para, self.sentences_label)]\n",
    "        else:\n",
    "            return [(self.question_tokens, sent_tokens, sent_label)\\\n",
    "                for sent_tokens,sent_label in zip(self.sents_in_para, self.sentences_label)]\n",
    "\n",
    "    def format_sents_in_para(self):\n",
    "        return ' '.join([f'{index}:{word}' for index,word in enumerate(self.sents_in_para)])\n",
    "\n",
    "    # features\n",
    "    def build_features(self):\n",
    "        '''build features from LM models'''\n",
    "    \n",
    "        self.question_features = get_feature_from_model(self.question_tokens)\n",
    "\n",
    "        para_features = 0\n",
    "        for one_line in self.get_ques_sent_label_list():\n",
    "\n",
    "            sent_features = get_feature_from_model(one_line[0], one_line[1], 'second')\n",
    "\n",
    "            if type(para_features) == int: para_features = sent_features.clone()\n",
    "            else: para_features = torch.cat((para_features, sent_features), dim=0)\n",
    "       \n",
    "        self.para_features = para_features\n",
    "        \n",
    "\n",
    "    def get_question_features(self):\n",
    "        return self.question_features\n",
    "\n",
    "    def get_paragraph_features(self):\n",
    "        return self.para_features\n",
    "\n",
    "    # other\n",
    "    def __str__(self):\n",
    "        return f'Q_P. p_id: {self.para_id}'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Q_P. p_id: {self.para_id}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8wYrwSgH7lFr"
   },
   "source": [
    "# class Node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     28,
     39,
     42,
     48,
     52,
     55,
     58
    ],
    "colab": {},
    "colab_type": "code",
    "id": "6lavnppTZ4gw"
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    '''Node class for graph'''\n",
    "    \n",
    "    def __init__(self, node_id, node_type, content_raw, content_tokens, \\\n",
    "                 content_NER_list, parent_id, content_features=None, is_support=False):\n",
    "        self.node_id = node_id\n",
    "        self.node_type = node_type\n",
    "        self.content_raw = content_raw\n",
    "        self.content_tokens = content_tokens\n",
    "        self.content_NER_list = content_NER_list\n",
    "        self.parent_id = parent_id\n",
    "        \n",
    "        # Q_node doesn't have.\n",
    "        self.paragraph_id = -1\n",
    "        self.start_in_paragraph = -1\n",
    "        self.end_in_paragraph = -1\n",
    "\n",
    "        # only for E_node\n",
    "        # E节点能够通过parent_id找到S节点.\n",
    "        self.start_in_sentence = -1\n",
    "        self.end_in_sentence = -1\n",
    "\n",
    "        self.is_support = is_support # 段落 句子 \n",
    "\n",
    "        self.content_features = content_features\n",
    "        self.cls_feature = None # final features. [1,dim]\n",
    "        \n",
    "    @classmethod\n",
    "    def build(cls, node_id, node_type, content_raw, parent_id, content_tokens=None):\n",
    "        '''content_tokens能加快计算速度.'''\n",
    "        # content_tokens = tokensize_and_repr_in_BERT(content_raw, flatten=True)\n",
    "        if node_type != 'Entity':\n",
    "            content_tokens_NOCLS, content_NER_list = find_NER_in_Model(content_raw, content_tokens)\n",
    "        else:\n",
    "            content_tokens_NOCLS = content_tokens\n",
    "            content_NER_list = None\n",
    "        # print(f'id:{node_id}\\n{content_raw}\\n{content_NER_list}\\n')\n",
    "        return cls(node_id, node_type, content_raw, content_tokens_NOCLS, content_NER_list, parent_id)\n",
    "    \n",
    "    def set_support(self):\n",
    "        self.is_support = True\n",
    "    \n",
    "    def set_span_in_paragraph(self, para_id, start):\n",
    "        self.paragraph_id = para_id\n",
    "        self.start_in_paragraph = start\n",
    "        self.end_in_paragraph = start + len(self.content_tokens)\n",
    "\n",
    "    # only for E_node.\n",
    "    def set_span_in_sentence(self, start):\n",
    "        self.start_in_sentence = start\n",
    "        self.end_in_sentence = start + len(self.content_tokens)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'Node: {self.node_type} {self.node_id}'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Node: {self.node_type} {self.node_id}'\n",
    "\n",
    "    def get_NER_tuples_list(self):\n",
    "        '''返回NER元组. e.g. [('ALLPE',id), ('DELL',id)]'''\n",
    "        return [(i['content'], self.node_id) for i in self.content_NER_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "agC59TUHeqe1"
   },
   "source": [
    "# class Adjacency_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     3
    ],
    "colab": {},
    "colab_type": "code",
    "id": "uycudvpHZ4ld"
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "class Adjacency_sp(object):\n",
    "    '''无重复稀疏邻接矩阵'''\n",
    "    def __init__(self):\n",
    "        self.v_i_j = []\n",
    "        self.i_j_find_table = []\n",
    "\n",
    "    def append(self, v, i, j):\n",
    "        if not (i,j) in self.i_j_find_table:\n",
    "            self.v_i_j.append([v,i,j])\n",
    "            self.i_j_find_table.append((i,j))\n",
    "    \n",
    "    def to_dense(self):\n",
    "        '''return numpy ndarray.'''\n",
    "        _len = max([i[0] for i in self.i_j_find_table] + [i[1] for i in self.i_j_find_table]) + 1\n",
    "        shape = (_len,_len)\n",
    "        np_adj = np.array(self.v_i_j)\n",
    "        full_adj = sp.coo_matrix((np_adj[:, 0], (np_adj[:, 1], np_adj[:, 2])), shape=shape, dtype=np.float32).todense()\n",
    "        full_adj = np.array(full_adj)\n",
    "        return full_adj\n",
    "\n",
    "    def to_dense_symmetric(self):\n",
    "        _len = max([i[0] for i in self.i_j_find_table] + [i[1] for i in self.i_j_find_table]) + 1\n",
    "        shape = (_len,_len)\n",
    "        np_adj = np.array(self.v_i_j)\n",
    "        adj = sp.coo_matrix((np_adj[:, 0], (np_adj[:, 1], np_adj[:, 2])), shape=shape, dtype=np.float32)\n",
    "        adj_symm = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj).todense()\n",
    "        adj_symm = np.array(adj_symm)\n",
    "        return adj_symm\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Adjacency_sp has {len(self.v_i_j)} edges'\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    def __len__(self):\n",
    "        return len(self.v_i_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    # drive.mount('/content/folders/')\n",
    "    json_train_path = '/content/folders/My Drive/HotpotQA/hotpot_train_v1.1.json'\n",
    "    save_cache_path = '/content/folders/My Drive/save_cache'\n",
    "    HotpotQA_path = '/content/folders/My Drive/HotpotQA'\n",
    "except:\n",
    "    json_train_path = 'data/hotpot_train_v1.1.json'\n",
    "    save_cache_path = 'save_cache/'\n",
    "    HotpotQA_path = './'\n",
    "\n",
    "sys.path.insert(0,HotpotQA_path)\n",
    "\n",
    "args = Namespace(\n",
    "    # Data and path information\n",
    "    json_train_path=json_train_path,\n",
    "    model_state_file = \"model_HotpotQA.pth\",\n",
    "    save_dir = save_cache_path,\n",
    "    HotpotQA_preprocess_file = 'hotpotQA_train_preprocess100.pkl',\n",
    "\n",
    "    # Model hyper parameter\n",
    "    use_proxy = False,\n",
    "    proxies={\"http_proxy\": \"127.0.0.1:10802\",\n",
    "             \"https_proxy\": \"127.0.0.1:10802\"},\n",
    "    tokenizer_type = \"bert-large-cased-whole-word-masking\",\n",
    "    model_type = 'bert-base-uncased',\n",
    "\n",
    "    # Dataset parameter\n",
    "    max_seq = 512,\n",
    "    pad_to_max = True,\n",
    "\n",
    "    # Training hyper parameter\n",
    "    num_epochs=2,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=64,\n",
    "    seed=1337,\n",
    "    early_stopping_criteria=5,\n",
    "    freeze_layer_name = 'bert.encoder.layer.10',\n",
    "\n",
    "    # Runtime hyper parameter\n",
    "    cuda=True,\n",
    "    device=None,\n",
    "    tpu=False,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    ")\n",
    "\n",
    "proxies = args.proxies if args.use_proxy else None\n",
    "\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.model_state_file = os.path.join(args.save_dir,args.model_state_file)\n",
    "    args.HotpotQA_preprocess_file = os.path.join(args.save_dir,args.HotpotQA_preprocess_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方法1: 使用BERT+offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_type,proxies=proxies)\n",
    "\n",
    "config = AutoConfig.from_pretrained(args.model_type)\n",
    "config.output_hidden_states = True\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_config(config)\n",
    "model.load_state_dict(torch.load(args.model_state_file))\n",
    "_ = model.eval()\n",
    "\n",
    "if args.cuda: model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.output_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 函数封装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_feature_from_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辅助函数\n",
    "def get_feature_from_model(text, text_pair=None, return_type = 'first', \\\n",
    "                           max_length=512, tokenizer=tokenizer, LM_model=model):\n",
    "    '''输入[seq_len], 返回[seq_len, dim]'''\n",
    "    assert return_type in ['first','second']\n",
    "    assert LM_model.config.output_hidden_states == True\n",
    "    \n",
    "    if text_pair == None: return_type = 'first'\n",
    "    \n",
    "    model_input = tokenizer.encode_plus(\n",
    "            text = text,\n",
    "            text_pair = text_pair,\n",
    "            add_special_tokens = True,\n",
    "            max_length = max_length,\n",
    "            truncation_strategy = 'only_second',\n",
    "            pad_to_max_length = False,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "    \n",
    "    if args.cuda:\n",
    "        model_input = {k:v.to(args.device) for k,v in model_input.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        res_tuple=LM_model(**model_input)\n",
    "        first_part_len = model_input['token_type_ids'].flatten().tolist().count(0)\n",
    "    \n",
    "    # last hidden layer.\n",
    "    seq_hidden = res_tuple[1][-1].squeeze().to('cpu')\n",
    "    \n",
    "    if text_pair == None or return_type == 'first': \n",
    "        return seq_hidden[1:-1]\n",
    "    else:\n",
    "        # first_part_len-1 is [SEP]\n",
    "        return seq_hidden[first_part_len:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_feature_from_model('hello i am dog').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_feature_from_model('hello i dog','what are you doing ?', 'second').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 19082,   178,  3676,   102,  1184,  1132,  1128,  1833,   136,\n",
       "            102]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = tokenizer.encode_plus('hello i dog','what are you doing ?',return_tensors='pt')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 19082,   117,   178,  1821, 23220,  1306,   102]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = tokenizer.encode_plus('hello, i am jim', return_tensors='pt')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['token_type_ids'].flatten().tolist().count(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给定LM_model, tokenizer, 对Q-P list先计算feature. \n",
    "# 然后初始化 Q P-->S-->\n",
    "lines = [\n",
    "    [['hello','i','am','Jim'],['hello','i','am','Jim','dog']],\n",
    "    [['hello','i','am','Jim'],['i','am','Jim','dog']],\n",
    "    [['hello','i','am','Jim'],['hello','i','am','Jim','dog','very','old']],\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def build_features(tokenizer, LM_model):\n",
    "    \n",
    "    question_features = get_feature_from_model(lines[0][0],\n",
    "                       tokenizer=tokenizer, \n",
    "                       LM_model=LM_model)\n",
    "    \n",
    "    para_features = 0\n",
    "    for one_line in lines:\n",
    "\n",
    "        sent_features = get_feature_from_model(one_line[0], one_line[1], 'second',\n",
    "                       tokenizer=tokenizer, \n",
    "                       LM_model=LM_model)\n",
    "        \n",
    "        if type(para_features) == int: para_features = sent_features.clone()\n",
    "        else: para_features = torch.cat((para_features, sent_features), dim=0)\n",
    "        \n",
    "#     return\n",
    "    print(question_features.shape)\n",
    "    print(para_features.shape)\n",
    "    \n",
    "build_features(tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.HotpotQA_preprocess_file,'rb')as fp:\n",
    "    hotpotQA_train_preprocess = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'node_list', 'sp_adj', 'ques_para_list'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotpotQA_train_preprocess[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_para_list = hotpotQA_train_preprocess[0]['ques_para_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_para_list[0].build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([167, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques_para_list[0].get_paragraph_features().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6e4393f64845869dda05e0b0eda9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ques_item in tqdm_notebook(hotpotQA_train_preprocess):\n",
    "    ques_para_list = ques_item['ques_para_list']\n",
    "    for q_p_item in ques_para_list:\n",
    "        q_p_item.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([92, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotpotQA_train_preprocess[7]['ques_para_list'][6].get_paragraph_features().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获得features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list = hotpotQA_train_preprocess[0]['node_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ques_item in tqdm_notebook(hotpotQA_train_preprocess):\n",
    "    node_list = ques_item['node_list']\n",
    "    ques_para_list = ques_item['ques_para_list']\n",
    "\n",
    "    # Q node\n",
    "    Q_node = node_list[0]\n",
    "    Q_node.content_features = ques_para_list[0].get_question_features()\n",
    "\n",
    "    # P node\n",
    "    for P_node in [i for i in node_list if i.node_type == 'Paragraph']:\n",
    "        P_node.content_features = [q for q in ques_para_list \\\n",
    "                                   if P_node.paragraph_id == q.para_id][0].get_paragraph_features()\n",
    "\n",
    "        P_node.content_tokens = [q for q in ques_para_list \\\n",
    "                                   if P_node.paragraph_id == q.para_id][0].get_para_tokens()\n",
    "        \n",
    "\n",
    "    # S node\n",
    "    for S_node in [i for i in node_list if i.node_type == 'Sentence']:\n",
    "        start = S_node.start_in_paragraph\n",
    "        end = S_node.end_in_paragraph\n",
    "        S_node.content_features = node_list[S_node.parent_id].content_features[start:end]\n",
    "        \n",
    "        # 验证OK\n",
    "#         print(S_node.content_tokens)\n",
    "#         print(start, end)\n",
    "#         print(node_list[S_node.parent_id].content_tokens[start:end])\n",
    "#         print('')\n",
    "\n",
    "    # E node\n",
    "    for E_node in [i for i in node_list if i.node_type == 'Entity']:\n",
    "        start = E_node.start_in_sentence\n",
    "        end = E_node.end_in_sentence\n",
    "        E_node.content_features = node_list[E_node.parent_id].content_features[start:end]        \n",
    "        \n",
    "        # 验证OK\n",
    "#         print(E_node.content_tokens)\n",
    "#         print(start, end)\n",
    "#         print(node_list[E_node.parent_id].content_tokens[start:end])\n",
    "#         print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotpotQA_train_preprocess[0]['node_list'][0].content_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([167, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotpotQA_train_preprocess[0]['node_list'][1].content_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotpotQA_train_preprocess[6]['node_list'][7].content_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立features矩阵\n",
    "\n",
    "**目标**: 获取`torch.Size([10, 768])`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用GRU获取\n",
    "\n",
    "使用GRU将变长序列缩减为[1,dim],需要考虑padding导致的效率问题. 最好使用`torch.nn.utils.rnn.pack_padded_sequence()`进行加速计算.\n",
    "\n",
    "难点在于,使用上述包要求输入的可变长度矩阵排列为上三角矩阵, 这要求同时改变邻接矩阵.\n",
    "\n",
    "**缺点**: 句子长度依然有600+词, 使用GRU依然存在梯度问题.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 5, 2, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens = [\n",
    "    [1,2,3],\n",
    "    [1],\n",
    "    [9,2,3,4,5],\n",
    "    [1,2],\n",
    "    [100,2,3],\n",
    "]\n",
    "test_adj=[\n",
    "    [1,0,1,0,0],\n",
    "    [0,1,1,1,0],\n",
    "    [1,1,1,0,0],\n",
    "    [0,1,0,1,1],\n",
    "    [0,0,0,1,1],\n",
    "]\n",
    "np_test_len2 = np.array([len(i) for i in test_tokens])\n",
    "np_adj = np.array(test_adj)\n",
    "\n",
    "np_test_len2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 0, 4, 2]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_argsort = np_test_len2.argsort(axis=0).tolist()\n",
    "index_argsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 1, 1],\n",
       "       [0, 1, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def sort_features_matrix(batch_)\n",
    "\n",
    "test_adj_sort = np.zeros_like(test_adj)\n",
    "for index_from,index_to in zip(range(len(np_test_len2)), index_argsort):\n",
    "    test_adj_sort[index_to,:] = np_adj[index_from,:]\n",
    "    test_adj_sort[:,index_to] = np_adj[:,index_from]\n",
    "test_adj_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([126, 325, 438, 188, 338])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模拟变长序列.\n",
    "seq_max = 512\n",
    "\n",
    "x_test = [torch.randn((np.random.randint(10,seq_max),768)) for i in range(5)]\n",
    "np_test_len = np.array([i.size(-2) for i in x_test])\n",
    "\n",
    "np_test_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_adj=[\n",
    "    [1,0,1,0,0],\n",
    "    [0,1,1,1,0],\n",
    "    [1,1,1,0,0],\n",
    "    [0,1,0,1,1],\n",
    "    [0,0,0,1,1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 1, 4, 2]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_argsort = np_test_len.argsort(axis=0).tolist()\n",
    "index_argsort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 封装adj 排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_features_adj(bsz_seq_dim, adj):\n",
    "    test_adj_sort = np.zeros_like(test_adj)\n",
    "    for index_from,index_to in zip(range(len(x_test)), index_argsort):\n",
    "        test_adj_sort[index_to,:] = np_adj[index_from,:]\n",
    "        test_adj_sort[:,index_to] = np_adj[:,index_from]\n",
    "    test_adj_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用线性层\n",
    "\n",
    "使用线性层也要将sentence进行padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟变长序列.\n",
    "seq_max = 1024\n",
    "x_test_padding = []\n",
    "x_test_mask = []\n",
    "batch_seq_dim = [torch.randn((1,np.random.randint(0,seq_max),768)) for i in range(10)]\n",
    "for t in batch_seq_dim:\n",
    "    # 左边不填, 右边填满0.\n",
    "    tokens_len = t.shape[-2]\n",
    "    x_test_mask.append([1]*tokens_len + [0]*(seq_max - tokens_len))\n",
    "    pd = (0,0,0, seq_max - tokens_len)\n",
    "    x_test_padding.append(F.pad(t, pd ,\"constant\", 0))\n",
    "\n",
    "x_test = torch.cat(x_test_padding, dim=0)\n",
    "x_test_mask = torch.tensor(x_test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 1024])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0].transpose(-1,-2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_mask[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 封装\n",
    "def build_padding(batch_seq_dim, seq_max = 19):\n",
    "    x_padding = []\n",
    "    x_mask = []\n",
    "    for t in batch_seq_dim:\n",
    "        tokens_len = t.shape[-2]\n",
    "        x_mask.append([1]*tokens_len + [0]*(seq_max - tokens_len))\n",
    "        pd = (0,0,0, seq_max - tokens_len)\n",
    "        x_padding.append(F.pad(t, pd ,\"constant\", 0))\n",
    "\n",
    "    x_padding = torch.cat(x_padding, dim=0)\n",
    "    x_mask = torch.tensor(x_mask)\n",
    "    \n",
    "    return x_padding, x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 768, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(x_test.transpose(-1,-2), torch.randn(seq_max,1).unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_MLP(nn.Module):\n",
    "    def __init__(self, seq_max, input_dim):\n",
    "        super(Simple_MLP, self).__init__()\n",
    "        \n",
    "        self.reducer = nn.Parameter(torch.randn([seq_max,1]))\n",
    "        \n",
    "    def forward(self, x): # [B,N,D]\n",
    "        out = torch.matmul(x.transpose(-1,-2), self.reducer.unsqueeze(0)).squeeze() # [10, 768, 1]        \n",
    "        return out\n",
    "    \n",
    "simple_mlp = Simple_MLP(seq_max,768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 768])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_mlp(x_test).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 直接使用max mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟变长序列.\n",
    "seq_max = 512\n",
    "x_test_padding = []\n",
    "for t in [torch.randn((1,np.random.randint(0,seq_max),768)) for i in range(10)]:\n",
    "    # 左边不填, 右边填满0.\n",
    "    pd = (0,0,0, seq_max - t.shape[-2])\n",
    "    x_test_padding.append(F.pad(t, pd ,\"constant\", 0))\n",
    "x_test = torch.cat(x_test_padding, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(x_test, dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 768])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, indices = torch.max(x_test, dim=1)\n",
    "values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用transformer\n",
    "\n",
    "获取transformer中`[CLS]`的表达."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "my_config = AutoConfig.from_pretrained(model_name)\n",
    "model_bert = AutoModel.from_config(my_config)\n",
    "_ = model_bert.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = [\n",
    "    [1,2,3,4,5,6],\n",
    "    [6,7,8,9,0,0],\n",
    "    [12,14,16,0,0,0]\n",
    "]\n",
    "x_test = torch.tensor(x_test, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state , pooler_output  = model_bert(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooler_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方法2: 使用CLS\n",
    "\n",
    "1. 使用`question-sentence features`的`[CLS]`作为**S节点**的初始化, 同时保存`hidden status`.\n",
    "2. 使用`question-sentence CLS`的`[CLS]`作为**P节点**的初始化.\n",
    "3. 使用**E节点**的offet获取S节点对应部分的`hidden status`, 使用MLP初始化.\n",
    "4. 使用`question-P node CLS`的`[CLS]`作为**Q节点**的初始化\n",
    "\n",
    "**注意** 此方法无需`class Question_Paragraph`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "HotpotQA_preprocess_file = 'save_cache/hotpotQA_train_preprocess100_new.pkl'\n",
    "with open(HotpotQA_preprocess_file,'rb')as fp:\n",
    "    hotpotQA_train_preprocess = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "AutoModel.from_pretrained('xlnet-large-cased',proxies=proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlnet-base-cased'\n",
    "proxies={\"http_proxy\": \"127.0.0.1:10802\",\n",
    "         \"https_proxy\": \"127.0.0.1:10802\"}\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name,proxies=proxies)\n",
    "\n",
    "\n",
    "tokenizer_XLNET = AutoTokenizer.from_pretrained(model_name,proxies=proxies)\n",
    "model_XLNET = AutoModel.from_config(config)\n",
    "DEVICE = 'cuda:1'\n",
    "# DEVICE = 'cpu'\n",
    "_ = model_XLNET.to(DEVICE)\n",
    "_ = model_XLNET.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 冻结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def freeze_to_layer(model, layer_name):\n",
    "    '''冻结层. 从0到layer_name.'''\n",
    "    \n",
    "    if layer_name == 'all':\n",
    "        index_start = len(model.state_dict())\n",
    "    else:\n",
    "        index_start = -1\n",
    "        for index, (key, _value) in enumerate(model.state_dict().items()):\n",
    "            if key.startswith(layer_name): \n",
    "                index_start = index\n",
    "                break\n",
    "\n",
    "    if index_start < 0:\n",
    "        print(f\"Don't find layer name: {layer_name}\")\n",
    "        return\n",
    "    \n",
    "    no_grad_nums = index_start + 1\n",
    "    grad_nums = 0\n",
    "\n",
    "    for index, i in enumerate(model.parameters()):\n",
    "        if index >= index_start:\n",
    "            i.requires_grad = True\n",
    "            grad_nums += 1\n",
    "        else:\n",
    "            i.requires_grad = False\n",
    "    \n",
    "    print(f\"freeze layers num: {no_grad_nums}, active layers num: {grad_nums}.\")\n",
    "    # no need to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freeze layers num: 207, active layers num: 0.\n"
     ]
    }
   ],
   "source": [
    "freeze_to_layer(model_XLNET, 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获得特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_features_from_XLNET(text,text_pair=None,\n",
    "                            tokenizer = None,\n",
    "                            model = None,\n",
    "                            add_special_tokens = True,\n",
    "                           device = 'cuda'):\n",
    "    '''XLNET在512张TPU v3上训练5.5天得到. 一张TPU 8核心 128GB内存.'''\n",
    "    \n",
    "    assert model\n",
    "    model_input = tokenizer_XLNET.encode_plus(text,text_pair,\n",
    "                                        add_special_tokens=add_special_tokens,\n",
    "                                        return_tensors='pt')\n",
    "    \n",
    "    model_input = {k:v.to(device) for k,v in model_input.items()}\n",
    "    \n",
    "    # 不能在函数里面设置device.\n",
    "    # model.to(device)\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**model_input)[0]\n",
    "    \n",
    "    return last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list = hotpotQA_train_preprocess[0]['node_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_adj = hotpotQA_train_preprocess[0]['sp_adj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 4., ..., 0., 0., 0.],\n",
       "       [0., 4., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 7., 0.],\n",
       "       [0., 0., 0., ..., 7., 0., 7.],\n",
       "       [0., 0., 0., ..., 0., 7., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_adj.to_dense_symmetric()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66096d01120548409064cafa4923bc56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='building features', style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ques_item in tqdm_notebook(hotpotQA_train_preprocess, desc = 'building features'):\n",
    "    node_list = ques_item['node_list']\n",
    "#     ques_para_list = ques_item['ques_para_list']\n",
    "\n",
    "    # Q node\n",
    "    Q_node = node_list[0]\n",
    "    Q_node.content_features = get_features_from_XLNET(Q_node.content_raw,\n",
    "                                                     tokenizer = tokenizer_XLNET,\n",
    "                                                      model = model_XLNET,\n",
    "                                                     device = DEVICE) # [1,N,D]\n",
    "    Q_node.cls_feature = Q_node.content_features[:,-1,:]\n",
    "\n",
    "    # S node\n",
    "    for S_node in [i for i in node_list if i.node_type == 'Sentence']:\n",
    "        # content_features不能包含特殊字符.\n",
    "        S_node.content_features = get_features_from_XLNET(S_node.content_raw,\n",
    "                                                            add_special_tokens=False,\n",
    "                                                            tokenizer = tokenizer_XLNET,\n",
    "                                                            model = model_XLNET,\n",
    "                                                            device = DEVICE)\n",
    "        \n",
    "        S_node.cls_feature = get_features_from_XLNET(Q_node.content_raw, \n",
    "                                                        S_node.content_raw,\n",
    "                                                        add_special_tokens=True,\n",
    "                                                        tokenizer = tokenizer_XLNET,\n",
    "                                                        model = model_XLNET,\n",
    "                                                        device = DEVICE)[:,-1,:]    \n",
    "    \n",
    "    # P node\n",
    "    for P_i, P_node in [(i,n) for i,n in enumerate(node_list) if n.node_type == 'Paragraph']:\n",
    "            S_in_P = [n for n in node_list if n.parent_id == P_i]\n",
    "            all_S_raw = ' '.join([n.content_raw for n in S_in_P])\n",
    "            P_node.content_features = [n.content_features for n in S_in_P]\n",
    "            P_node.cls_feature = get_features_from_XLNET(Q_node.content_raw, \n",
    "                                                            all_S_raw,\n",
    "                                                            add_special_tokens=True,\n",
    "                                                            tokenizer = tokenizer_XLNET,\n",
    "                                                            model = model_XLNET,\n",
    "                                                            device = DEVICE)[:,-1,:]\n",
    "\n",
    "    # E node\n",
    "    for E_node in [i for i in node_list if i.node_type == 'Entity']:\n",
    "        start = E_node.start_in_sentence\n",
    "        end = E_node.end_in_sentence\n",
    "        E_node.content_features = node_list[E_node.parent_id].content_features[:,start:end,:]\n",
    "        E_node.cls_feature = torch.mean(E_node.content_features, dim = 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list = hotpotQA_train_preprocess[0]['node_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tQuestion:\t\ttorch.Size([1, 768])\n",
      "1\tParagraph:\t\ttorch.Size([1, 768])\n",
      "2\tSentence:\t\ttorch.Size([1, 768])\n",
      "3\tEntity:\t\ttorch.Size([1, 768])\n",
      "4\tSentence:\t\ttorch.Size([1, 768])\n",
      "5\tEntity:\t\ttorch.Size([1, 768])\n",
      "6\tSentence:\t\ttorch.Size([1, 768])\n",
      "7\tEntity:\t\ttorch.Size([1, 768])\n",
      "8\tSentence:\t\ttorch.Size([1, 768])\n",
      "9\tEntity:\t\ttorch.Size([1, 768])\n",
      "10\tEntity:\t\ttorch.Size([1, 768])\n",
      "11\tSentence:\t\ttorch.Size([1, 768])\n",
      "12\tEntity:\t\ttorch.Size([1, 768])\n",
      "13\tEntity:\t\ttorch.Size([1, 768])\n",
      "14\tEntity:\t\ttorch.Size([1, 768])\n",
      "15\tSentence:\t\ttorch.Size([1, 768])\n",
      "16\tEntity:\t\ttorch.Size([1, 768])\n",
      "17\tSentence:\t\ttorch.Size([1, 768])\n",
      "18\tEntity:\t\ttorch.Size([1, 768])\n",
      "19\tParagraph:\t\ttorch.Size([1, 768])\n",
      "20\tSentence:\t\ttorch.Size([1, 768])\n",
      "21\tEntity:\t\ttorch.Size([1, 768])\n",
      "22\tEntity:\t\ttorch.Size([1, 768])\n",
      "23\tSentence:\t\ttorch.Size([1, 768])\n",
      "24\tEntity:\t\ttorch.Size([1, 768])\n",
      "25\tSentence:\t\ttorch.Size([1, 768])\n",
      "26\tEntity:\t\ttorch.Size([1, 768])\n",
      "27\tEntity:\t\ttorch.Size([1, 768])\n",
      "28\tEntity:\t\ttorch.Size([1, 768])\n",
      "29\tSentence:\t\ttorch.Size([1, 768])\n",
      "30\tEntity:\t\ttorch.Size([1, 768])\n",
      "31\tEntity:\t\ttorch.Size([1, 768])\n",
      "32\tParagraph:\t\ttorch.Size([1, 768])\n",
      "33\tSentence:\t\ttorch.Size([1, 768])\n",
      "34\tEntity:\t\ttorch.Size([1, 768])\n",
      "35\tEntity:\t\ttorch.Size([1, 768])\n",
      "36\tSentence:\t\ttorch.Size([1, 768])\n",
      "37\tEntity:\t\ttorch.Size([1, 768])\n",
      "38\tSentence:\t\ttorch.Size([1, 768])\n",
      "39\tEntity:\t\ttorch.Size([1, 768])\n",
      "40\tEntity:\t\ttorch.Size([1, 768])\n",
      "41\tSentence:\t\ttorch.Size([1, 768])\n",
      "42\tEntity:\t\ttorch.Size([1, 768])\n",
      "43\tSentence:\t\ttorch.Size([1, 768])\n",
      "44\tEntity:\t\ttorch.Size([1, 768])\n",
      "45\tSentence:\t\ttorch.Size([1, 768])\n",
      "46\tEntity:\t\ttorch.Size([1, 768])\n",
      "47\tEntity:\t\ttorch.Size([1, 768])\n",
      "48\tEntity:\t\ttorch.Size([1, 768])\n",
      "49\tSentence:\t\ttorch.Size([1, 768])\n",
      "50\tEntity:\t\ttorch.Size([1, 768])\n",
      "51\tSentence:\t\ttorch.Size([1, 768])\n",
      "52\tEntity:\t\ttorch.Size([1, 768])\n",
      "53\tSentence:\t\ttorch.Size([1, 768])\n",
      "54\tEntity:\t\ttorch.Size([1, 768])\n",
      "55\tParagraph:\t\ttorch.Size([1, 768])\n",
      "56\tSentence:\t\ttorch.Size([1, 768])\n",
      "57\tEntity:\t\ttorch.Size([1, 768])\n",
      "58\tEntity:\t\ttorch.Size([1, 768])\n",
      "59\tSentence:\t\ttorch.Size([1, 768])\n",
      "60\tSentence:\t\ttorch.Size([1, 768])\n",
      "61\tEntity:\t\ttorch.Size([1, 768])\n",
      "62\tEntity:\t\ttorch.Size([1, 768])\n",
      "63\tSentence:\t\ttorch.Size([1, 768])\n",
      "64\tEntity:\t\ttorch.Size([1, 768])\n",
      "65\tParagraph:\t\ttorch.Size([1, 768])\n",
      "66\tSentence:\t\ttorch.Size([1, 768])\n",
      "67\tEntity:\t\ttorch.Size([1, 768])\n",
      "68\tEntity:\t\ttorch.Size([1, 768])\n",
      "69\tParagraph:\t\ttorch.Size([1, 768])\n",
      "70\tSentence:\t\ttorch.Size([1, 768])\n",
      "71\tEntity:\t\ttorch.Size([1, 768])\n",
      "72\tEntity:\t\ttorch.Size([1, 768])\n",
      "73\tSentence:\t\ttorch.Size([1, 768])\n",
      "74\tEntity:\t\ttorch.Size([1, 768])\n",
      "75\tEntity:\t\ttorch.Size([1, 768])\n",
      "76\tSentence:\t\ttorch.Size([1, 768])\n",
      "77\tEntity:\t\ttorch.Size([1, 768])\n",
      "78\tParagraph:\t\ttorch.Size([1, 768])\n",
      "79\tSentence:\t\ttorch.Size([1, 768])\n",
      "80\tEntity:\t\ttorch.Size([1, 768])\n",
      "81\tEntity:\t\ttorch.Size([1, 768])\n",
      "82\tSentence:\t\ttorch.Size([1, 768])\n",
      "83\tEntity:\t\ttorch.Size([1, 768])\n",
      "84\tSentence:\t\ttorch.Size([1, 768])\n",
      "85\tEntity:\t\ttorch.Size([1, 768])\n",
      "86\tSentence:\t\ttorch.Size([1, 768])\n",
      "87\tSentence:\t\ttorch.Size([1, 768])\n",
      "88\tEntity:\t\ttorch.Size([1, 768])\n",
      "89\tEntity:\t\ttorch.Size([1, 768])\n",
      "90\tParagraph:\t\ttorch.Size([1, 768])\n",
      "91\tSentence:\t\ttorch.Size([1, 768])\n",
      "92\tEntity:\t\ttorch.Size([1, 768])\n",
      "93\tEntity:\t\ttorch.Size([1, 768])\n",
      "94\tSentence:\t\ttorch.Size([1, 768])\n",
      "95\tSentence:\t\ttorch.Size([1, 768])\n",
      "96\tEntity:\t\ttorch.Size([1, 768])\n",
      "97\tSentence:\t\ttorch.Size([1, 768])\n",
      "98\tParagraph:\t\ttorch.Size([1, 768])\n",
      "99\tSentence:\t\ttorch.Size([1, 768])\n",
      "100\tEntity:\t\ttorch.Size([1, 768])\n",
      "101\tEntity:\t\ttorch.Size([1, 768])\n",
      "102\tSentence:\t\ttorch.Size([1, 768])\n",
      "103\tSentence:\t\ttorch.Size([1, 768])\n",
      "104\tEntity:\t\ttorch.Size([1, 768])\n",
      "105\tEntity:\t\ttorch.Size([1, 768])\n",
      "106\tSentence:\t\ttorch.Size([1, 768])\n",
      "107\tEntity:\t\ttorch.Size([1, 768])\n",
      "108\tParagraph:\t\ttorch.Size([1, 768])\n",
      "109\tSentence:\t\ttorch.Size([1, 768])\n",
      "110\tEntity:\t\ttorch.Size([1, 768])\n",
      "111\tEntity:\t\ttorch.Size([1, 768])\n",
      "112\tSentence:\t\ttorch.Size([1, 768])\n",
      "113\tSentence:\t\ttorch.Size([1, 768])\n",
      "114\tEntity:\t\ttorch.Size([1, 768])\n",
      "115\tEntity:\t\ttorch.Size([1, 768])\n",
      "116\tSentence:\t\ttorch.Size([1, 768])\n",
      "117\tSentence:\t\ttorch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "for i,n in enumerate(node_list):\n",
    "    print(f\"{i}\\t{n.node_type}:\\t\\t{n.cls_feature.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\tSentence:\t\ttorch.Size([1, 19, 768])\n",
      "4\tSentence:\t\ttorch.Size([1, 53, 768])\n",
      "6\tSentence:\t\ttorch.Size([1, 9, 768])\n",
      "8\tSentence:\t\ttorch.Size([1, 24, 768])\n",
      "11\tSentence:\t\ttorch.Size([1, 43, 768])\n",
      "15\tSentence:\t\ttorch.Size([1, 13, 768])\n",
      "17\tSentence:\t\ttorch.Size([1, 9, 768])\n",
      "20\tSentence:\t\ttorch.Size([1, 17, 768])\n",
      "23\tSentence:\t\ttorch.Size([1, 53, 768])\n",
      "25\tSentence:\t\ttorch.Size([1, 31, 768])\n",
      "29\tSentence:\t\ttorch.Size([1, 31, 768])\n",
      "33\tSentence:\t\ttorch.Size([1, 21, 768])\n",
      "36\tSentence:\t\ttorch.Size([1, 33, 768])\n",
      "38\tSentence:\t\ttorch.Size([1, 13, 768])\n",
      "41\tSentence:\t\ttorch.Size([1, 7, 768])\n",
      "43\tSentence:\t\ttorch.Size([1, 5, 768])\n",
      "45\tSentence:\t\ttorch.Size([1, 57, 768])\n",
      "49\tSentence:\t\ttorch.Size([1, 6, 768])\n",
      "51\tSentence:\t\ttorch.Size([1, 22, 768])\n",
      "53\tSentence:\t\ttorch.Size([1, 23, 768])\n",
      "56\tSentence:\t\ttorch.Size([1, 45, 768])\n",
      "59\tSentence:\t\ttorch.Size([1, 13, 768])\n",
      "60\tSentence:\t\ttorch.Size([1, 28, 768])\n",
      "63\tSentence:\t\ttorch.Size([1, 22, 768])\n",
      "66\tSentence:\t\ttorch.Size([1, 27, 768])\n",
      "70\tSentence:\t\ttorch.Size([1, 26, 768])\n",
      "73\tSentence:\t\ttorch.Size([1, 41, 768])\n",
      "76\tSentence:\t\ttorch.Size([1, 19, 768])\n",
      "79\tSentence:\t\ttorch.Size([1, 18, 768])\n",
      "82\tSentence:\t\ttorch.Size([1, 26, 768])\n",
      "84\tSentence:\t\ttorch.Size([1, 29, 768])\n",
      "86\tSentence:\t\ttorch.Size([1, 17, 768])\n",
      "87\tSentence:\t\ttorch.Size([1, 18, 768])\n",
      "91\tSentence:\t\ttorch.Size([1, 18, 768])\n",
      "94\tSentence:\t\ttorch.Size([1, 7, 768])\n",
      "95\tSentence:\t\ttorch.Size([1, 12, 768])\n",
      "97\tSentence:\t\ttorch.Size([1, 16, 768])\n",
      "99\tSentence:\t\ttorch.Size([1, 21, 768])\n",
      "102\tSentence:\t\ttorch.Size([1, 13, 768])\n",
      "103\tSentence:\t\ttorch.Size([1, 30, 768])\n",
      "106\tSentence:\t\ttorch.Size([1, 24, 768])\n",
      "109\tSentence:\t\ttorch.Size([1, 17, 768])\n",
      "112\tSentence:\t\ttorch.Size([1, 9, 768])\n",
      "113\tSentence:\t\ttorch.Size([1, 28, 768])\n",
      "116\tSentence:\t\ttorch.Size([1, 14, 768])\n",
      "117\tSentence:\t\ttorch.Size([1, 25, 768])\n"
     ]
    }
   ],
   "source": [
    "for i,n in enumerate(node_list):\n",
    "    if n.node_type != 'Sentence': continue\n",
    "    print(f\"{i}\\t{n.node_type}:\\t\\t{n.content_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存\n",
    "\n",
    "直接保存太大.\n",
    "\n",
    "```python\n",
    "save_cache_path = 'save_cache/'\n",
    "\n",
    "with open(save_cache_path+'hotpotQA_train_preprocess100_features.pkl', 'wb') as fp:\n",
    "    pickle.dump(hotpotQA_train_preprocess, fp, protocol=-1)\n",
    "!ls -hl $save_cache_path\n",
    "\n",
    "# -rw-r--r-- 1 root root 2.4G Mar  9 21:06 hotpotQA_train_preprocess100_features.pkl\n",
    "```\n",
    "\n",
    "直接保存为`matrix`和`adj`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from traceback import print_exc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotpotQA_train_preprocess2 = deepcopy(hotpotQA_train_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotpotQA_train_preprocess[10]['node_list'][0].content_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f79ed5194ce4d71ae2382fa509ab182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm_notebook(hotpotQA_train_preprocess2):\n",
    "    for node in item['node_list']:\n",
    "        node.content_features = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.7G\r\n",
      "-rw-r--r-- 1 root root 1.3G Mar  9 21:39 hotpotQA_train_preprocess100_features.pkl\r\n",
      "-rw-r--r-- 1 root root 3.5M Mar  9 10:16 hotpotQA_train_preprocess100.pkl\r\n",
      "-rw-r--r-- 1 root root 418M Mar  8 11:34 model_HotpotQA.pth\r\n"
     ]
    }
   ],
   "source": [
    "save_cache_path = 'save_cache/'\n",
    "\n",
    "with open(save_cache_path+'hotpotQA_train_preprocess100_features.pkl', 'wb') as fp:\n",
    "    pickle.dump(hotpotQA_train_preprocess2, fp, protocol=-1)\n",
    "!ls -hl $save_cache_path\n",
    "\n",
    "# -rw-r--r-- 1 root root 2.4G Mar  9 21:06 hotpotQA_train_preprocess100_features.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 只保存特征矩阵和邻接矩阵\n",
    "\n",
    "**不能!** 因为之后要用到整个句子的tokens, 需要保存整个对象.\n",
    "\n",
    "对100个ques_item, `node.content_features`部分:\n",
    "\n",
    "- 保存为tensor, 占用**2.4G**.\n",
    "- 保存为list, 占用**3.6**.\n",
    "- 删掉, 占用**1.3G**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb56febed5d437c890a819c8be0941b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "matrix_adj_label = []\n",
    "for ques_item in tqdm_notebook(hotpotQA_train_preprocess2):\n",
    "    adj = ques_item['sp_adj']\n",
    "    features_matrix = torch.cat([n.cls_feature for n in ques_item['node_list']], dim = 0)\n",
    "    labels = torch.stack([torch.tensor([n.is_support]) \\\n",
    "                          if n.node_type != 'Paragraph' else torch.tensor([False])\\\n",
    "                          for n in ques_item['node_list']], dim=0).bool()\n",
    "    matrix_adj_label.append((features_matrix, adj, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 458M\r\n",
      "-rw-r--r-- 1 root root  37M Mar 10 10:33 hotpotQA_train_preprocess100_feat_adj.pkl\r\n",
      "-rw-r--r-- 1 root root 3.5M Mar  9 10:16 hotpotQA_train_preprocess100.pkl\r\n",
      "-rw-r--r-- 1 root root 418M Mar  8 11:34 model_HotpotQA.pth\r\n"
     ]
    }
   ],
   "source": [
    "save_cache_path = 'save_cache/'\n",
    "\n",
    "with open(save_cache_path+'hotpotQA_train_preprocess100_feat_adj.pkl', 'wb') as fp:\n",
    "    pickle.dump(matrix_adj_label, fp, protocol=-1)\n",
    "!ls -hl $save_cache_path\n",
    "\n",
    "# -rw-r--r-- 1 root root 2.4G Mar  9 21:06 hotpotQA_train_preprocess100_features.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 并行(失败)\n",
    "\n",
    "cpu模型下能跑通, 但非常慢. GPU模式失败."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def _process(ques_item):\n",
    "    node_list = ques_item['node_list']\n",
    "    ques_para_list = ques_item['ques_para_list']\n",
    "\n",
    "    # Q node\n",
    "    Q_node = node_list[0]\n",
    "    Q_node.content_features = get_features_from_XLNET(Q_node.content_raw,\n",
    "                                                     tokenizer = tokenizer_XLNET,\n",
    "                                                      model = model_XLNET,\n",
    "                                                     device = DEVICE) # [1,N,D]\n",
    "    Q_node.cls_feature = Q_node.content_features[:,-1,:]\n",
    "\n",
    "    # S node\n",
    "    for S_node in [i for i in node_list if i.node_type == 'Sentence']:\n",
    "        # content_features不能包含特殊字符.\n",
    "        S_node.content_features = get_features_from_XLNET(S_node.content_raw,\n",
    "                                                            add_special_tokens=False,\n",
    "                                                            tokenizer = tokenizer_XLNET,\n",
    "                                                            model = model_XLNET,\n",
    "                                                            device = DEVICE)\n",
    "        \n",
    "        S_node.cls_feature = get_features_from_XLNET(Q_node.content_raw, \n",
    "                                                        S_node.content_raw,\n",
    "                                                        add_special_tokens=True,\n",
    "                                                        tokenizer = tokenizer_XLNET,\n",
    "                                                        model = model_XLNET,\n",
    "                                                        device = DEVICE)[:,-1,:]\n",
    "    \n",
    "\n",
    "    \n",
    "    # P node\n",
    "    for P_i, P_node in [(i,n) for i,n in enumerate(node_list) if n.node_type == 'Paragraph']:\n",
    "            S_in_P = [n for n in node_list if n.parent_id == P_i]\n",
    "            all_S_raw = ' '.join([n.content_raw for n in S_in_P])\n",
    "            P_node.content_features = [n.content_features for n in S_in_P]\n",
    "            P_node.cls_feature = get_features_from_XLNET(Q_node.content_raw, \n",
    "                                                            all_S_raw,\n",
    "                                                            add_special_tokens=True,\n",
    "                                                            tokenizer = tokenizer_XLNET,\n",
    "                                                            model = model_XLNET,\n",
    "                                                            device = DEVICE)[:,-1,:]\n",
    "\n",
    "\n",
    "    # E node\n",
    "    for E_node in [i for i in node_list if i.node_type == 'Entity']:\n",
    "        start = E_node.start_in_sentence\n",
    "        end = E_node.end_in_sentence\n",
    "        E_node.content_features = node_list[E_node.parent_id].content_features[start:end]\n",
    "        E_node.cls_feature = E_node.content_features\n",
    "        \n",
    "#     return ques_item"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import torch\n",
    "\n",
    "def preprocessing(item_num = 2, thread_num = 1):\n",
    "    \n",
    "    '''main multi-processes function.'''\n",
    "    item_num = None if item_num<0 else item_num\n",
    "    thread_num = 1 if thread_num<0 else thread_num\n",
    "    \n",
    "    resturn_list = []\n",
    "    pbar = tqdm_notebook(total = len(hotpotQA_train_preprocess[:item_num]), desc = f'processing json items')\n",
    "    with Pool(thread_num) as pool:\n",
    "        pool_iter = pool.imap(_process, hotpotQA_train_preprocess[:item_num])\n",
    "        for i,r in enumerate(pool_iter):\n",
    "            resturn_list.append(r)\n",
    "            pbar.update()\n",
    "    return resturn_list\n",
    "\n",
    "# CPU: 10个, 2进程. [02:20<00:00, 14.03s/it]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in tqdm_notebook(hotpotQA_train_preprocess):\n",
    "    _process(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
