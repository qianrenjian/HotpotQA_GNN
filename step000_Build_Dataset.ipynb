{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "import pickle\n",
    "\n",
    "from class_new import QuestionNode, ParagraphTitleNode, SentenceNode, EntityNode, Adjacency_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 阶段1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_features_from_XLNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_features_from_XLNET(text,text_pair=None,\n",
    "                            tokenizer = None,\n",
    "                            model = None,\n",
    "                            add_special_tokens = True,\n",
    "                           device = 'cuda'):\n",
    "    '''XLNET在512张TPU v3上训练5.5天得到. 一张TPU 8核心 128GB内存.'''\n",
    "    \n",
    "    assert model\n",
    "    model_input = tokenizer_XLNET.encode_plus(text,text_pair,\n",
    "                                        add_special_tokens=add_special_tokens,\n",
    "                                        return_tensors='pt')\n",
    "    \n",
    "    model_input = {k:v.to(device) for k,v in model_input.items()}\n",
    "    \n",
    "    # 不能在函数里面设置device.\n",
    "    # model.to(device)\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**model_input)[0]\n",
    "    \n",
    "    return last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "HotpotQA_preprocess_file = 'save_cache/hotpotQA_train_preprocess20.pkl'\n",
    "with open(HotpotQA_preprocess_file,'rb')as fp:\n",
    "    hotpotQA_train_preprocess = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model_name = 'xlnet-base-cased'\n",
    "proxies={\"http_proxy\": \"127.0.0.1:10802\",\n",
    "         \"https_proxy\": \"127.0.0.1:10802\"}\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name,proxies=proxies)\n",
    "\n",
    "tokenizer_XLNET = AutoTokenizer.from_pretrained(model_name,proxies=proxies)\n",
    "model_XLNET = AutoModel.from_config(config)\n",
    "DEVICE = 'cuda'\n",
    "# DEVICE = 'cpu'\n",
    "_ = model_XLNET.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237ffbf813d84aafb9d835c2497e18b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='building features', max=20, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ques_item in tqdm_notebook(hotpotQA_train_preprocess, desc = 'building features'):\n",
    "    node_list = ques_item['node_list']\n",
    "\n",
    "    # Q node\n",
    "    Q_node = node_list[0]\n",
    "    Q_node.content_features = get_features_from_XLNET(Q_node.content_raw,\n",
    "                                                     tokenizer = tokenizer_XLNET,\n",
    "                                                      model = model_XLNET,\n",
    "                                                     device = DEVICE) # [1,N,D]\n",
    "    Q_node.cls_feature = Q_node.content_features[:,-1,:]\n",
    "\n",
    "    # S node\n",
    "    for S_node in [i for i in node_list if i.node_type == 'Sentence']:\n",
    "        # content_features不能包含特殊字符.\n",
    "        S_node.content_features = get_features_from_XLNET(S_node.content_raw,\n",
    "                                                            add_special_tokens=False,\n",
    "                                                            tokenizer = tokenizer_XLNET,\n",
    "                                                            model = model_XLNET,\n",
    "                                                            device = DEVICE)\n",
    "        \n",
    "        S_node.cls_feature = get_features_from_XLNET(Q_node.content_raw, \n",
    "                                                        S_node.content_raw,\n",
    "                                                        add_special_tokens=True,\n",
    "                                                        tokenizer = tokenizer_XLNET,\n",
    "                                                        model = model_XLNET,\n",
    "                                                        device = DEVICE)[:,-1,:]    \n",
    "    \n",
    "    # P node\n",
    "    for P_i, P_node in [(i,n) for i,n in enumerate(node_list) if n.node_type == 'Paragraph']:\n",
    "            S_in_P = [n for n in node_list if n.parent_id == P_i]\n",
    "            all_S_raw = ' '.join([n.content_raw for n in S_in_P])\n",
    "            P_node.content_features = [n.content_features for n in S_in_P]\n",
    "            P_node.cls_feature = get_features_from_XLNET(Q_node.content_raw, \n",
    "                                                            all_S_raw,\n",
    "                                                            add_special_tokens=True,\n",
    "                                                            tokenizer = tokenizer_XLNET,\n",
    "                                                            model = model_XLNET,\n",
    "                                                            device = DEVICE)[:,-1,:]\n",
    "\n",
    "    # E node\n",
    "    for E_node in [i for i in node_list if i.node_type == 'Entity']:\n",
    "        start = E_node.start_in_sentence\n",
    "        end = E_node.end_in_sentence\n",
    "        E_node.content_features = node_list[E_node.parent_id].content_features[:,start:end,:]\n",
    "        E_node.cls_feature = torch.mean(E_node.content_features, dim = 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     45
    ]
   },
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        # features=dim, hidden=8\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features))) # (dim, 8)\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1))) #(2*8,1)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, feat_matrix, adj):\n",
    "        # features (B, N, dim) , adj (B, N, N)\n",
    "        h = torch.matmul(feat_matrix, self.W) # (B,N,8)\n",
    "        N = h.shape[-2] # N\n",
    "        B = feat_matrix.shape[0]\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1, 1, N).view(B, N * N, -1), h.repeat(1, N, 1)], dim=-1)\\\n",
    "                                        .view(-1, N, N, 2 * self.out_features) # (B, N, N, 16)\n",
    "\n",
    "        # 节点聚合!! 后两维(N, 16)* (16, 1)表示对节点i,计算N个节点对(i,j): 进行线性变换后产生一个标量. 对应原文的e_ij\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(-1)) # (B, N, N, 16) * (16, 1) --> (B, N, N)\n",
    "        # e没有normalizaze?\n",
    "\n",
    "        zero_vec = -9e9*torch.ones_like(e) # (B, N, N)\n",
    "        attention = torch.where(adj > 0, e, zero_vec) # 都是[B, N, N]\n",
    "        attention = F.softmax(attention, dim = -1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.bmm(attention, h)  # (B, N, N)*(B, N ,8)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime) # 一种激活函数\n",
    "        else:\n",
    "            return h_prime # [B, N, 8]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, features, hidden, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Dense version of GAT.\"\"\"\n",
    "        # features=1433, hidden=8, nclass=7, dropout=0.6, alpha=0.3, nheads=8\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(features, hidden, dropout=dropout, alpha=alpha, concat=True) \\\n",
    "                           for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        # hidden * nheads = 8*8, nclass= 7 \n",
    "        self.out_att = GraphAttentionLayer(hidden * nheads, nclass, dropout=dropout, alpha=alpha, concat=False) # (2708,7)\n",
    "\n",
    "    def forward(self, feat_matrix, adj):\n",
    "        # features (B, N, dim) , adj (B, N, N)\n",
    "        feat_matrix = F.dropout(feat_matrix, self.dropout, training=self.training)\n",
    "        feat_matrix = torch.cat([att(feat_matrix, adj) for att in self.attentions], dim=-1) # (N,8*heads)\n",
    "        feat_matrix = F.dropout(feat_matrix, self.dropout, training=self.training)\n",
    "        logits = F.elu(self.out_att(feat_matrix, adj))\n",
    "\n",
    "        return logits # [B, N, num_class]\n",
    "    \n",
    "g_model = GAT(features=768, hidden=32, nclass=2, dropout=0.6, alpha=0.3, nheads=8).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HotpotQA_Dataset_New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     22,
     35,
     54,
     141
    ]
   },
   "outputs": [],
   "source": [
    "class HotpotQA_Dataset_New(Dataset):\n",
    "    def __init__(self, train_list, val_list):\n",
    "        \n",
    "        # elements\n",
    "        self.train_list = train_list # [(sen1, sen2, label), ...]\n",
    "        self.train_size = len(self.train_list)\n",
    "        \n",
    "        self.val_list = val_list\n",
    "        self.val_size = len(self.val_list)\n",
    "        \n",
    "        # func\n",
    "        self._lookup_dict = {'train': (self.train_list, self.train_size),\n",
    "                    'val': (self.val_list, self.val_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "        # parameters\n",
    "        self.pad_max_num = 0\n",
    "        self.pad_value = 0\n",
    "        self.pad_to_max_length = True\n",
    "        \n",
    "    @classmethod\n",
    "    def build_dataset(cls, hotpotQA_train_preprocess, ratio_train=0.7, seed=123):\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(hotpotQA_train_preprocess)        \n",
    "        \n",
    "        sample_train_num = int(ratio_train * len(hotpotQA_train_preprocess))\n",
    "\n",
    "        train_list = hotpotQA_train_preprocess[:sample_train_num]\n",
    "        val_list = hotpotQA_train_preprocess[sample_train_num:]\n",
    "\n",
    "        return cls(train_list, val_list)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_dataset_from_path(QA_preprocess_path, ratio_train=0.7, seed=123):\n",
    "        \n",
    "        with open(QA_preprocess_path,'rb')as fp:\n",
    "            hotpotQA_train_preprocess = pickle.load(fp)\n",
    "        \n",
    "        return HotpotQA_Dataset_New.build_dataset(hotpotQA_train_preprocess,\n",
    "                            ratio_train,\n",
    "                            seed,)\n",
    "\n",
    "    def set_parameters(self, pad_max_num, pad_value=0, pad_to_max_length=True):\n",
    "        self.pad_max_num = pad_max_num\n",
    "        self.pad_value = pad_value\n",
    "        self.pad_to_max_length = pad_to_max_length\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        assert split in ['train', 'val', 'test']\n",
    "        self._target_split = split\n",
    "        self._target_pair, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        QA_item = self.train_list[index]\n",
    "        node_list = QA_item['node_list']\n",
    "        sp_adj = QA_item['sp_adj']\n",
    "\n",
    "        # no padding\n",
    "        feature_matrix = torch.cat([n.cls_feature for n in node_list], dim=0)\n",
    "        adj = torch.from_numpy(sp_adj.to_dense_symmetric())\n",
    "        sent_mask = torch.tensor([1 if n.node_type == 'Sentence' else 0 for n in node_list]).unsqueeze(-1)\n",
    "        sent_label = torch.tensor([1 if n.node_type == 'Sentence' and n.is_support else 0 for n in node_list]).unsqueeze(-1)\n",
    "\n",
    "        para_mask = torch.tensor([1 if n.node_type == 'Paragraph' else 0 for n in node_list]).unsqueeze(-1)\n",
    "        para_label = torch.tensor([1 if n.node_type == 'Paragraph' and n.is_support else 0 for n in node_list]).unsqueeze(-1)\n",
    "\n",
    "        answer_type = 1 if node_list[0].answer in ['yes', 'no'] else 0\n",
    "        ans_yes_no = 1 if node_list[0].answer == 'yes' else 0\n",
    "\n",
    "        answer_type = torch.tensor(answer_type).unsqueeze(-1)\n",
    "        ans_yes_no = torch.tensor(ans_yes_no).unsqueeze(-1)\n",
    "\n",
    "        # find ans span in top 4 sentences.\n",
    "        answer_tokens = node_list[0].answer_tokens\n",
    "        sent_tokens = [n.content_tokens for n in node_list if n.node_type == 'Sentence']\n",
    "\n",
    "        ques_features = node_list[0].cls_feature\n",
    "        sent_features = [n.content_features for n in node_list if n.node_type == 'Sentence']\n",
    "\n",
    "\n",
    "        if self.pad_to_max_length:\n",
    "            node_len = feature_matrix.shape[-2]\n",
    "            pad_max_num = max(self.pad_max_num, node_len)\n",
    "            pad_value = self.pad_value\n",
    "            node_dim = feature_matrix.shape[-1]\n",
    "\n",
    "            feature_matrix_p = torch.zeros([pad_max_num, node_dim]).fill_(pad_value)\n",
    "            feature_matrix_p[:node_len,:] = feature_matrix\n",
    "            feature_matrix = feature_matrix_p\n",
    "\n",
    "            adj_p = torch.zeros([pad_max_num, pad_max_num]).fill_(pad_value)\n",
    "            adj_p[:node_len,:node_len] = adj\n",
    "            adj = adj_p\n",
    "\n",
    "            sent_mask_p = torch.zeros([pad_max_num, 1]).fill_(pad_value)\n",
    "            sent_mask_p[:node_len,:] = sent_mask\n",
    "            sent_mask = sent_mask_p\n",
    "            sent_label_p = torch.zeros([pad_max_num, 1]).fill_(pad_value)\n",
    "            sent_label_p[:node_len,:] = sent_label\n",
    "            sent_label = sent_label_p\n",
    "\n",
    "            para_mask_p = torch.zeros([pad_max_num, 1]).fill_(pad_value)\n",
    "            para_mask_p[:node_len,:] = para_mask\n",
    "            para_mask = para_mask_p\n",
    "            para_label_p = torch.zeros([pad_max_num, 1]).fill_(pad_value)\n",
    "            para_label_p[:node_len,:] = para_label\n",
    "            para_label = para_label_p\n",
    "\n",
    "        item_info_dict = {\n",
    "            'feature_matrix': feature_matrix,\n",
    "            'adj': adj,\n",
    "            'sent_mask': sent_mask,\n",
    "            'sent_label': sent_label,\n",
    "            'para_mask': para_mask,\n",
    "            'para_label': para_label,\n",
    "            'answer_type': answer_type,\n",
    "            'ans_yes_no': ans_yes_no,\n",
    "            'answer_tokens': answer_tokens,\n",
    "            'sent_tokens': sent_tokens,\n",
    "            'ques_features': ques_features,\n",
    "            'sent_features': sent_features,\n",
    "        }\n",
    "\n",
    "        return item_info_dict\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'HotpotQA Dataset. mode: {}. size: {}. max_seq: {}'.format\\\n",
    "            (self._target_split,self.pair_type,self.__len__(),self.max_length)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "    \n",
    "def gen_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu', seed = 123):\n",
    "\n",
    "    if seed: np.random.seed(seed)\n",
    "    dataset_len = dataset.__len__()\n",
    "    \n",
    "    index_pool = [i for i in range(dataset_len)]\n",
    "    if shuffle: np.random.shuffle(index_pool)\n",
    "\n",
    "    cursor = 0\n",
    "    while cursor < dataset_len:\n",
    "        # last batch\n",
    "        if cursor + batch_size > dataset_len:\n",
    "            if drop_last: break\n",
    "\n",
    "        FLAG_FIRST = True\n",
    "        for index in index_pool[cursor: min(cursor + batch_size, dataset_len)]:\n",
    "            if FLAG_FIRST:\n",
    "                feature_matrix = dataset[index]['feature_matrix'].unsqueeze(0)\n",
    "                adj = dataset[index]['adj'].unsqueeze(0)\n",
    "                sent_mask = dataset[index]['sent_mask'].unsqueeze(0)\n",
    "                sent_label = dataset[index]['sent_label'].unsqueeze(0)\n",
    "                para_mask = dataset[index]['para_mask'].unsqueeze(0)\n",
    "                para_label = dataset[index]['para_label'].unsqueeze(0)\n",
    "\n",
    "                answer_type = dataset[index]['answer_type']\n",
    "                ans_yes_no = dataset[index]['ans_yes_no']\n",
    "                answer_tokens = dataset[index]['answer_tokens']\n",
    "\n",
    "                sent_tokens,ques_features,sent_features = [],[],[]\n",
    "\n",
    "                FLAG_FIRST = False\n",
    "            else:\n",
    "                feature_matrix = torch.cat([feature_matrix, dataset[index]['feature_matrix'].unsqueeze(0)], dim=0)\n",
    "                adj = torch.cat([adj, dataset[index]['adj'].unsqueeze(0)], dim=0)\n",
    "                sent_mask = torch.cat([sent_mask, dataset[index]['sent_mask'].unsqueeze(0)], dim=0)\n",
    "                sent_label = torch.cat([sent_label, dataset[index]['sent_label'].unsqueeze(0)], dim=0)\n",
    "                para_mask = torch.cat([para_mask ,dataset[index]['para_mask'].unsqueeze(0)], dim=0)\n",
    "                para_label = torch.cat([para_label, dataset[index]['para_label'].unsqueeze(0)], dim=0)\n",
    "\n",
    "            sent_tokens.append(dataset[index]['sent_tokens'])\n",
    "            ques_features.append(dataset[index]['ques_features'])\n",
    "            sent_features.append(dataset[index]['sent_features'])\n",
    "\n",
    "        cursor += batch_size\n",
    "        \n",
    "        \n",
    "        feature_matrix = feature_matrix.to(device)\n",
    "        adj = adj.to(device)\n",
    "        sent_mask = sent_mask.to(device)\n",
    "        sent_label = sent_label.to(device)\n",
    "        para_mask = para_mask.to(device)\n",
    "        para_label = para_label.to(device)\n",
    "        \n",
    "        batch_item_info_dict = {\n",
    "            'feature_matrix': feature_matrix,\n",
    "            'adj': adj,\n",
    "            'sent_mask': sent_mask,\n",
    "            'sent_label': sent_label,\n",
    "            'para_mask': para_mask,\n",
    "            'para_label': para_label,\n",
    "            'answer_type': answer_type,\n",
    "            'ans_yes_no': ans_yes_no,\n",
    "            'answer_tokens': answer_tokens,\n",
    "            'sent_tokens': sent_tokens,\n",
    "            'ques_features': ques_features,\n",
    "            'sent_features': sent_features,\n",
    "        }\n",
    "        yield batch_item_info_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "dataset = HotpotQA_Dataset_New.build_dataset(hotpotQA_train_preprocess)\n",
    "dataset.set_parameters(200)\n",
    "batch_generator = gen_batches(dataset,batch_size=2, device=DEVICE)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_item_info_dict = {\n",
    "    'feature_matrix': feature_matrix,\n",
    "    'adj': adj,\n",
    "    'sent_mask': sent_mask,\n",
    "    'sent_label': sent_label,\n",
    "    'para_mask': para_mask,\n",
    "    'para_label': para_label,\n",
    "    'answer_type': answer_type,\n",
    "    'ans_yes_no': ans_yes_no,\n",
    "    'answer_tokens': answer_tokens,\n",
    "    'sent_tokens': sent_tokens,\n",
    "    'ques_features': ques_features,\n",
    "    'sent_features': sent_features,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "torch.Size([1, 38, 768])\n"
     ]
    }
   ],
   "source": [
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    \n",
    "    logits = g_model(batch_dict['feature_matrix'], batch_dict['adj'])\n",
    "    \n",
    "    logits\n",
    "\n",
    "    sent_mask = batch_dict['sent_mask']\n",
    "    sent_label = batch_dict['sent_label']\n",
    "    \n",
    "    \n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = (logits * sent_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 阶段2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find top K sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_index = torch.nonzero(sent_mask[0].view(-1,).long()).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  5,  8, 10, 13, 17, 21, 23, 25, 26, 29, 32, 35, 38, 41, 44, 47, 49,\n",
       "        53, 56, 59, 61, 64, 67, 71, 75, 78, 81, 84], device='cuda:0')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29, 2])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_logits = torch.index_select(logits[0], 0, sent_index)\n",
    "sent_logits_soft = F.softmax(sent_logits, dim = -1)\n",
    "sent_logits_soft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value, index = torch.max(sent_logits_soft, dim=-1)\n",
    "sent_scores = value.cpu().tolist()\n",
    "len(sent_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_score_index = [(score, index) for score, index in zip(sent_scores, range(len(sent_scores)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 19, 21, 18]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_score_index_sort = sorted(sent_score_index,key=lambda x:x[0],reverse=True)\n",
    "sent_indexes = [x[-1] for x in sent_score_index_sort[:4]]\n",
    "sent_indexes\n",
    "# 获取batch中一个元素的前4句."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试find span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_final_tokens = []\n",
    "for i in sent_indexes: sent_final_tokens.extend(sent_tokens[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_tokens_test =  ['Circuit', ',', 'near']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_tokens_test in sent_final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Circuit', ',', 'near']\n",
      "['Mount', 'Pan', '##orama', 'Circuit', ',', 'near']\n"
     ]
    }
   ],
   "source": [
    "print(answer_tokens_test)\n",
    "print(sent_final_tokens[55:61])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def find_ans_spans(target, tokens, offets_type = 'position', top_num = None):\n",
    "    assert offets_type in ['position', 'range'] and \\\n",
    "        type(target) == type(tokens) == list\n",
    "    len_x1 = len(target)\n",
    "    len_x2 = len(tokens)\n",
    "    if len_x1 == 0 or len_x2 == 0 or len_x1 > len_x2:\n",
    "        return [0,0]\n",
    "    \n",
    "    i1=0\n",
    "    i2=0\n",
    "    i2_current = 0\n",
    "    spans = []\n",
    "    while i2 <= len_x2 - len_x1:\n",
    "        if top_num and len(spans) == top_num:\n",
    "            break\n",
    "        i2_current = i2\n",
    "        while i1 < len_x1:\n",
    "            if target[i1] != tokens[i2]: \n",
    "                i1 = 0\n",
    "                i2 = i2_current + 1\n",
    "                break\n",
    "            else:\n",
    "                i1 += 1\n",
    "                i2 += 1\n",
    "                \n",
    "        if not i1 < len_x1:\n",
    "            i1 = 0\n",
    "            if offets_type == 'position':  \n",
    "                spans.append([i2_current, i2_current+len_x1-1])\n",
    "            else:\n",
    "                spans.append([i2_current, i2_current+len_x1])\n",
    "                \n",
    "    return spans[:top_num+1] if top_num else spans[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[58, 60]]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_ans_spans(answer_tokens_test, sent_final_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算span\n",
    "\n",
    "将`ques_feature`和`content_features`加上`[SEP] [CLS]`拼接在一起."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_tokens_test2 = [ 'Australian', 'GT', 'Championship',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens_combine = []\n",
    "for s_index in sent_indexes: sent_tokens_combine.extend(sent_tokens[0][s_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[88, 90]]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tokens = ['<x1>','[ques]','<x2>'] + sent_tokens_combine + ['<cls>']\n",
    "ans_spans = find_ans_spans(answer_tokens_test2, final_tokens, top_num = 2)\n",
    "ans_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取最终features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP_rep = torch.randn([768,]).to(DEVICE)\n",
    "CLS_rep = torch.randn([768,]).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rep = [SEP_rep, ques_features[0].squeeze(), SEP_rep]\n",
    "for s_index in sent_indexes: final_rep.extend(sent_features[0][s_index].view(-1, 768))\n",
    "final_rep = final_rep + [CLS_rep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([183, 768])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(final_rep, dim = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 19, 21, 18]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_item_info_dict = {\n",
    "    'feature_matrix': feature_matrix,\n",
    "    'adj': adj,\n",
    "    'sent_mask': sent_mask,\n",
    "    'sent_label': sent_label,\n",
    "    'para_mask': para_mask,\n",
    "    'para_label': para_label,\n",
    "    'answer_type': answer_type,\n",
    "    'ans_yes_no': ans_yes_no,\n",
    "    'answer_tokens': answer_tokens,\n",
    "    'sent_tokens': sent_tokens,\n",
    "    'ques_features': ques_features,\n",
    "    'sent_features': sent_features,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "218.275px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
