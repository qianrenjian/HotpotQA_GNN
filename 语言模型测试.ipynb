{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>语言模型测试<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#使用BERT-seq\" data-toc-modified-id=\"使用BERT-seq-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>使用BERT-seq</a></span><ul class=\"toc-item\"><li><span><a href=\"#正常训练\" data-toc-modified-id=\"正常训练-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>正常训练</a></span></li><li><span><a href=\"#冻结模型\" data-toc-modified-id=\"冻结模型-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>冻结模型</a></span><ul class=\"toc-item\"><li><span><a href=\"#封装\" data-toc-modified-id=\"封装-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>封装</a></span></li><li><span><a href=\"#训练\" data-toc-modified-id=\"训练-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>训练</a></span></li></ul></li><li><span><a href=\"#半精度float16(无冻结)\" data-toc-modified-id=\"半精度float16(无冻结)-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>半精度float16(无冻结)</a></span></li><li><span><a href=\"#float16+冻结模型\" data-toc-modified-id=\"float16+冻结模型-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>float16+冻结模型</a></span></li><li><span><a href=\"#分布式训练\" data-toc-modified-id=\"分布式训练-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>分布式训练</a></span></li><li><span><a href=\"#GPU使用情况\" data-toc-modified-id=\"GPU使用情况-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>GPU使用情况</a></span><ul class=\"toc-item\"><li><span><a href=\"#释放显存\" data-toc-modified-id=\"释放显存-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>释放显存</a></span></li></ul></li></ul></li><li><span><a href=\"#使用bert-Model\" data-toc-modified-id=\"使用bert-Model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>使用bert Model</a></span></li><li><span><a href=\"#自定义self-attention-encoder\" data-toc-modified-id=\"自定义self-attention-encoder-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>自定义self-attention encoder</a></span></li><li><span><a href=\"#自定义bert-model\" data-toc-modified-id=\"自定义bert-model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>自定义bert-model</a></span></li><li><span><a href=\"#试验:-tokenizer和BERT-NER\" data-toc-modified-id=\"试验:-tokenizer和BERT-NER-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>试验: tokenizer和BERT-NER</a></span></li><li><span><a href=\"#试验:-tokenizer和Spacy-NER\" data-toc-modified-id=\"试验:-tokenizer和Spacy-NER-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>试验: tokenizer和Spacy-NER</a></span></li><li><span><a href=\"#XLNET测试\" data-toc-modified-id=\"XLNET测试-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>XLNET测试</a></span></li><li><span><a href=\"#试验:-BertForQuestionAnswering\" data-toc-modified-id=\"试验:-BertForQuestionAnswering-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>试验: BertForQuestionAnswering</a></span><ul class=\"toc-item\"><li><span><a href=\"#配置小型模型\" data-toc-modified-id=\"配置小型模型-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>配置小型模型</a></span></li></ul></li><li><span><a href=\"#测试Dataset与字符串\" data-toc-modified-id=\"测试Dataset与字符串-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>测试Dataset与字符串</a></span></li><li><span><a href=\"#nomalizations比较\" data-toc-modified-id=\"nomalizations比较-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>nomalizations比较</a></span></li><li><span><a href=\"#Albert\" data-toc-modified-id=\"Albert-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Albert</a></span></li><li><span><a href=\"#RoBerta\" data-toc-modified-id=\"RoBerta-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>RoBerta</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AfDT6pWbG9-J",
    "outputId": "09dfb5b8-a7cd-4ccb-e596-3076781d3c25"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/folders/')\n",
    "    # !pip install transformers\n",
    "    # !pip install -U spacy[cuda100]\n",
    "    # !wget -P /content/folders/My\\ Drive/download/ https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz\n",
    "    # !pip install /content/folders/My\\ Drive/download/en_core_web_lg-2.2.5.tar.gz\n",
    "    # !wget -P /content/folders/My\\ Drive/HotpotQA/ http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json\n",
    "    # json_train_path = '/content/folders/My Drive/HotpotQA/样例_hotpot_train_v1.1.json' # 5个例子\n",
    "    json_train_path = '/content/folders/My Drive/HotpotQA/hotpot_train_v1.1.json'\n",
    "    save_cache_path = '/content/folders/My Drive/save_cache/'\n",
    "    save_cache_path_linux = '/content/folders/My\\ Drive/save_cache/'\n",
    "    HotpotQA_path = '/content/folders/My Drive/HotpotQA'\n",
    "    proxies = None\n",
    "except:\n",
    "    json_train_path = r'./data/hotpot_train_v1.1.json'\n",
    "    HotpotQA_path = './'\n",
    "    save_cache_path = 'save_cache/'\n",
    "    use_proxy = False\n",
    "    proxies={\"http_proxy\": \"127.0.0.1:10802\",\n",
    "        \"https_proxy\": \"127.0.0.1:10802\"} if use_proxy else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iYQPIma5Zl19"
   },
   "source": [
    "# 使用BERT-seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XeSUheukML2C"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "colab_type": "code",
    "id": "qRGlnSXhWqwd",
    "outputId": "c6b35217-0e25-49cb-b932-2df6097975fc"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "bert_BASE_Seq = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "_ = bert_BASE_Seq.train()\n",
    "_ = bert_BASE_Seq.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FmyV1mUIQNqf"
   },
   "outputs": [],
   "source": [
    "class Test_Dateset(Dataset):\n",
    "    def __init__(self, max_num=5000, max_seq=512, vocab_size=30522, label_num=2):\n",
    "        self.max_num = max_num\n",
    "        self.max_seq = max_seq\n",
    "        self.vocab_size = vocab_size\n",
    "        self.label_num = label_num\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.max_num\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = torch.randint(0,self.vocab_size,(self.max_seq,), device='cuda').unsqueeze(0)\n",
    "        labels = torch.randint(0,self.label_num-1,(1,), device='cuda')\n",
    "        return {'input_ids':data,\n",
    "                'labels':labels}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cuda'): \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, _tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].squeeze(1).to(device)\n",
    "        yield out_data_dict\n",
    "\n",
    "def compute_accuracy(logits, y_target):\n",
    "    _, logits_indices = logits.max(dim=1)\n",
    "    n_correct = torch.eq(logits_indices, y_target).sum().item()\n",
    "    return n_correct / len(logits_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8AzMneFxTiND"
   },
   "outputs": [],
   "source": [
    "dateset_01 = Test_Dateset(max_seq=200)\n",
    "\n",
    "test_logits = torch.randn([5,2])\n",
    "test_labels = torch.randint(0,1,[5,1])\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tTwOXJgIczbX",
    "outputId": "98f112b3-e114-49eb-fdfa-79d52828d10e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.squeeze_(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bPfg1jvwcPaB",
    "outputId": "059714c1-71c7-4527-8817-09ef6dfe3179"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7182)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(test_logits,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "ngztNDp4b4K6",
    "outputId": "422e4b3a-5f4c-492d-9fe3-e2567991bd82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7058, device='cuda:0', grad_fn=<NllLossBackward>),\n",
       " tensor([[0.2318, 0.2569]], device='cuda:0', grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_BASE_Seq(**dateset_01.__getitem__(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正常训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426,
     "referenced_widgets": [
      "8448acdc3f2d499a8e74e83017ddc598",
      "a4cc6cfa11c94e618977fca5052abce3",
      "bfda5d69d2814342a529fc0bf468006d",
      "a8d53ff93e564331a353d83c4fd3766e",
      "1c3f72e51a9d4839a0d1258e207407d2",
      "50f8e08509dc446a9f0f9a1b1a303cba",
      "4521ff8c574f4f978f44506256752762",
      "23a49623c1f844f581c3db47170057e4"
     ]
    },
    "colab_type": "code",
    "id": "AYIe6pASQNtc",
    "outputId": "56bb5e8d-f13f-43bd-a581-9d827fb8ee12",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 26 #  10311MiB / 11019MiB\n",
    "epoch_bar = tqdm_notebook(desc='training routine',\n",
    "                total=dateset_01.get_num_batches(batch_size),\n",
    "                position=0)\n",
    "optimizer = optim.Adam(bert_BASE_Seq.parameters(), 0.001)\n",
    "\n",
    "for i,batch_dict in enumerate(generate_batches(dateset_01,batch_size)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_target = batch_dict.pop('labels')\n",
    "    res_tuple = bert_BASE_Seq(**batch_dict)\n",
    "    logits = res_tuple[0]\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()(logits, y_target)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    acc = compute_accuracy(logits, y_target)\n",
    "\n",
    "    epoch_bar.set_postfix(loss=loss.item(), acc=acc, batch=i)\n",
    "    epoch_bar.update()\n",
    "\n",
    "# 192/192 [01:13<00:00, 2.60it/s, acc=100, batch=191, loss=1e-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a2kKKcoEMK6R"
   },
   "outputs": [],
   "source": [
    "for i in bert_BASE_Seq.parameters():\n",
    "    print(i.data.shape)\n",
    "    print(i.requires_grad)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AVwIXqcAQngn"
   },
   "source": [
    "## 冻结模型\n",
    "\n",
    "以`bert-base-uncased`为例.\n",
    "\n",
    "- `model.parameters()`\n",
    "    - 返回tensor的子类, 有data和requires_grad.\n",
    "    - 直接设置grad可以不用返回model.\n",
    "\n",
    "\n",
    "- `model.state_dict()`\n",
    "    - 返回有序字典\n",
    "    - key是该层的name.\n",
    "    \n",
    "\n",
    "- `model.children()`\n",
    "    - 树状结构. root下有3个children.\n",
    "    1. 11个attention层.\n",
    "    2. Dropout(p=0.1, inplace=False)\n",
    "    3. Linear(in_features=768, out_features=2, bias=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0102, -0.0615, -0.0265,  ..., -0.0199, -0.0372, -0.0098],\n",
      "        [-0.0117, -0.0600, -0.0323,  ..., -0.0168, -0.0401, -0.0107],\n",
      "        [-0.0198, -0.0627, -0.0326,  ..., -0.0165, -0.0420, -0.0032],\n",
      "        ...,\n",
      "        [-0.0218, -0.0556, -0.0135,  ..., -0.0043, -0.0151, -0.0249],\n",
      "        [-0.0462, -0.0565, -0.0019,  ...,  0.0157, -0.0139, -0.0095],\n",
      "        [ 0.0015, -0.0821, -0.0160,  ..., -0.0081, -0.0475,  0.0753]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for index, layer in enumerate(bert_BASE_Seq.parameters()):\n",
    "    print(layer)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 返回orderDict. k,v分别是名称和tensor.\n",
    "FLAG_no_grad = True\n",
    "for index, (key, value) in enumerate(bert_BASE_Seq.state_dict().items()):\n",
    "    if FLAG_no_grad: value.requires_grad_ = True\n",
    "    if key.startswith('bert.encoder.layer.10'): FLAG_no_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight False\n"
     ]
    }
   ],
   "source": [
    "for (k, v) in bert_BASE_Seq.state_dict().items():\n",
    "    print(k ,v.requires_grad)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 False\n",
      "1 False\n",
      "2 True\n",
      "3 True\n"
     ]
    }
   ],
   "source": [
    "# torch.nn.parameter.Parameter  are Tensor subclasses\n",
    "# data and requires_grad \n",
    "# 方法可行\n",
    "for index, i in enumerate(bert_BASE_Seq.parameters()):\n",
    "    if index > 1 :\n",
    "        i.requires_grad = True\n",
    "    else:\n",
    "        i.requires_grad = False\n",
    "    \n",
    "    if index > 4: break\n",
    "    \n",
    "\n",
    "for index, i in enumerate(bert_BASE_Seq.parameters()):\n",
    "    if index > 3: break\n",
    "    print(index, i.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_to_layer(model, layer_name):\n",
    "    '''冻结层. 从0到layer_name.'''\n",
    "    \n",
    "    if layer_name == 'all':\n",
    "        index_start = len(model.state_dict())\n",
    "    else:\n",
    "        index_start = -1\n",
    "        for index, (key, _value) in enumerate(model.state_dict().items()):\n",
    "            if key.startswith(layer_name): \n",
    "                index_start = index\n",
    "                break\n",
    "\n",
    "    if index_start < 0:\n",
    "        print(f\"Don't find layer name: {layer_name}\")\n",
    "        return\n",
    "    \n",
    "    no_grad_nums = index_start + 1\n",
    "    grad_nums = 0\n",
    "\n",
    "    for index, i in enumerate(model.parameters()):\n",
    "        if index >= index_start:\n",
    "            i.requires_grad = True\n",
    "            grad_nums += 1\n",
    "        else:\n",
    "            i.requires_grad = False\n",
    "    \n",
    "    print(f\"freeze layers num: {no_grad_nums}, active layers num: {grad_nums}.\")\n",
    "    # no need to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freeze layers num: 166, active layers num: 36.\n"
     ]
    }
   ],
   "source": [
    "freeze_to_layer(bert_BASE_Seq,'bert.encoder.layer.10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e50612a46b9434f8dc063fc614ee97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training routine', max=39, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128 # 几乎极限 10741MiB / 11019MiB\n",
    "epoch_bar = tqdm_notebook(desc='training routine',\n",
    "                total=dateset_01.get_num_batches(batch_size),\n",
    "                position=0)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, bert_BASE_Seq.parameters()), 0.001)\n",
    "\n",
    "for i,batch_dict in enumerate(generate_batches(dateset_01,batch_size)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_target = batch_dict.pop('labels')\n",
    "    res_tuple = bert_BASE_Seq(**batch_dict)\n",
    "    logits = res_tuple[0]\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()(logits, y_target)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    acc = compute_accuracy(logits, y_target)\n",
    "\n",
    "    epoch_bar.set_postfix(loss=loss.item(), acc=acc, batch=i)\n",
    "    epoch_bar.update()\n",
    "\n",
    "# 39/39 [00:29<00:00, 1.32it/s, acc=100, batch=38, loss=2.76e-7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WeEIAxp34vtj"
   },
   "source": [
    "## 半精度float16(无冻结)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "colab_type": "code",
    "id": "lZ0FWp2ZBOl-",
    "outputId": "2d5c3cf5-49da-4b20-fefe-10cd9a2cee20"
   },
   "source": [
    "git clone https://github.com/NVIDIA/apex\n",
    "cd apex\n",
    "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lhWBiyAmMLBQ"
   },
   "outputs": [],
   "source": [
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training routine', max=156, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # 9527MiB / 11019MiB\n",
    "epoch_bar = tqdm_notebook(desc='training routine',\n",
    "                total=dateset_01.get_num_batches(batch_size),\n",
    "                position=0)\n",
    "optimizer = optim.Adam(bert_BASE_Seq.parameters(), 0.001)\n",
    "\n",
    "# add.\n",
    "bert_BASE_Seq, optimizer = amp.initialize(bert_BASE_Seq, optimizer)\n",
    "\n",
    "for i,batch_dict in enumerate(generate_batches(dateset_01,batch_size)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_target = batch_dict.pop('labels')\n",
    "    res_tuple = bert_BASE_Seq(**batch_dict)\n",
    "    logits = res_tuple[0]\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()(logits, y_target)\n",
    "\n",
    "    # add.\n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    acc = compute_accuracy(logits, y_target)\n",
    "\n",
    "    epoch_bar.set_postfix(loss=loss.item(), acc=acc, batch=i)\n",
    "    epoch_bar.update()\n",
    "    \n",
    "# 156/156 [00:40<00:00, 3.84it/s, acc=100, batch=155, loss=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## float16+冻结模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freeze layers num: 166, active layers num: 36.\n"
     ]
    }
   ],
   "source": [
    "def freeze_to_layer(model, layer_name):\n",
    "    '''冻结层. 从0到layer_name.'''\n",
    "    index_start = -1\n",
    "    for index, (key, _value) in enumerate(model.state_dict().items()):\n",
    "        if key.startswith(layer_name): \n",
    "            index_start = index\n",
    "            break\n",
    "    \n",
    "    if index_start < 0:\n",
    "        print(f\"Don't find layer name: {layer_name}\")\n",
    "        return\n",
    "    \n",
    "    no_grad_nums = index_start + 1\n",
    "    grad_nums = 0\n",
    "\n",
    "    for index, i in enumerate(model.parameters()):\n",
    "        if index >= index_start:\n",
    "            i.requires_grad = True\n",
    "            grad_nums += 1\n",
    "        else:\n",
    "            i.requires_grad = False\n",
    "    \n",
    "    print(f\"freeze layers num: {no_grad_nums}, active layers num: {grad_nums}.\")\n",
    "    # no need to return.\n",
    "freeze_to_layer(bert_BASE_Seq,'bert.encoder.layer.10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lhWBiyAmMLBQ"
   },
   "outputs": [],
   "source": [
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b118842bcd634bdfbaf97b9321d828e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training routine', max=23, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    }
   ],
   "source": [
    "batch_size = 212 # 10931MiB / 11019MiB\n",
    "epoch_bar = tqdm_notebook(desc='training routine',\n",
    "                total=dateset_01.get_num_batches(batch_size),\n",
    "                position=0)\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, bert_BASE_Seq.parameters()), 0.001)\n",
    "\n",
    "# add.\n",
    "bert_BASE_Seq, optimizer = amp.initialize(bert_BASE_Seq, optimizer)\n",
    "\n",
    "for i,batch_dict in enumerate(generate_batches(dateset_01,batch_size)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_target = batch_dict.pop('labels')\n",
    "    res_tuple = bert_BASE_Seq(**batch_dict)\n",
    "    logits = res_tuple[0]\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()(logits, y_target)\n",
    "\n",
    "    # add.\n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    acc = compute_accuracy(logits, y_target)\n",
    "\n",
    "    epoch_bar.set_postfix(loss=loss.item(), acc=acc, batch=i)\n",
    "    epoch_bar.update()\n",
    "    \n",
    "# 23/23 [00:13<00:00, 1.66it/s, acc=100, batch=22, loss=4.5e-9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分布式训练\n",
    "\n",
    "需要使用命令行."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJ9MG6tN3ix7"
   },
   "source": [
    "## GPU使用情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NMLaIasMK8t"
   },
   "outputs": [],
   "source": [
    "def print_gpu_info(_print = True):\n",
    "    if not _print: return None\n",
    "    gpu_info = !nvidia-smi\n",
    "    gpu_info = '\\n'.join(gpu_info)\n",
    "    if gpu_info.find('failed') >= 0:\n",
    "        print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "        print('and then re-execute this cell.')\n",
    "    else:\n",
    "        print(gpu_info)\n",
    "\n",
    "# 参数量\n",
    "import numpy as np\n",
    "def model_parameters(model, type_size=4):\n",
    "    para = sum([np.prod(list(p.size())) for p in model.parameters()])\n",
    "    print('Model {} : params: {:4f}M'.format(model._get_name(), para * type_size / 1000 / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model BertForSequenceClassification : params: 437.935112M\n"
     ]
    }
   ],
   "source": [
    "model_parameters(bert_BASE_Seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "bZiX9Up7MLFb",
    "outputId": "a47ab6da-098b-4724-9a9b-b9e5128c3d5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar  8 00:51:22 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.36       Driver Version: 440.36       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:19:00.0 Off |                  N/A |\n",
      "| 45%   47C    P2    45W / 250W |  10075MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 27%   33C    P8     4W / 250W |     12MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:67:00.0 Off |                  N/A |\n",
      "| 27%   34C    P8    12W / 250W |     12MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:68:00.0 Off |                  N/A |\n",
      "| 27%   35C    P8    20W / 250W |    150MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     48853      C   /root/anaconda3/bin/python                 10063MiB |\n",
      "|    3     12973      G   /usr/bin/X                                    80MiB |\n",
      "|    3     14954      G   /usr/bin/gnome-shell                          56MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print_gpu_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dueGIb2T338j"
   },
   "source": [
    "### 释放显存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "3a3hMLxjMLD3",
    "outputId": "05b1e8d1-d9b5-4d9a-bf5c-07bd3d23659a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar  8 00:51:24 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.36       Driver Version: 440.36       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:19:00.0 Off |                  N/A |\n",
      "| 45%   47C    P2    53W / 250W |   2679MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 27%   33C    P8     5W / 250W |     12MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:67:00.0 Off |                  N/A |\n",
      "| 27%   35C    P8    12W / 250W |     12MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:68:00.0 Off |                  N/A |\n",
      "| 27%   35C    P8    20W / 250W |    150MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     48853      C   /root/anaconda3/bin/python                  2667MiB |\n",
      "|    3     12973      G   /usr/bin/X                                    80MiB |\n",
      "|    3     14954      G   /usr/bin/gnome-shell                          56MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print_gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "my_config = AutoConfig.from_pretrained(model_name)\n",
    "model_bert = AutoModel.from_config(my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model_bert.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Dateset(Dataset):\n",
    "    def __init__(self, max_num=5000, max_seq=512, vocab_size=30522):\n",
    "        self.max_num = max_num\n",
    "        self.max_seq = max_seq\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.max_num\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = torch.randint(0,self.vocab_size,(self.max_seq,), device='cuda').unsqueeze(0)\n",
    "        return {'input_ids':data}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateset_02 = Test_Dateset(max_seq=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state , pooler_output  = model_bert(**dateset_02.__getitem__(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooler_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义self-attention encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给定变长features 返回padding和mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": [
     16,
     29,
     39,
     54,
     66,
     99,
     110,
     119
    ]
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        # 第二个参数是匿名函数 -- 因为sublayer第二个参数直接拿来调用.\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "    \n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        # mask == 0是boolen矩阵, 标明哪些位置是0. 这里就是直接对mask为true的部分取-1e9.\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        # -1应该是句子长度.\n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "    \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "    \n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # 不再求导.\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "def make_model(N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = make_model(3, d_model=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 封装\n",
    "def build_padding(batch_seq_dim, seq_max = 19):\n",
    "    x_padding = []\n",
    "    x_mask = []\n",
    "    for t in batch_seq_dim:\n",
    "        tokens_len = t.shape[-2]\n",
    "        x_mask.append([1]*tokens_len + [0]*(seq_max - tokens_len))\n",
    "        pd = (0,0,0, seq_max - tokens_len)\n",
    "        x_padding.append(F.pad(t, pd ,\"constant\", 0))\n",
    "\n",
    "    x_padding = torch.cat(x_padding, dim=0)\n",
    "    x_mask = torch.tensor(x_mask).unsqueeze(-2)\n",
    "    \n",
    "    return x_padding, x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_max = 512\n",
    "batch_seq_dim = [torch.randn((1,np.random.randint(0,seq_max),768)) for i in range(10)]\n",
    "x_test, x_mask = build_padding(batch_seq_dim, seq_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_res = tmp_model(x_test, x_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512, 768])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义bert-model\n",
    "\n",
    "使用`config`自定义参数, 不使用预训练模型. 传入的是mask的`[bsz, seq, dim]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel, BertConfig\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_config = BertConfig(max_length=1024,\n",
    "                      num_hidden_layers = 3,\n",
    "                      num_attention_heads = 8,\n",
    "                      max_position_embeddings = 1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bert = AutoModel.from_config(my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = {'inputs_embeds':x_test,\n",
    "               'attention_mask':x_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state, pooler_output = model_bert(**model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1024, 768])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 768])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooler_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 试验: tokenizer和BERT-NER\n",
    "\n",
    "使用XLNET做分词,使用BERT-NER进行命名实体识别. **不行!!**. BERT-NER本身最长输入就是512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlnet-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990bb3f220954a61a4876d62520b359f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=690, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "proxies={\"http_proxy\": \"127.0.0.1:10802\",\n",
    "\"https_proxy\": \"127.0.0.1:10802\"}\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name, proxies=proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "model_NER = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "                                                           proxies = proxies)\n",
    "\n",
    "tokenizer_XLNET = AutoTokenizer.from_pretrained(model_name,proxies = proxies)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-cased-whole-word-masking\",\n",
    "                                         proxies = proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sen_in_train = \"\"\"The family first appeared in 2006 and the family have been involved in a number of the show's most high-profile storylines, most notably John Paul McQueen's (James Sutton) affair with Craig Dean (Guy Burnet); Jacqui McQueen's (Claire Cooper) whirlwind relationship with Tony Hutchinson (Nick Pickard); Myra McQueen's (Nicole Barber-Lane) long-lost son Niall Rafferty's (Barry Sloane) revenge on his family by holding them hostage in an abandoned church and blowing it up, ultimately killing his half-sister Tina Reilly (Leah Hackett); Theresa McQueen's (Jorgie Porter) pregnancy by her cousin Carmel McQueen's (Gemma Merna) fiancé Calvin Valentine (Ricky Whittle) and later shooting him dead on their wedding day; Mercedes McQueen's (Jennifer Metcalfe) affair with her fiancé Riley Costello's (Rob Norbuy) father Carl (Paul Opacic); being kidnapped by Riley's grandfather Silas; staking Riley's second cousin Mitzee Minniver; Jacqui coping with the death of her husband Rhys Ashworth (Andrew Moss) in a bus crash, learning that he had been having an affair with Cindy Cunningham (Stephanie Waring) and that he got Sinead O'Connor (Stephanie Davis) pregnant; Mercedes stalking Mitzeee (Rachel Shenton) and stabbing herself and framing her; Carmel's facial disfigurement; Myra faking her own death to escape her daughter Mercedes' evil husband, Dr. Paul Browning (Joseph Thompson); Mercedes killing her husband Doctor Browning by striking him over the head with a shovel; John Paul's male rape at the hands of his pupil Finn O'Connor (Keith Rice); the train crash which ultimately killed Carmel; Mercedes faking her death to help Grace Black (Tamara Wall) get revenge on Freddie Roscoe (Charlie Clapham); Theresa donating her kidney to Nico Blake (Persephone Swales-Dawson); Porsche (Twinnie Lee Moore) and Cleo McQueen's (Nadine Rose Mulkerrin) sexual abuse at the hands of their mother Reenie McQueen's (Zöe Lucker) fiancé Pete Buchanan (Kai Owen); Phoebe McQueen's (Mandip Gill) murder in hospital by the Gloved Hand Killer; the stillbirth of Mercedes' baby Gabriel McQueen; John Paul's transgender boss Sally St. Claire (Annie Wallace) being revealed as his biological father, Mercedes being framed for drugs by Joanne Cardsley (Rachel Leskovac), Celine McQueen (Sarah George) and Diego Salvador Martinez Hernandez De La Cruz (Juan Pablo Yepez)'s sham wedding for money and Celine being murdered by her ex-boyfriend and serial killer Cameron Campbell (Cameron Moore) after discover he causes the fire at the fair on Halloween 2016.\"\"\"\n",
    "max_p = 'Loan modification is the systematic alteration of mortgage loan agreements that help those having problems making the payments by reducing interest rates, monthly payments or principal balances. Lending institutions could make one or more of these changes to relieve financial pressure on borrowers to prevent the condition of foreclosure. Loan modifications have been practiced in the United States since The 2008 Crash Of The Housing Market from Washington Mutual, Chase Home Finance, Chase, JP Morgan & Chase, other contributors like MER\\'s. Crimes of Mortgage ad Real Estate Staff had long assisted nd finally the squeaky will could not continue as their deviant practices broke the state and crashed. Modification owners either ordered by The United States Department of Housing, The United States IRS or President Obamas letters from Note Holders came to those various departments asking for the Democratic process to help them keep their homes and protection them from explosion. Thus the birth of Modifications. It is yet to date for clarity how theses enforcements came into existence and except b whom, but t is certain that note holders form the Midwest reached out in the Democratic Process for assistance. FBI Mortgage Fraud Department came into existence. Modifications HMAP HARP were also birthed to help note holders get Justice through reduced mortgage by making terms legal. Modification of mortgage terms was introduced by IRS staff addressing the crisis called the HAMP TEAMS that went across the United States desiring the new products to assist homeowners that were victims of predatory lending practices, unethical staff, brokers, attorneys and lenders that contributed to the crash. Modification were a fix to the crash as litigation has ensued as the lenders reorganized and renamed the lending institutions and government agencies are to closely monitor them. Prior to modifications loan holders that experiences crisis would use Loan assumptions and Loan transfers to keep the note in the 1930s. During the Great Depression, loan transfers, loan assumption, and loan bail out programs took place at the state level in an effort to reduce levels of loan foreclosures while the Federal Bureau of Investigation, Federal Trade Commission, Comptroller, the United States Government and State Government responded to lending institution violations of law in these arenas by setting public court records that are legal precedence of such illegal actions. The legal precedents and reporting agencies were created to address the violations of laws to consumers while the Modifications were created to assist the consumers that are victims of predatory lending practices. During the so-called \"Great Recession\" of the early 21st century, loan modification became a matter of national policy, with various actions taken to alter mortgage loan terms to prevent further economic destabilization. Due to absorbent personal profits nothing has been done to educate Homeowners or Creditors that this money from equity, escrow is truly theirs the Loan Note Holder and it is their monetary rights as the real prize and reason for the Housing Crash was the profit n obtaining the mortgage holders Escrow. The Escrow and Equity that is accursed form the Note Holders payments various staff through the United States claimed as recorded and cashed by all staff in real-estate from local residential Tax Assessing Staff, Real Estate Staff, Ordinance Staff, Police Staff, Brokers, attorneys, lending institutional staff but typically Attorneys who are also typically the owners or Rental properties that are trained through Bankruptcies\\'. that collect the Escrow that is rightfully the Homeowners but because most Homeowners are unaware of what money is due them and how they can loose their escrow. Most Creditors are unaware that as the note holder that the Note Holder are due a annual or semi annual equity check and again bank or other lending and or legal intuitions staff claim this monies instead. This money Note Holders were unaware of is the prize of real estate and the cause of the Real Estate Crash of 2008 where Lending Institutions provided mortgages to people years prior they know they would eventually loose with Loan holders purchasing Balloon Mortgages lending product that is designed to make fast money off the note holder whom is always typically unaware of their escrow, equity and that are further victimized by conferences and books on HOW TO MAKE MONEY IN REAL STATE - when in fact the money is the Note Holder. The key of the crash was not the House, but the loan product used and the interest and money that was accrued form the note holders that staff too immorally. The immoral and illegal actions of predatory lending station and their staff began with the inception of balloon mortgages although illegal activity has always existed in the arena, yet the crash created \"Watch Dog\" like HAMP TEAM, IRS, COMPTROLLER< Federal Trade Commission Consumer Protection Bureau, FBI, CIA, Local Police Department, ICE ( The FBI online Computer crime division receives and investigates computer crimes that record keeping staff from title companies, lending institutional staff, legal staff and others created fraudulent documents to change payments and billing of note holders to obtain the money note holders are typically unaware of) and other watch dog agencies came into existence to examine if houses were purchased through a processed check at Government Debited office as many obtained free homes illegally. Many were incarcerated for such illegal actions. Modifications fixed the Notes to proper lower interest, escrow, tax fees that staff typically raised for no reason. Many people from various arenas involved in reals estate have been incarcerated for these actions as well as other illegal actions like charging for a modification. Additionally Modifications were also made to address the falsifications such as inappropriate mortgage charges, filing of fraudulently deeds, reporting of and at times filing of fraudulent mortgages that were already paid off that were fraudulently continued by lenders staff and attorneys or brokers or anyone in the Real Estate Chain through the issues of real estate terms to continue to violate United States Laws, contract law and legal precedence where collusion was often done again to defraud and steal from the Note Holder was such a common practice that was evidence as to why the Mortgage Crash in 2008 occurred for the purpose of wining the prize of stealing form Homeowners and those that foreclosed was actually often purposefully for these monies note holders were unaware of to be obtained which was why Balloon mortgages and loans were given to the staff in the Real Estate Market with the hoper and the expectation that the loan holders would default as it offered opportunity to commit illegal transactions of obtaining the homeowners funds. While such scams were addressed through modifications in 2008. The Market relied heavily on Consumers ignorance to prosper, ignorance of real estate terms, ignorance on what they were to be charged properly for unethical financial gain and while staff in real estates lending arenas mingled terms to deceive y deliberate confusion consumers out of cash and homes while the USA Government provided Justice through President Obamas Inception and IRS Inception of Modifications which addressed these unethical profits in Reals Estate. It was in 2009 that HARP, HAMP and Modifications were introduced to stop the victimization of Note Holders. Taking on the Banks that ran USA Government was a great and dangerous undertaking that made America Great Again as Justice for Consumers reigned. Legal action taken against institutions that have such business practices can be viewed in State Code of Law and Federal Law on precedent cases that are available to the public. Finally, It had been unlawful to be charged by an attorney to modify as well as fro banking staff to modify terms to increase a mortgage and or change lending product to a balloon in an concerted effort to make homeowner foreclose which is also illegal, computer fraud and not the governments intended purpose or definition of a modification.'\n",
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer_XLNET.tokenize(max_sen_in_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ' '.join([i.replace( '▁', '') for i in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def find_NER_in_Model(content_raw, tokens=None, model=model_NER, tokenizer = tokenizer):\n",
    "    '''返回: tokens[1:-1], inputs, entities_list\n",
    "    第一个是不包含[CLS] [SEP]的分词序列(content_tokens是包含的).\n",
    "    第二个是实体列表.'''\n",
    "    label_list = [\n",
    "        \"O\",       # Outside of a named entity\n",
    "        \"B-MISC\",  # Beginning of a miscellaneous entity right after another miscellaneous entity\n",
    "        \"I-MISC\",  # Miscellaneous entity\n",
    "        \"B-PER\",   # Beginning of a person's name right after another person's name\n",
    "        \"I-PER\",   # Person's name\n",
    "        \"B-ORG\",   # Beginning of an organisation right after another organisation\n",
    "        \"I-ORG\",   # Organisation\n",
    "        \"B-LOC\",   # Beginning of a location right after another location\n",
    "        \"I-LOC\"    # Location\n",
    "        ]\n",
    "    # Bit of a hack to get the tokens with the special tokens\n",
    "    if not tokens: tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(content_raw)))\n",
    "    inputs = tokenizer.encode(content_raw, return_tensors='pt').to(DEVICE)\n",
    "    outputs = model_NER(inputs)[0]\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    # 去除 [cls] [sep]\n",
    "    res = [(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].tolist())][1:-1]\n",
    "    # print(predictions)\n",
    "    # print(res)\n",
    "\n",
    "    entities_list = []\n",
    "    cursor_1 = 0\n",
    "    cursor_2 = 0\n",
    "\n",
    "    while cursor_1 < len(res):\n",
    "        entities_dict = {}\n",
    "        temp = []\n",
    "        if res[cursor_1][1] == 'O': \n",
    "            cursor_1+=1\n",
    "            continue\n",
    "        \n",
    "        entities_dict['type'] = res[cursor_1][1]\n",
    "        entities_dict['span_start'] = cursor_1\n",
    "        temp.append(res[cursor_1][0])\n",
    "        cursor_2 = cursor_1 + 1\n",
    "        while cursor_2 < len(res):\n",
    "            if res[cursor_2][1] == 'O':\n",
    "                cursor_1 = cursor_2 + 1\n",
    "                break\n",
    "            \n",
    "            temp.append(res[cursor_2][0])\n",
    "            cursor_2 += 1\n",
    "        \n",
    "        cursor_1 += cursor_2\n",
    "        entities_dict['content'] = ' '.join(temp).replace(' ##', '')\n",
    "        entities_dict['content_tokens'] = temp\n",
    "        entities_list.append(entities_dict)\n",
    "\n",
    "    return tokens[1:-1], entities_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "find_NER_in_Model(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 试验: tokenizer和Spacy-NER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "tokenizer_XLNET = AutoTokenizer.from_pretrained(model_name,proxies = proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sen_in_train = \"\"\"The family first appeared in 2006 and the family have been involved in a number of the show's most high-profile storylines, most notably John Paul McQueen's (James Sutton) affair with Craig Dean (Guy Burnet); Jacqui McQueen's (Claire Cooper) whirlwind relationship with Tony Hutchinson (Nick Pickard); Myra McQueen's (Nicole Barber-Lane) long-lost son Niall Rafferty's (Barry Sloane) revenge on his family by holding them hostage in an abandoned church and blowing it up, ultimately killing his half-sister Tina Reilly (Leah Hackett); Theresa McQueen's (Jorgie Porter) pregnancy by her cousin Carmel McQueen's (Gemma Merna) fiancé Calvin Valentine (Ricky Whittle) and later shooting him dead on their wedding day; Mercedes McQueen's (Jennifer Metcalfe) affair with her fiancé Riley Costello's (Rob Norbuy) father Carl (Paul Opacic); being kidnapped by Riley's grandfather Silas; staking Riley's second cousin Mitzee Minniver; Jacqui coping with the death of her husband Rhys Ashworth (Andrew Moss) in a bus crash, learning that he had been having an affair with Cindy Cunningham (Stephanie Waring) and that he got Sinead O'Connor (Stephanie Davis) pregnant; Mercedes stalking Mitzeee (Rachel Shenton) and stabbing herself and framing her; Carmel's facial disfigurement; Myra faking her own death to escape her daughter Mercedes' evil husband, Dr. Paul Browning (Joseph Thompson); Mercedes killing her husband Doctor Browning by striking him over the head with a shovel; John Paul's male rape at the hands of his pupil Finn O'Connor (Keith Rice); the train crash which ultimately killed Carmel; Mercedes faking her death to help Grace Black (Tamara Wall) get revenge on Freddie Roscoe (Charlie Clapham); Theresa donating her kidney to Nico Blake (Persephone Swales-Dawson); Porsche (Twinnie Lee Moore) and Cleo McQueen's (Nadine Rose Mulkerrin) sexual abuse at the hands of their mother Reenie McQueen's (Zöe Lucker) fiancé Pete Buchanan (Kai Owen); Phoebe McQueen's (Mandip Gill) murder in hospital by the Gloved Hand Killer; the stillbirth of Mercedes' baby Gabriel McQueen; John Paul's transgender boss Sally St. Claire (Annie Wallace) being revealed as his biological father, Mercedes being framed for drugs by Joanne Cardsley (Rachel Leskovac), Celine McQueen (Sarah George) and Diego Salvador Martinez Hernandez De La Cruz (Juan Pablo Yepez)'s sham wedding for money and Celine being murdered by her ex-boyfriend and serial killer Cameron Campbell (Cameron Moore) after discover he causes the fire at the fair on Halloween 2016.\"\"\"\n",
    "max_p = 'Loan modification is the systematic alteration of mortgage loan agreements that help those having problems making the payments by reducing interest rates, monthly payments or principal balances. Lending institutions could make one or more of these changes to relieve financial pressure on borrowers to prevent the condition of foreclosure. Loan modifications have been practiced in the United States since The 2008 Crash Of The Housing Market from Washington Mutual, Chase Home Finance, Chase, JP Morgan & Chase, other contributors like MER\\'s. Crimes of Mortgage ad Real Estate Staff had long assisted nd finally the squeaky will could not continue as their deviant practices broke the state and crashed. Modification owners either ordered by The United States Department of Housing, The United States IRS or President Obamas letters from Note Holders came to those various departments asking for the Democratic process to help them keep their homes and protection them from explosion. Thus the birth of Modifications. It is yet to date for clarity how theses enforcements came into existence and except b whom, but t is certain that note holders form the Midwest reached out in the Democratic Process for assistance. FBI Mortgage Fraud Department came into existence. Modifications HMAP HARP were also birthed to help note holders get Justice through reduced mortgage by making terms legal. Modification of mortgage terms was introduced by IRS staff addressing the crisis called the HAMP TEAMS that went across the United States desiring the new products to assist homeowners that were victims of predatory lending practices, unethical staff, brokers, attorneys and lenders that contributed to the crash. Modification were a fix to the crash as litigation has ensued as the lenders reorganized and renamed the lending institutions and government agencies are to closely monitor them. Prior to modifications loan holders that experiences crisis would use Loan assumptions and Loan transfers to keep the note in the 1930s. During the Great Depression, loan transfers, loan assumption, and loan bail out programs took place at the state level in an effort to reduce levels of loan foreclosures while the Federal Bureau of Investigation, Federal Trade Commission, Comptroller, the United States Government and State Government responded to lending institution violations of law in these arenas by setting public court records that are legal precedence of such illegal actions. The legal precedents and reporting agencies were created to address the violations of laws to consumers while the Modifications were created to assist the consumers that are victims of predatory lending practices. During the so-called \"Great Recession\" of the early 21st century, loan modification became a matter of national policy, with various actions taken to alter mortgage loan terms to prevent further economic destabilization. Due to absorbent personal profits nothing has been done to educate Homeowners or Creditors that this money from equity, escrow is truly theirs the Loan Note Holder and it is their monetary rights as the real prize and reason for the Housing Crash was the profit n obtaining the mortgage holders Escrow. The Escrow and Equity that is accursed form the Note Holders payments various staff through the United States claimed as recorded and cashed by all staff in real-estate from local residential Tax Assessing Staff, Real Estate Staff, Ordinance Staff, Police Staff, Brokers, attorneys, lending institutional staff but typically Attorneys who are also typically the owners or Rental properties that are trained through Bankruptcies\\'. that collect the Escrow that is rightfully the Homeowners but because most Homeowners are unaware of what money is due them and how they can loose their escrow. Most Creditors are unaware that as the note holder that the Note Holder are due a annual or semi annual equity check and again bank or other lending and or legal intuitions staff claim this monies instead. This money Note Holders were unaware of is the prize of real estate and the cause of the Real Estate Crash of 2008 where Lending Institutions provided mortgages to people years prior they know they would eventually loose with Loan holders purchasing Balloon Mortgages lending product that is designed to make fast money off the note holder whom is always typically unaware of their escrow, equity and that are further victimized by conferences and books on HOW TO MAKE MONEY IN REAL STATE - when in fact the money is the Note Holder. The key of the crash was not the House, but the loan product used and the interest and money that was accrued form the note holders that staff too immorally. The immoral and illegal actions of predatory lending station and their staff began with the inception of balloon mortgages although illegal activity has always existed in the arena, yet the crash created \"Watch Dog\" like HAMP TEAM, IRS, COMPTROLLER< Federal Trade Commission Consumer Protection Bureau, FBI, CIA, Local Police Department, ICE ( The FBI online Computer crime division receives and investigates computer crimes that record keeping staff from title companies, lending institutional staff, legal staff and others created fraudulent documents to change payments and billing of note holders to obtain the money note holders are typically unaware of) and other watch dog agencies came into existence to examine if houses were purchased through a processed check at Government Debited office as many obtained free homes illegally. Many were incarcerated for such illegal actions. Modifications fixed the Notes to proper lower interest, escrow, tax fees that staff typically raised for no reason. Many people from various arenas involved in reals estate have been incarcerated for these actions as well as other illegal actions like charging for a modification. Additionally Modifications were also made to address the falsifications such as inappropriate mortgage charges, filing of fraudulently deeds, reporting of and at times filing of fraudulent mortgages that were already paid off that were fraudulently continued by lenders staff and attorneys or brokers or anyone in the Real Estate Chain through the issues of real estate terms to continue to violate United States Laws, contract law and legal precedence where collusion was often done again to defraud and steal from the Note Holder was such a common practice that was evidence as to why the Mortgage Crash in 2008 occurred for the purpose of wining the prize of stealing form Homeowners and those that foreclosed was actually often purposefully for these monies note holders were unaware of to be obtained which was why Balloon mortgages and loans were given to the staff in the Real Estate Market with the hoper and the expectation that the loan holders would default as it offered opportunity to commit illegal transactions of obtaining the homeowners funds. While such scams were addressed through modifications in 2008. The Market relied heavily on Consumers ignorance to prosper, ignorance of real estate terms, ignorance on what they were to be charged properly for unethical financial gain and while staff in real estates lending arenas mingled terms to deceive y deliberate confusion consumers out of cash and homes while the USA Government provided Justice through President Obamas Inception and IRS Inception of Modifications which addressed these unethical profits in Reals Estate. It was in 2009 that HARP, HAMP and Modifications were introduced to stop the victimization of Note Holders. Taking on the Banks that ran USA Government was a great and dangerous undertaking that made America Great Again as Justice for Consumers reigned. Legal action taken against institutions that have such business practices can be viewed in State Code of Law and Federal Law on precedent cases that are available to the public. Finally, It had been unlawful to be charged by an attorney to modify as well as fro banking staff to modify terms to increase a mortgage and or change lending product to a balloon in an concerted effort to make homeowner foreclose which is also illegal, computer fraud and not the governments intended purpose or definition of a modification.'\n",
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer_XLNET.tokenize(max_sen_in_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ' '.join([i.replace( '▁', '') for i in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8to-ov_oZ4Zm"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "# spacy.prefer_cpu()\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# 使用BERT进行fine-turning,文本也应该使用bert进行分词,以确保embedding的一致性.\n",
    "# 这要求ner模块能够接受分好词的list,而不是自己进行分词,否则实体span获取不准确."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "m_iyjbfHZ4b6"
   },
   "outputs": [],
   "source": [
    "def find_NER_in_spacy(raw_content, nlp = nlp, tokensize=True, ner=False, exclude_list = ['PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']):\n",
    "    '''使用spacy进行NER.\n",
    "    标注解释: https://spacy.io/api/annotation\n",
    "    '''\n",
    "    res_nlp = nlp(raw_content)\n",
    "    tokens = []\n",
    "    if tokensize:\n",
    "        tokens =  [str(i) for i in res_nlp.doc]\n",
    "    \n",
    "    entities_list = []\n",
    "    if ner:\n",
    "        for item in res_nlp.ents:\n",
    "            if item.label_ in exclude_list: continue\n",
    "            entities_dict = {}\n",
    "            entities_dict['type'] = item.label_\n",
    "            entities_dict['span_start'] = item.start\n",
    "            entities_dict['content'] = item.text\n",
    "            entities_dict['span_end'] = item.end\n",
    "            entities_list.append(entities_dict)\n",
    "        # print(dir(item))\n",
    "    return tokens, entities_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "11EGV2n1B_Ku"
   },
   "outputs": [],
   "source": [
    "ts, es = find_NER_in_spacy(max_sen_in_train, ner=True)\n",
    "print(ts)\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLNET测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel, BertConfig, AutoTokenizer\n",
    "from transformers import XLNetForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlnet-base-cased'\n",
    "config = AutoConfig.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_XLNET = AutoTokenizer.from_pretrained(model_name)\n",
    "model_XLNET = XLNetForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello i am jim<sep> who are you<sep><cls>'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tokenizer_XLNET.encode('hello i am jim','who are you',add_special_tokens=True)\n",
    "test = tokenizer_XLNET.decode(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['transformer.mask_emb', 'transformer.word_embedding.weight', 'transformer.layer.0.rel_attn.q', 'transformer.layer.0.rel_attn.k', 'transformer.layer.0.rel_attn.v', 'transformer.layer.0.rel_attn.o', 'transformer.layer.0.rel_attn.r', 'transformer.layer.0.rel_attn.r_r_bias', 'transformer.layer.0.rel_attn.r_s_bias', 'transformer.layer.0.rel_attn.r_w_bias', 'transformer.layer.0.rel_attn.seg_embed', 'transformer.layer.0.rel_attn.layer_norm.weight', 'transformer.layer.0.rel_attn.layer_norm.bias', 'transformer.layer.0.ff.layer_norm.weight', 'transformer.layer.0.ff.layer_norm.bias', 'transformer.layer.0.ff.layer_1.weight', 'transformer.layer.0.ff.layer_1.bias', 'transformer.layer.0.ff.layer_2.weight', 'transformer.layer.0.ff.layer_2.bias', 'transformer.layer.1.rel_attn.q', 'transformer.layer.1.rel_attn.k', 'transformer.layer.1.rel_attn.v', 'transformer.layer.1.rel_attn.o', 'transformer.layer.1.rel_attn.r', 'transformer.layer.1.rel_attn.r_r_bias', 'transformer.layer.1.rel_attn.r_s_bias', 'transformer.layer.1.rel_attn.r_w_bias', 'transformer.layer.1.rel_attn.seg_embed', 'transformer.layer.1.rel_attn.layer_norm.weight', 'transformer.layer.1.rel_attn.layer_norm.bias', 'transformer.layer.1.ff.layer_norm.weight', 'transformer.layer.1.ff.layer_norm.bias', 'transformer.layer.1.ff.layer_1.weight', 'transformer.layer.1.ff.layer_1.bias', 'transformer.layer.1.ff.layer_2.weight', 'transformer.layer.1.ff.layer_2.bias', 'transformer.layer.2.rel_attn.q', 'transformer.layer.2.rel_attn.k', 'transformer.layer.2.rel_attn.v', 'transformer.layer.2.rel_attn.o', 'transformer.layer.2.rel_attn.r', 'transformer.layer.2.rel_attn.r_r_bias', 'transformer.layer.2.rel_attn.r_s_bias', 'transformer.layer.2.rel_attn.r_w_bias', 'transformer.layer.2.rel_attn.seg_embed', 'transformer.layer.2.rel_attn.layer_norm.weight', 'transformer.layer.2.rel_attn.layer_norm.bias', 'transformer.layer.2.ff.layer_norm.weight', 'transformer.layer.2.ff.layer_norm.bias', 'transformer.layer.2.ff.layer_1.weight', 'transformer.layer.2.ff.layer_1.bias', 'transformer.layer.2.ff.layer_2.weight', 'transformer.layer.2.ff.layer_2.bias', 'transformer.layer.3.rel_attn.q', 'transformer.layer.3.rel_attn.k', 'transformer.layer.3.rel_attn.v', 'transformer.layer.3.rel_attn.o', 'transformer.layer.3.rel_attn.r', 'transformer.layer.3.rel_attn.r_r_bias', 'transformer.layer.3.rel_attn.r_s_bias', 'transformer.layer.3.rel_attn.r_w_bias', 'transformer.layer.3.rel_attn.seg_embed', 'transformer.layer.3.rel_attn.layer_norm.weight', 'transformer.layer.3.rel_attn.layer_norm.bias', 'transformer.layer.3.ff.layer_norm.weight', 'transformer.layer.3.ff.layer_norm.bias', 'transformer.layer.3.ff.layer_1.weight', 'transformer.layer.3.ff.layer_1.bias', 'transformer.layer.3.ff.layer_2.weight', 'transformer.layer.3.ff.layer_2.bias', 'transformer.layer.4.rel_attn.q', 'transformer.layer.4.rel_attn.k', 'transformer.layer.4.rel_attn.v', 'transformer.layer.4.rel_attn.o', 'transformer.layer.4.rel_attn.r', 'transformer.layer.4.rel_attn.r_r_bias', 'transformer.layer.4.rel_attn.r_s_bias', 'transformer.layer.4.rel_attn.r_w_bias', 'transformer.layer.4.rel_attn.seg_embed', 'transformer.layer.4.rel_attn.layer_norm.weight', 'transformer.layer.4.rel_attn.layer_norm.bias', 'transformer.layer.4.ff.layer_norm.weight', 'transformer.layer.4.ff.layer_norm.bias', 'transformer.layer.4.ff.layer_1.weight', 'transformer.layer.4.ff.layer_1.bias', 'transformer.layer.4.ff.layer_2.weight', 'transformer.layer.4.ff.layer_2.bias', 'transformer.layer.5.rel_attn.q', 'transformer.layer.5.rel_attn.k', 'transformer.layer.5.rel_attn.v', 'transformer.layer.5.rel_attn.o', 'transformer.layer.5.rel_attn.r', 'transformer.layer.5.rel_attn.r_r_bias', 'transformer.layer.5.rel_attn.r_s_bias', 'transformer.layer.5.rel_attn.r_w_bias', 'transformer.layer.5.rel_attn.seg_embed', 'transformer.layer.5.rel_attn.layer_norm.weight', 'transformer.layer.5.rel_attn.layer_norm.bias', 'transformer.layer.5.ff.layer_norm.weight', 'transformer.layer.5.ff.layer_norm.bias', 'transformer.layer.5.ff.layer_1.weight', 'transformer.layer.5.ff.layer_1.bias', 'transformer.layer.5.ff.layer_2.weight', 'transformer.layer.5.ff.layer_2.bias', 'transformer.layer.6.rel_attn.q', 'transformer.layer.6.rel_attn.k', 'transformer.layer.6.rel_attn.v', 'transformer.layer.6.rel_attn.o', 'transformer.layer.6.rel_attn.r', 'transformer.layer.6.rel_attn.r_r_bias', 'transformer.layer.6.rel_attn.r_s_bias', 'transformer.layer.6.rel_attn.r_w_bias', 'transformer.layer.6.rel_attn.seg_embed', 'transformer.layer.6.rel_attn.layer_norm.weight', 'transformer.layer.6.rel_attn.layer_norm.bias', 'transformer.layer.6.ff.layer_norm.weight', 'transformer.layer.6.ff.layer_norm.bias', 'transformer.layer.6.ff.layer_1.weight', 'transformer.layer.6.ff.layer_1.bias', 'transformer.layer.6.ff.layer_2.weight', 'transformer.layer.6.ff.layer_2.bias', 'transformer.layer.7.rel_attn.q', 'transformer.layer.7.rel_attn.k', 'transformer.layer.7.rel_attn.v', 'transformer.layer.7.rel_attn.o', 'transformer.layer.7.rel_attn.r', 'transformer.layer.7.rel_attn.r_r_bias', 'transformer.layer.7.rel_attn.r_s_bias', 'transformer.layer.7.rel_attn.r_w_bias', 'transformer.layer.7.rel_attn.seg_embed', 'transformer.layer.7.rel_attn.layer_norm.weight', 'transformer.layer.7.rel_attn.layer_norm.bias', 'transformer.layer.7.ff.layer_norm.weight', 'transformer.layer.7.ff.layer_norm.bias', 'transformer.layer.7.ff.layer_1.weight', 'transformer.layer.7.ff.layer_1.bias', 'transformer.layer.7.ff.layer_2.weight', 'transformer.layer.7.ff.layer_2.bias', 'transformer.layer.8.rel_attn.q', 'transformer.layer.8.rel_attn.k', 'transformer.layer.8.rel_attn.v', 'transformer.layer.8.rel_attn.o', 'transformer.layer.8.rel_attn.r', 'transformer.layer.8.rel_attn.r_r_bias', 'transformer.layer.8.rel_attn.r_s_bias', 'transformer.layer.8.rel_attn.r_w_bias', 'transformer.layer.8.rel_attn.seg_embed', 'transformer.layer.8.rel_attn.layer_norm.weight', 'transformer.layer.8.rel_attn.layer_norm.bias', 'transformer.layer.8.ff.layer_norm.weight', 'transformer.layer.8.ff.layer_norm.bias', 'transformer.layer.8.ff.layer_1.weight', 'transformer.layer.8.ff.layer_1.bias', 'transformer.layer.8.ff.layer_2.weight', 'transformer.layer.8.ff.layer_2.bias', 'transformer.layer.9.rel_attn.q', 'transformer.layer.9.rel_attn.k', 'transformer.layer.9.rel_attn.v', 'transformer.layer.9.rel_attn.o', 'transformer.layer.9.rel_attn.r', 'transformer.layer.9.rel_attn.r_r_bias', 'transformer.layer.9.rel_attn.r_s_bias', 'transformer.layer.9.rel_attn.r_w_bias', 'transformer.layer.9.rel_attn.seg_embed', 'transformer.layer.9.rel_attn.layer_norm.weight', 'transformer.layer.9.rel_attn.layer_norm.bias', 'transformer.layer.9.ff.layer_norm.weight', 'transformer.layer.9.ff.layer_norm.bias', 'transformer.layer.9.ff.layer_1.weight', 'transformer.layer.9.ff.layer_1.bias', 'transformer.layer.9.ff.layer_2.weight', 'transformer.layer.9.ff.layer_2.bias', 'transformer.layer.10.rel_attn.q', 'transformer.layer.10.rel_attn.k', 'transformer.layer.10.rel_attn.v', 'transformer.layer.10.rel_attn.o', 'transformer.layer.10.rel_attn.r', 'transformer.layer.10.rel_attn.r_r_bias', 'transformer.layer.10.rel_attn.r_s_bias', 'transformer.layer.10.rel_attn.r_w_bias', 'transformer.layer.10.rel_attn.seg_embed', 'transformer.layer.10.rel_attn.layer_norm.weight', 'transformer.layer.10.rel_attn.layer_norm.bias', 'transformer.layer.10.ff.layer_norm.weight', 'transformer.layer.10.ff.layer_norm.bias', 'transformer.layer.10.ff.layer_1.weight', 'transformer.layer.10.ff.layer_1.bias', 'transformer.layer.10.ff.layer_2.weight', 'transformer.layer.10.ff.layer_2.bias', 'transformer.layer.11.rel_attn.q', 'transformer.layer.11.rel_attn.k', 'transformer.layer.11.rel_attn.v', 'transformer.layer.11.rel_attn.o', 'transformer.layer.11.rel_attn.r', 'transformer.layer.11.rel_attn.r_r_bias', 'transformer.layer.11.rel_attn.r_s_bias', 'transformer.layer.11.rel_attn.r_w_bias', 'transformer.layer.11.rel_attn.seg_embed', 'transformer.layer.11.rel_attn.layer_norm.weight', 'transformer.layer.11.rel_attn.layer_norm.bias', 'transformer.layer.11.ff.layer_norm.weight', 'transformer.layer.11.ff.layer_norm.bias', 'transformer.layer.11.ff.layer_1.weight', 'transformer.layer.11.ff.layer_1.bias', 'transformer.layer.11.ff.layer_2.weight', 'transformer.layer.11.ff.layer_2.bias', 'start_logits.dense.weight', 'start_logits.dense.bias', 'end_logits.dense_0.weight', 'end_logits.dense_0.bias', 'end_logits.LayerNorm.weight', 'end_logits.LayerNorm.bias', 'end_logits.dense_1.weight', 'end_logits.dense_1.bias', 'answer_class.dense_0.weight', 'answer_class.dense_0.bias', 'answer_class.dense_1.weight'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_XLNET.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = tokenizer_XLNET.encode_plus(max_sen_in_train,\n",
    "                                          add_special_tokens=True,\n",
    "                                         return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model_XLNET(**model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 687, 768])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_from_XLNET(text,text_pair=None,\n",
    "                            tokenizer = tokenizer_XLNET,\n",
    "                            model = model_XLNET,\n",
    "                            add_special_tokens = True,\n",
    "                           device = 'cuda'):\n",
    "    \n",
    "    model_input = tokenizer_XLNET.encode_plus(text,text_pair,\n",
    "                                        add_special_tokens=add_special_tokens,\n",
    "                                        return_tensors='pt')\n",
    "    print(tokenizer_XLNET.encode(text,text_pair ,add_special_tokens=add_special_tokens))\n",
    "    \n",
    "    model_input = {k:v.to(device) for k,v in model_input.items()}\n",
    "    model.to(device)\n",
    "    \n",
    "    last_hidden_state = model(**model_input)[0]\n",
    "    \n",
    "    return last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24717, 17, 150, 569, 17, 2030, 98, 9, 61, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "last_hidden_state = get_features_from_XLNET('hello i am jim. who')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 768])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', '', 'i', 'am', '', 'ji', 'm', '.', 'who', '<sep>', '<cls>']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer_XLNET.decode(i) for i in [24717, 17, 150, 569, 17, 2030, 98, 9, 61, 4, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state[:,-1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24717, 17, 150, 569, 17, 2030, 98, 9, 61]\n"
     ]
    }
   ],
   "source": [
    "last_hidden_state2 = get_features_from_XLNET('hello i am jim. who',add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 768])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_emb torch.Size([1, 1, 768])\n",
      "word_embedding.weight torch.Size([32000, 768])\n",
      "layer.0.rel_attn.q torch.Size([768, 12, 64])\n",
      "layer.0.rel_attn.k torch.Size([768, 12, 64])\n",
      "layer.0.rel_attn.v torch.Size([768, 12, 64])\n",
      "layer.0.rel_attn.o torch.Size([768, 12, 64])\n",
      "layer.0.rel_attn.r torch.Size([768, 12, 64])\n",
      "layer.0.rel_attn.r_r_bias torch.Size([12, 64])\n",
      "layer.0.rel_attn.r_s_bias torch.Size([12, 64])\n",
      "layer.0.rel_attn.r_w_bias torch.Size([12, 64])\n",
      "layer.0.rel_attn.seg_embed torch.Size([2, 12, 64])\n",
      "layer.0.rel_attn.layer_norm.weight torch.Size([768])\n",
      "layer.0.rel_attn.layer_norm.bias torch.Size([768])\n",
      "layer.0.ff.layer_norm.weight torch.Size([768])\n",
      "layer.0.ff.layer_norm.bias torch.Size([768])\n",
      "layer.0.ff.layer_1.weight torch.Size([3072, 768])\n",
      "layer.0.ff.layer_1.bias torch.Size([3072])\n",
      "layer.0.ff.layer_2.weight torch.Size([768, 3072])\n",
      "layer.0.ff.layer_2.bias torch.Size([768])\n",
      "layer.1.rel_attn.q torch.Size([768, 12, 64])\n",
      "layer.1.rel_attn.k torch.Size([768, 12, 64])\n",
      "layer.1.rel_attn.v torch.Size([768, 12, 64])\n",
      "layer.1.rel_attn.o torch.Size([768, 12, 64])\n",
      "layer.1.rel_attn.r torch.Size([768, 12, 64])\n",
      "layer.1.rel_attn.r_r_bias torch.Size([12, 64])\n",
      "layer.1.rel_attn.r_s_bias torch.Size([12, 64])\n",
      "layer.1.rel_attn.r_w_bias torch.Size([12, 64])\n",
      "layer.1.rel_attn.seg_embed torch.Size([2, 12, 64])\n",
      "layer.1.rel_attn.layer_norm.weight torch.Size([768])\n",
      "layer.1.rel_attn.layer_norm.bias torch.Size([768])\n",
      "layer.1.ff.layer_norm.weight torch.Size([768])\n",
      "layer.1.ff.layer_norm.bias torch.Size([768])\n",
      "layer.1.ff.layer_1.weight torch.Size([3072, 768])\n",
      "layer.1.ff.layer_1.bias torch.Size([3072])\n",
      "layer.1.ff.layer_2.weight torch.Size([768, 3072])\n",
      "layer.1.ff.layer_2.bias torch.Size([768])\n",
      "layer.2.rel_attn.q torch.Size([768, 12, 64])\n",
      "layer.2.rel_attn.k torch.Size([768, 12, 64])\n",
      "layer.2.rel_attn.v torch.Size([768, 12, 64])\n",
      "layer.2.rel_attn.o torch.Size([768, 12, 64])\n",
      "layer.2.rel_attn.r torch.Size([768, 12, 64])\n",
      "layer.2.rel_attn.r_r_bias torch.Size([12, 64])\n",
      "layer.2.rel_attn.r_s_bias torch.Size([12, 64])\n",
      "layer.2.rel_attn.r_w_bias torch.Size([12, 64])\n",
      "layer.2.rel_attn.seg_embed torch.Size([2, 12, 64])\n",
      "layer.2.rel_attn.layer_norm.weight torch.Size([768])\n",
      "layer.2.rel_attn.layer_norm.bias torch.Size([768])\n",
      "layer.2.ff.layer_norm.weight torch.Size([768])\n",
      "layer.2.ff.layer_norm.bias torch.Size([768])\n",
      "layer.2.ff.layer_1.weight torch.Size([3072, 768])\n",
      "layer.2.ff.layer_1.bias torch.Size([3072])\n",
      "layer.2.ff.layer_2.weight torch.Size([768, 3072])\n",
      "layer.2.ff.layer_2.bias torch.Size([768])\n",
      "layer.3.rel_attn.q torch.Size([768, 12, 64])\n",
      "layer.3.rel_attn.k torch.Size([768, 12, 64])\n",
      "layer.3.rel_attn.v torch.Size([768, 12, 64])\n",
      "layer.3.rel_attn.o torch.Size([768, 12, 64])\n",
      "layer.3.rel_attn.r torch.Size([768, 12, 64])\n",
      "layer.3.rel_attn.r_r_bias torch.Size([12, 64])\n",
      "layer.3.rel_attn.r_s_bias torch.Size([12, 64])\n",
      "layer.3.rel_attn.r_w_bias torch.Size([12, 64])\n",
      "layer.3.rel_attn.seg_embed torch.Size([2, 12, 64])\n",
      "layer.3.rel_attn.layer_norm.weight torch.Size([768])\n",
      "layer.3.rel_attn.layer_norm.bias torch.Size([768])\n",
      "layer.3.ff.layer_norm.weight torch.Size([768])\n",
      "layer.3.ff.layer_norm.bias torch.Size([768])\n",
      "layer.3.ff.layer_1.weight torch.Size([3072, 768])\n",
      "layer.3.ff.layer_1.bias torch.Size([3072])\n",
      "layer.3.ff.layer_2.weight torch.Size([768, 3072])\n",
      "layer.3.ff.layer_2.bias torch.Size([768])\n",
      "layer.4.rel_attn.q torch.Size([768, 12, 64])\n",
      "layer.4.rel_attn.k torch.Size([768, 12, 64])\n",
      "layer.4.rel_attn.v torch.Size([768, 12, 64])\n",
      "layer.4.rel_attn.o torch.Size([768, 12, 64])\n",
      "layer.4.rel_attn.r torch.Size([768, 12, 64])\n",
      "layer.4.rel_attn.r_r_bias torch.Size([12, 64])\n",
      "layer.4.rel_attn.r_s_bias torch.Size([12, 64])\n",
      "layer.4.rel_attn.r_w_bias torch.Size([12, 64])\n",
      "layer.4.rel_attn.seg_embed torch.Size([2, 12, 64])\n",
      "layer.4.rel_attn.layer_norm.weight torch.Size([768])\n",
      "layer.4.rel_attn.layer_norm.bias torch.Size([768])\n",
      "layer.4.ff.layer_norm.weight torch.Size([768])\n",
      "layer.4.ff.layer_norm.bias torch.Size([768])\n",
      "layer.4.ff.layer_1.weight torch.Size([3072, 768])\n",
      "layer.4.ff.layer_1.bias torch.Size([3072])\n",
      "layer.4.ff.layer_2.weight torch.Size([768, 3072])\n",
      "layer.4.ff.layer_2.bias torch.Size([768])\n",
      "layer.5.rel_attn.q torch.Size([768, 12, 64])\n",
      "layer.5.rel_attn.k torch.Size([768, 12, 64])\n",
      "layer.5.rel_attn.v torch.Size([768, 12, 64])\n",
      "layer.5.rel_attn.o torch.Size([768, 12, 64])\n",
      "layer.5.rel_attn.r torch.Size([768, 12, 64])\n",
      "layer.5.rel_attn.r_r_bias torch.Size([12, 64])\n",
      "layer.5.rel_attn.r_s_bias torch.Size([12, 64])\n",
      "layer.5.rel_attn.r_w_bias torch.Size([12, 64])\n",
      "layer.5.rel_attn.seg_embed torch.Size([2, 12, 64])\n",
      "layer.5.rel_attn.layer_norm.weight torch.Size([768])\n",
      "layer.5.rel_attn.layer_norm.bias torch.Size([768])\n",
      "layer.5.ff.layer_norm.weight torch.Size([768])\n",
      "layer.5.ff.layer_norm.bias torch.Size([768])\n",
      "layer.5.ff.layer_1.weight torch.Size([3072, 768])\n",
      "layer.5.ff.layer_1.bias torch.Size([3072])\n",
      "layer.5.ff.layer_2.weight torch.Size([768, 3072])\n",
      "layer.5.ff.layer_2.bias torch.Size([768])\n",
      "layer.6.rel_attn.q torch.Size([768, 12, 64])\n",
      "layer.6.rel_attn.k torch.Size([768, 12, 64])\n",
      "layer.6.rel_attn.v torch.Size([768, 12, 64])\n",
      "layer.6.rel_attn.o torch.Size([768, 12, 64])\n",
      "layer.6.rel_attn.r torch.Size([768, 12, 64])\n",
      "layer.6.rel_attn.r_r_bias torch.Size([12, 64])\n",
      "layer.6.rel_attn.r_s_bias torch.Size([12, 64])\n",
      "layer.6.rel_attn.r_w_bias torch.Size([12, 64])\n",
      "layer.6.rel_attn.seg_embed torch.Size([2, 12, 64])\n",
      "layer.6.rel_attn.layer_norm.weight torch.Size([768])\n",
      "layer.6.rel_attn.layer_norm.bias torch.Size([768])\n",
      "layer.6.ff.layer_norm.weight torch.Size([768])\n",
      "layer.6.ff.layer_norm.bias torch.Size([768])\n",
      "layer.6.ff.layer_1.weight torch.Size([3072, 768])\n",
      "layer.6.ff.layer_1.bias torch.Size([3072])\n",
      "layer.6.ff.layer_2.weight torch.Size([768, 3072])\n",
      "layer.6.ff.layer_2.bias torch.Size([768])\n",
      "layer.7.rel_attn.q torch.Size([768, 12, 64])\n",
      "layer.7.rel_attn.k torch.Size([768, 12, 64])\n",
      "layer.7.rel_attn.v torch.Size([768, 12, 64])\n",
      "layer.7.rel_attn.o torch.Size([768, 12, 64])\n",
      "layer.7.rel_attn.r torch.Size([768, 12, 64])\n",
      "layer.7.rel_attn.r_r_bias torch.Size([12, 64])\n",
      "layer.7.rel_attn.r_s_bias torch.Size([12, 64])\n",
      "layer.7.rel_attn.r_w_bias torch.Size([12, 64])\n",
      "layer.7.rel_attn.seg_embed torch.Size([2, 12, 64])\n",
      "layer.7.rel_attn.layer_norm.weight torch.Size([768])\n",
      "layer.7.rel_attn.layer_norm.bias torch.Size([768])\n",
      "layer.7.ff.layer_norm.weight torch.Size([768])\n",
      "layer.7.ff.layer_norm.bias torch.Size([768])\n",
      "layer.7.ff.layer_1.weight torch.Size([3072, 768])\n",
      "layer.7.ff.layer_1.bias torch.Size([3072])\n",
      "layer.7.ff.layer_2.weight torch.Size([768, 3072])\n",
      "layer.7.ff.layer_2.bias torch.Size([768])\n",
      "layer.8.rel_attn.q torch.Size([768, 12, 64])\n",
      "layer.8.rel_attn.k torch.Size([768, 12, 64])\n",
      "layer.8.rel_attn.v torch.Size([768, 12, 64])\n",
      "layer.8.rel_attn.o torch.Size([768, 12, 64])\n",
      "layer.8.rel_attn.r torch.Size([768, 12, 64])\n",
      "layer.8.rel_attn.r_r_bias torch.Size([12, 64])\n",
      "layer.8.rel_attn.r_s_bias torch.Size([12, 64])\n",
      "layer.8.rel_attn.r_w_bias torch.Size([12, 64])\n",
      "layer.8.rel_attn.seg_embed torch.Size([2, 12, 64])\n",
      "layer.8.rel_attn.layer_norm.weight torch.Size([768])\n",
      "layer.8.rel_attn.layer_norm.bias torch.Size([768])\n",
      "layer.8.ff.layer_norm.weight torch.Size([768])\n",
      "layer.8.ff.layer_norm.bias torch.Size([768])\n",
      "layer.8.ff.layer_1.weight torch.Size([3072, 768])\n",
      "layer.8.ff.layer_1.bias torch.Size([3072])\n",
      "layer.8.ff.layer_2.weight torch.Size([768, 3072])\n",
      "layer.8.ff.layer_2.bias torch.Size([768])\n",
      "layer.9.rel_attn.q torch.Size([768, 12, 64])\n",
      "layer.9.rel_attn.k torch.Size([768, 12, 64])\n",
      "layer.9.rel_attn.v torch.Size([768, 12, 64])\n",
      "layer.9.rel_attn.o torch.Size([768, 12, 64])\n",
      "layer.9.rel_attn.r torch.Size([768, 12, 64])\n",
      "layer.9.rel_attn.r_r_bias torch.Size([12, 64])\n",
      "layer.9.rel_attn.r_s_bias torch.Size([12, 64])\n",
      "layer.9.rel_attn.r_w_bias torch.Size([12, 64])\n",
      "layer.9.rel_attn.seg_embed torch.Size([2, 12, 64])\n",
      "layer.9.rel_attn.layer_norm.weight torch.Size([768])\n",
      "layer.9.rel_attn.layer_norm.bias torch.Size([768])\n",
      "layer.9.ff.layer_norm.weight torch.Size([768])\n",
      "layer.9.ff.layer_norm.bias torch.Size([768])\n",
      "layer.9.ff.layer_1.weight torch.Size([3072, 768])\n",
      "layer.9.ff.layer_1.bias torch.Size([3072])\n",
      "layer.9.ff.layer_2.weight torch.Size([768, 3072])\n",
      "layer.9.ff.layer_2.bias torch.Size([768])\n",
      "layer.10.rel_attn.q torch.Size([768, 12, 64])\n",
      "layer.10.rel_attn.k torch.Size([768, 12, 64])\n",
      "layer.10.rel_attn.v torch.Size([768, 12, 64])\n",
      "layer.10.rel_attn.o torch.Size([768, 12, 64])\n",
      "layer.10.rel_attn.r torch.Size([768, 12, 64])\n",
      "layer.10.rel_attn.r_r_bias torch.Size([12, 64])\n",
      "layer.10.rel_attn.r_s_bias torch.Size([12, 64])\n",
      "layer.10.rel_attn.r_w_bias torch.Size([12, 64])\n",
      "layer.10.rel_attn.seg_embed torch.Size([2, 12, 64])\n",
      "layer.10.rel_attn.layer_norm.weight torch.Size([768])\n",
      "layer.10.rel_attn.layer_norm.bias torch.Size([768])\n",
      "layer.10.ff.layer_norm.weight torch.Size([768])\n",
      "layer.10.ff.layer_norm.bias torch.Size([768])\n",
      "layer.10.ff.layer_1.weight torch.Size([3072, 768])\n",
      "layer.10.ff.layer_1.bias torch.Size([3072])\n",
      "layer.10.ff.layer_2.weight torch.Size([768, 3072])\n",
      "layer.10.ff.layer_2.bias torch.Size([768])\n",
      "layer.11.rel_attn.q torch.Size([768, 12, 64])\n",
      "layer.11.rel_attn.k torch.Size([768, 12, 64])\n",
      "layer.11.rel_attn.v torch.Size([768, 12, 64])\n",
      "layer.11.rel_attn.o torch.Size([768, 12, 64])\n",
      "layer.11.rel_attn.r torch.Size([768, 12, 64])\n",
      "layer.11.rel_attn.r_r_bias torch.Size([12, 64])\n",
      "layer.11.rel_attn.r_s_bias torch.Size([12, 64])\n",
      "layer.11.rel_attn.r_w_bias torch.Size([12, 64])\n",
      "layer.11.rel_attn.seg_embed torch.Size([2, 12, 64])\n",
      "layer.11.rel_attn.layer_norm.weight torch.Size([768])\n",
      "layer.11.rel_attn.layer_norm.bias torch.Size([768])\n",
      "layer.11.ff.layer_norm.weight torch.Size([768])\n",
      "layer.11.ff.layer_norm.bias torch.Size([768])\n",
      "layer.11.ff.layer_1.weight torch.Size([3072, 768])\n",
      "layer.11.ff.layer_1.bias torch.Size([3072])\n",
      "layer.11.ff.layer_2.weight torch.Size([768, 3072])\n",
      "layer.11.ff.layer_2.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for i,k in model_XLNET.state_dict().items():\n",
    "    print(i, k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_XLNET' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8725a450583d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_XLNET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_XLNET' is not defined"
     ]
    }
   ],
   "source": [
    "len(model_XLNET.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 试验: BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721e682ce05b4c9bafce4985b9d0d81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_QA = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "tokenizer_QA = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "model_input = tokenizer_QA.encode_plus(question,\n",
    "                                      text,\n",
    "                                      return_tensors='pt')\n",
    "start_scores, end_scores = model_QA(**model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2292,  0.1850,  0.5994,  0.1678,  0.4288,  0.2153,  0.4109,  0.0382,\n",
       "          0.4649,  0.4879,  0.2751,  0.0596, -0.0334,  0.5149]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2515,  0.3538,  0.1440,  0.1174,  0.3960, -0.0503, -0.5198,  0.0995,\n",
       "          0.2090,  0.1094, -0.1096, -0.1006,  0.0448, -0.3144]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置小型模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_config = BertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_config.max_length = 1024 # get ans span\n",
    "my_config.max_position_embeddings = 1024\n",
    "my_config.num_hidden_layers = 2\n",
    "my_config.num_attention_heads = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = BertForQuestionAnswering(my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_feat = torch.randn([3,5,11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "\n",
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "input_ids = tokenizer.encode(question, text)\n",
    "token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n",
    "start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n",
    "\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = torch.randn([3,5,768])\n",
    "test_attn_mask = [\n",
    "    [1,1,1,0,0],\n",
    "    [1,1,0,0,0],\n",
    "    [1,1,1,1,0]\n",
    "]\n",
    "test_span = [\n",
    "    [1,4],\n",
    "    [0,3],\n",
    "    [2,4]\n",
    "]\n",
    "\n",
    "model_input = {}\n",
    "model_input['start_positions'], model_input['end_positions'] = torch.tensor(test_span).split(1, dim = -1)\n",
    "model_input['attention_mask'] = torch.tensor(test_attn_mask)\n",
    "model_input['inputs_embeds'] = test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.7547, grad_fn=<DivBackward0>),\n",
       " tensor([[ 0.2590, -0.1622, -0.6363,  0.5475, -0.0411],\n",
       "         [ 0.3774,  0.3416, -0.1097,  0.5342, -0.4611],\n",
       "         [-0.6236,  0.8827,  0.1005, -0.8110,  0.8700]],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([[ 0.3241, -0.0710,  0.6727,  0.3628, -0.2442],\n",
       "         [ 0.2463,  0.7926,  0.0895,  0.7744,  0.2147],\n",
       "         [-0.4006, -0.4710,  0.5912, -0.0770, -0.4697]],\n",
       "        grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model(**model_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试Dataset与字符串\n",
    "\n",
    "结果:\n",
    "\n",
    "```python\n",
    "TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class '__main__.test_ABC'>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_ABC(object):\n",
    "    def __init__(self):\n",
    "        self.data = np.random.randn(1,3)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "strr = [\n",
    "    ['hello', 'i', 'am'],\n",
    "    ['old1', 'dog'],\n",
    "    ['old2', 'dog'],\n",
    "    ['old3', 'dog'],\n",
    "    ['old4', 'dog'],\n",
    "    ['old5', 'dog'],\n",
    "    ['old6', 'dog'],\n",
    "    ['old7', 'dog'],\n",
    "    ['old8', 'dog'],\n",
    "    ['old9', 'dog'],\n",
    "]\n",
    "\n",
    "class Test_Dateset(Dataset):\n",
    "    def __init__(self, max_num=5000, max_seq=512, vocab_size=30522, label_num=2):\n",
    "        self.max_num = max_num\n",
    "        self.max_seq = max_seq\n",
    "        self.vocab_size = vocab_size\n",
    "        self.label_num = label_num\n",
    "        \n",
    "        self.trains = [test_ABC() for i in range(10)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.trains)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = torch.randint(0,self.vocab_size,(self.max_seq,), device='cuda').unsqueeze(0)\n",
    "        labels = torch.randint(0,self.label_num-1,(1,), device='cuda')\n",
    "        return {'input_ids':data,\n",
    "                'labels':labels,\n",
    "               'test': strr[index]}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "# def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cuda'): \n",
    "#     dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "#                             shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "#     for data_dict in dataloader:\n",
    "#         out_data_dict = {}\n",
    "#         for name, _tensor in data_dict.items():\n",
    "#             out_data_dict[name] = data_dict[name].squeeze(1).to(device)\n",
    "#         yield out_data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[28500,  7537, 28433,  ..., 25806, 12358,  6663]],\n",
      "\n",
      "        [[ 4321,  1716, 26861,  ..., 29733, 13880,    31]],\n",
      "\n",
      "        [[25880,  1496,  7014,  ..., 20770, 16101, 30424]]], device='cuda:0'), 'labels': tensor([[0],\n",
      "        [0],\n",
      "        [0]], device='cuda:0'), 'test': [('old5', 'old3', 'old1'), ('dog', 'dog', 'dog')]}\n",
      "{'input_ids': tensor([[[19712,  7453, 29928,  ...,  3488, 13098,   999]],\n",
      "\n",
      "        [[ 1767, 27465, 12974,  ...,   353, 24580,  8575]],\n",
      "\n",
      "        [[20590, 10373, 21864,  ..., 21217, 14208, 10837]]], device='cuda:0'), 'labels': tensor([[0],\n",
      "        [0],\n",
      "        [0]], device='cuda:0'), 'test': [('old4', 'old8', 'hello'), ('dog', 'dog', 'i')]}\n",
      "{'input_ids': tensor([[[ 7880, 13294, 13267,  ..., 20274,  7448, 22592]],\n",
      "\n",
      "        [[12936, 10716,  7366,  ..., 20027, 17670, 20574]],\n",
      "\n",
      "        [[16402, 29196, 19612,  ..., 20883, 25794, 19608]]], device='cuda:0'), 'labels': tensor([[0],\n",
      "        [0],\n",
      "        [0]], device='cuda:0'), 'test': [('old7', 'old2', 'old9'), ('dog', 'dog', 'dog')]}\n"
     ]
    }
   ],
   "source": [
    "dataset = Test_Dateset()\n",
    "batch_size = 3\n",
    "\n",
    "for item in DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=True, drop_last=True):\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nomalizations比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 16 elements not 794",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-65dc6f1943a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Batch Normalization层,因为输入是将高度H和宽度W合成了一个维度,所以这里用1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m794\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 传入通道数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 全局的均值mu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1668\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1669\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m     )\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 16 elements not 794"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "x = torch.rand(100, 16, 784)\n",
    "\n",
    "# Batch Normalization层,因为输入是将高度H和宽度W合成了一个维度,所以这里用1d\n",
    "layer = nn.BatchNorm1d(794)  # 传入通道数\n",
    "out = layer(x)\n",
    "\n",
    "# 全局的均值mu\n",
    "print(layer.running_mean)\n",
    "# 全局的方差sigma^2\n",
    "print(layer.running_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 16, 784])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(5, 200, 32)\n",
    "# With Learnable Parameters\n",
    "m1 = nn.LayerNorm(input.size()[1:])\n",
    "# Without Learnable Parameters\n",
    "m2 = nn.LayerNorm(input.size()[1:], elementwise_affine=False)\n",
    "# Normalize over last two dimensions\n",
    "m3 = nn.LayerNorm([200, 32])\n",
    "# Normalize over last dimension of size 10\n",
    "m4 = nn.LayerNorm([200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 200, 32])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = m1(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 200, 32])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = m2(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 200, 32])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = m3(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[200], expected input with shape [*, 200], but got input of size[5, 200, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-8bf0fb67c20f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 153\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   1694\u001b[0m     \"\"\"\n\u001b[1;32m   1695\u001b[0m     return torch.layer_norm(input, normalized_shape, weight, bias, eps,\n\u001b[0;32m-> 1696\u001b[0;31m                             torch.backends.cudnn.enabled)\n\u001b[0m\u001b[1;32m   1697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[200], expected input with shape [*, 200], but got input of size[5, 200, 32]"
     ]
    }
   ],
   "source": [
    "output = m4(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "model_path = 'data/models/bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert = AutoTokenizer.from_pretrained(model_path)\n",
    "model_bert = AutoModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'special_tokens_mask': tensor([[1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'input_ids': tensor([[  101, 19082,   102,   178,  1821, 23220,  1306,   119,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101,  1293,   102,  1132,  1128,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bert.batch_encode_plus([(\"hello\",\"i am jim.\"),\n",
    "                                  (\"how\", \"are you\")],\n",
    "                                return_tensors='pt',\n",
    "                                max_length = 20,\n",
    "                                pad_to_max_length = True,\n",
    "                                return_special_tokens_masks = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 5144, 1161, 1112, 1363, 102]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bert.encode('china as good', add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bert.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 100, 102, 0, 101, 103, 102]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bert.encode('[UNK] [SEP] [PAD] [CLS] [MASK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD] [unused1] [unused2] [unused3] [unused4] [unused5] [unused6] [unused7] [unused8] [unused9] [unused10] [unused11] [unused12] [unused13] [unused14] [unused15] [unused16] [unused17] [unused18] [unused19] [unused20] [unused21] [unused22] [unused23] [unused24] [unused25] [unused26] [unused27] [unused28] [unused29] [unused30] [unused31] [unused32] [unused33] [unused34] [unused35] [unused36] [unused37] [unused38] [unused39] [unused40] [unused41] [unused42] [unused43] [unused44] [unused45] [unused46] [unused47] [unused48] [unused49] [unused50] [unused51] [unused52] [unused53] [unused54] [unused55] [unused56] [unused57] [unused58] [unused59] [unused60] [unused61] [unused62] [unused63] [unused64] [unused65] [unused66] [unused67] [unused68] [unused69] [unused70] [unused71] [unused72] [unused73] [unused74] [unused75] [unused76] [unused77] [unused78] [unused79] [unused80] [unused81] [unused82] [unused83] [unused84] [unused85] [unused86] [unused87] [unused88] [unused89] [unused90] [unused91] [unused92] [unused93] [unused94] [unused95] [unused96] [unused97] [unused98] [unused99] [UNK] [CLS] [SEP] [MASK] [unused100] [unused101]! \" # $ % &\\'( ) * +, -. / 0 1 2 3 4 5 6 7 8 9 : ; < = >? @ A B C D E F G H I J K L M N O P Q R S T U V W X Y Z [ \\\\ ] ^ _ ` a b c d e f g h i j k l m n o p q r s t u v w x y z { | } ~ ¡ ¢ £ ¥ § ¨ © ª « ¬ ® ° ± ² ³ ´ µ ¶ · ¹ º » ¼ ½ ¾ ¿ À Á Â Ä Å Æ Ç È É Í Î Ñ Ó Ö × Ø Ú Ü Þ ß à á â ã ä å æ ç è é ê ë ì í î ï ð ñ ò ó ô õ ö ÷ ø ù ú û ü ý þ ÿ Ā ā ă ą Ć ć Č č ď Đ đ ē ė ę ě ğ ġ Ħ ħ ĩ Ī ī İ ı ļ Ľ ľ Ł ł ń ņ ň ŋ Ō ō ŏ ő Œ œ ř Ś ś Ş ş Š š Ţ ţ ť ũ ū ŭ ů ű ų ŵ ŷ ź Ż ż Ž ž Ə ƒ ơ ư ǎ ǐ ǒ ǔ ǫ Ș ș Ț ț ɐ ɑ ɔ ɕ ə ɛ ɡ ɣ ɨ ɪ ɲ ɾ ʀ ʁ ʂ ʃ ʊ ʋ ʌ ʐ ʑ ʒ ʔ ʰ ʲ ʳ ʷ ʻ ʼ ʾ ʿ ˈ ː ˡ ˢ ˣ ́ ̃ ̍ ̯ ͡ Α Β Γ Δ Ε Η Θ Ι Κ Λ Μ Ν Ο Π Σ Τ Φ Χ Ψ Ω ά έ ή ί α β γ δ ε ζ η θ ι κ λ μ ν ξ ο π ρ ς σ τ υ φ χ ψ ω ό ύ ώ І Ј А Б В Г Д Е Ж З И К Л М Н О П Р С Т У Ф Х Ц Ч Ш Э Ю Я а б в г д е ж з и й к л м н о п р с т у ф х ц ч ш щ ъ ы ь э ю я ё і ї ј њ ћ Ա Հ ա ե ի կ մ յ ն ո ս տ ր ւ ְ ִ ֵ ֶ ַ ָ ֹ ּ א ב ג ד ה ו ז ח ט י כ ל ם מ ן נ ס ע פ צ ק ר ש ת ، ء آ أ إ ئ ا ب ة ت ث ج ح خ د ذ ر ز س ش ص ض ط ظ ع غ ف ق ك ل م ن ه و ى ي َ ِ ٹ پ چ ک گ ہ ی ے ं आ क ग च ज ण त द ध न प ब भ म य र ल व श ष स ह ा ि ी ु े ो ् । ॥ আ ই এ ও ক খ গ চ ছ জ ট ত থ দ ধ ন প ব ম য র ল শ স হ ় া ি ী ু ে ো ্ য় க த ப ம ய ர ல வ ா ி ு ் ร ་ ག ང ད ན བ མ ར ལ ས ི ུ ེ ོ ა ე ი ლ ნ ო რ ს ᴬ ᴵ ᵀ ᵃ ᵇ ᵈ ᵉ ᵍ ᵏ ᵐ ᵒ ᵖ ᵗ ᵘ ᵢ ᵣ ᵤ ᵥ ᶜ ᶠ ḍ Ḥ ḥ Ḩ ḩ ḳ ṃ ṅ ṇ ṛ ṣ ṭ ạ ả ấ ầ ẩ ậ ắ ế ề ể ễ ệ ị ọ ố ồ ổ ộ ớ ờ ợ ụ ủ ứ ừ ử ữ ự ỳ ỹ ἀ ἐ ὁ ὐ ὰ ὶ ὸ ῆ ῖ ῦ ῶ ‐ ‑ ‒ – — ― ‖ ‘ ’ ‚ “ ” „ † ‡ • … ‰ ′ ″ ⁄ ⁰ ⁱ ⁴ ⁵ ⁶ ⁷ ⁸ ⁹ ⁺ ⁻ ⁿ ₀ ₁ ₂ ₃ ₄ ₅ ₆ ₇ ₈ ₉ ₊ ₍ ₎ ₐ ₑ ₒ ₓ ₕ ₖ ₘ ₙ ₚ ₛ ₜ ₤ € ₱ ₹ ℓ № ℝ ⅓ ← ↑ → ↔ ⇌ ⇒ ∂ ∈ − ∗ ∘ √ ∞ ∧ ∨ ∩ ∪ ≈ ≠ ≡ ≤ ≥ ⊂ ⊆ ⊕ ⋅ ─ │ ■ ● ★ ☆ ☉ ♠ ♣ ♥ ♦ ♭ ♯ ⟨ ⟩ ⱼ 、 。 《 》 「 」 『 』 〜 い う え お か き く け こ さ し す せ そ た ち つ て と な に の は ひ ま み む め も や ゆ よ ら り る れ ん ア ィ イ ウ エ オ カ ガ キ ク グ コ サ シ ジ ス ズ タ ダ ッ テ デ ト ド ナ ニ ハ バ パ フ ブ プ マ ミ ム ャ ュ ラ リ ル レ ロ ン ・ ー 一 三 上 下 中 事 二 井 京 人 亻 仁 佐 侍 光 公 力 北 十 南 原 口 史 司'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bert.decode([i for i in range(0, 1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bert.decode([127])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sssss'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'sssSS'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Albert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertForQuestionAnswering, AlbertTokenizer\n",
    "model_path = 'data/models/albert-base-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_albert = AlbertTokenizer.from_pretrained(model_path)\n",
    "model_albert = AlbertForQuestionAnswering.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [('hello, i am 88899', 'who are you 88899?'),]\n",
    "text_str = 'hello, i am 88899, who are you 88899?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁hello',\n",
       " ',',\n",
       " '▁i',\n",
       " '▁am',\n",
       " '▁8',\n",
       " '88',\n",
       " '99',\n",
       " ',',\n",
       " '▁who',\n",
       " '▁are',\n",
       " '▁you',\n",
       " '▁8',\n",
       " '88',\n",
       " '99',\n",
       " '?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_albert.tokenize(text_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 998, 3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_albert.encode('China')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad><unk>[CLS][SEP][MASK]()\"-.–£€  the, of ands in to a\\' was he is for on as with that i it his by at her fromt she an had youd be: were but thisi'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_albert.decode([i for i in range(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 10975,    15,    31,   589,   469,  3020,  3483,     3,    72,\n",
       "             50,    42,   469,  3020,  3483,    60,     3]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_albert.batch_encode_plus(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of AlbertForQuestionAnswering(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_albert.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QA_models import AutoQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.embeddings.word_embeddings.weight torch.Size([30522, 1024])\n",
      "transformer.embeddings.position_embeddings.weight torch.Size([512, 1024])\n",
      "transformer.embeddings.token_type_embeddings.weight torch.Size([2, 1024])\n",
      "transformer.embeddings.LayerNorm.weight torch.Size([1024])\n",
      "transformer.embeddings.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.0.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.0.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.0.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.0.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.0.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.0.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.0.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.0.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.0.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.0.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.0.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.0.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.0.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.0.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.1.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.1.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.1.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.1.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.1.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.1.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.1.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.1.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.1.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.1.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.1.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.1.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.1.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.1.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.2.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.2.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.2.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.2.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.2.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.2.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.2.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.2.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.2.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.2.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.2.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.2.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.2.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.2.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.3.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.3.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.3.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.3.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.3.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.3.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.3.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.3.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.3.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.3.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.3.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.3.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.3.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.3.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.4.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.4.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.4.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.4.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.4.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.4.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.4.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.4.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.4.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.4.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.4.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.4.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.4.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.4.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.5.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.5.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.5.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.5.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.5.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.5.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.5.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.5.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.5.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.5.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.5.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.5.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.5.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.5.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.6.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.6.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.6.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.6.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.6.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.6.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.6.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.6.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.6.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.6.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.6.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.6.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.6.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.6.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.7.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.7.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.7.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.7.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.7.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.7.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.7.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.7.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.7.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.7.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.7.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.7.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.7.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.7.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.8.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.8.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.8.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.8.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.8.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.8.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.8.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.8.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.8.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.8.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.8.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.8.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.8.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.8.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.9.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.9.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.9.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.9.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.9.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.9.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.9.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.9.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.9.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.9.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.9.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.9.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.9.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.9.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.10.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.10.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.10.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.10.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.10.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.10.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.10.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.10.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.10.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.10.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.10.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.10.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.10.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.10.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.11.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.11.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.11.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.11.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.11.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.11.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.11.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.11.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.11.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.11.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.11.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.11.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.11.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.11.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.12.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.12.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.12.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.12.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.12.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.12.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.12.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.12.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.12.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.12.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.12.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.12.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.12.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.12.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.12.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.12.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.13.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.13.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.13.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.13.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.13.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.13.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.13.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.13.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.13.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.13.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.13.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.13.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.13.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.13.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.13.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.13.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.14.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.14.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.14.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.14.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.14.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.14.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.14.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.14.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.14.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.14.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.14.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.14.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.14.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.14.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.14.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.14.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.15.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.15.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.15.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.15.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.15.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.15.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.15.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.15.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.15.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.15.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.15.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.15.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.15.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.15.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.15.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.15.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.16.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.16.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.16.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.16.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.16.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.16.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.16.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.16.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.16.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.16.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.16.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.16.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.16.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.16.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.16.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.16.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.17.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.17.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.17.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.17.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.17.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.17.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.17.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.17.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.17.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.17.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.17.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.17.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.17.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.17.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.17.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.17.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.18.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.18.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.18.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.18.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.18.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.18.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.18.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.18.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.18.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.18.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.18.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.18.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.18.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.18.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.18.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.18.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.19.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.19.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.19.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.19.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.19.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.19.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.19.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.19.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.19.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.19.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.19.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.19.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.19.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.19.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.19.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.19.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.20.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.20.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.20.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.20.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.20.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.20.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.20.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.20.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.20.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.20.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.20.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.20.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.20.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.20.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.20.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.20.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.21.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.21.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.21.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.21.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.21.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.21.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.21.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.21.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.21.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.21.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.21.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.21.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.21.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.21.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.21.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.21.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.22.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.22.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.22.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.22.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.22.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.22.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.22.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.22.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.22.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.22.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.22.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.22.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.22.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.22.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.22.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.22.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.23.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.23.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.23.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.23.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.23.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.23.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.23.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.23.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.23.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.23.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.23.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.23.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.23.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.23.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.23.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.23.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.pooler.dense.weight torch.Size([1024, 1024])\n",
      "transformer.pooler.dense.bias torch.Size([1024])\n",
      "start_logits.dense.weight torch.Size([1, 1024])\n",
      "start_logits.dense.bias torch.Size([1])\n",
      "end_logits.dense_0.weight torch.Size([1024, 2048])\n",
      "end_logits.dense_0.bias torch.Size([1024])\n",
      "end_logits.LayerNorm.weight torch.Size([1024])\n",
      "end_logits.LayerNorm.bias torch.Size([1024])\n",
      "end_logits.dense_1.weight torch.Size([1, 1024])\n",
      "end_logits.dense_1.bias torch.Size([1])\n",
      "answer_class.dense.weight torch.Size([2, 1024])\n",
      "answer_class.dense.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "model = AutoQuestionAnswering.from_pretrained('data/models/bert-large-uncased-whole-word-masking')\n",
    "for k,v in model.state_dict().items():\n",
    "    print(k, v.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.embeddings.word_embeddings.weight torch.Size([28996, 1024])\n",
      "transformer.embeddings.position_embeddings.weight torch.Size([512, 1024])\n",
      "transformer.embeddings.token_type_embeddings.weight torch.Size([2, 1024])\n",
      "transformer.embeddings.LayerNorm.weight torch.Size([1024])\n",
      "transformer.embeddings.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.0.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.0.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.0.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.0.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.0.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.0.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.0.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.0.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.0.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.0.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.0.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.0.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.0.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.0.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.1.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.1.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.1.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.1.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.1.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.1.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.1.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.1.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.1.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.1.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.1.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.1.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.1.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.1.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.2.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.2.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.2.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.2.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.2.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.2.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.2.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.2.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.2.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.2.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.2.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.2.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.2.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.2.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.3.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.3.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.3.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.3.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.3.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.3.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.3.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.3.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.3.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.3.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.3.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.3.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.3.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.3.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.4.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.4.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.4.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.4.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.4.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.4.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.4.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.4.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.4.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.4.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.4.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.4.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.4.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.4.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.5.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.5.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.5.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.5.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.5.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.5.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.5.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.5.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.5.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.5.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.5.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.5.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.5.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.5.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.6.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.6.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.6.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.6.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.6.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.6.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.6.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.6.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.6.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.6.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.6.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.6.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.6.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.6.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.7.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.7.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.7.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.7.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.7.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.7.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.7.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.7.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.7.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.7.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.7.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.7.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.7.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.7.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.8.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.8.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.8.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.8.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.8.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.8.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.8.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.8.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.8.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.8.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.8.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.8.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.8.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.8.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.9.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.9.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.9.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.9.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.9.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.9.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.9.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.9.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.9.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.9.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.9.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.9.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.9.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.9.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.10.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.10.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.10.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.10.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.10.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.10.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.10.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.10.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.10.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.10.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.10.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.10.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.10.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.10.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.11.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.11.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.11.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.11.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.11.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.11.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.11.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.11.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.11.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.11.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.11.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.11.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.11.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.11.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.12.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.12.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.12.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.12.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.12.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.12.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.12.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.12.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.12.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.12.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.12.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.12.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.12.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.12.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.12.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.12.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.13.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.13.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.13.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.13.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.13.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.13.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.13.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.13.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.13.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.13.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.13.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.13.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.13.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.13.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.13.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.13.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.14.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.14.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.14.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.14.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.14.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.14.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.14.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.14.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.14.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.14.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.14.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.14.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.14.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.14.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.14.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.14.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.15.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.15.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.15.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.15.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.15.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.15.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.15.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.15.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.15.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.15.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.15.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.15.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.15.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.15.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.15.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.15.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.16.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.16.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.16.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.16.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.16.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.16.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.16.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.16.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.16.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.16.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.16.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.16.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.16.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.16.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.16.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.16.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.17.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.17.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.17.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.17.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.17.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.17.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.17.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.17.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.17.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.17.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.17.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.17.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.17.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.17.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.17.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.17.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.18.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.18.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.18.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.18.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.18.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.18.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.18.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.18.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.18.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.18.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.18.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.18.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.18.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.18.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.18.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.18.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.19.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.19.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.19.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.19.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.19.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.19.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.19.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.19.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.19.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.19.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.19.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.19.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.19.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.19.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.19.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.19.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.20.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.20.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.20.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.20.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.20.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.20.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.20.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.20.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.20.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.20.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.20.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.20.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.20.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.20.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.20.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.20.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.21.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.21.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.21.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.21.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.21.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.21.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.21.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.21.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.21.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.21.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.21.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.21.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.21.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.21.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.21.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.21.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.22.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.22.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.22.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.22.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.22.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.22.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.22.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.22.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.22.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.22.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.22.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.22.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.22.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.22.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.22.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.22.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.23.attention.self.query.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.23.attention.self.query.bias torch.Size([1024])\n",
      "transformer.encoder.layer.23.attention.self.key.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.23.attention.self.key.bias torch.Size([1024])\n",
      "transformer.encoder.layer.23.attention.self.value.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.23.attention.self.value.bias torch.Size([1024])\n",
      "transformer.encoder.layer.23.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "transformer.encoder.layer.23.attention.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.23.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.23.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.encoder.layer.23.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "transformer.encoder.layer.23.intermediate.dense.bias torch.Size([4096])\n",
      "transformer.encoder.layer.23.output.dense.weight torch.Size([1024, 4096])\n",
      "transformer.encoder.layer.23.output.dense.bias torch.Size([1024])\n",
      "transformer.encoder.layer.23.output.LayerNorm.weight torch.Size([1024])\n",
      "transformer.encoder.layer.23.output.LayerNorm.bias torch.Size([1024])\n",
      "transformer.pooler.dense.weight torch.Size([1024, 1024])\n",
      "transformer.pooler.dense.bias torch.Size([1024])\n",
      "start_logits.dense.weight torch.Size([1, 1024])\n",
      "start_logits.dense.bias torch.Size([1])\n",
      "end_logits.dense_0.weight torch.Size([1024, 2048])\n",
      "end_logits.dense_0.bias torch.Size([1024])\n",
      "end_logits.LayerNorm.weight torch.Size([1024])\n",
      "end_logits.LayerNorm.bias torch.Size([1024])\n",
      "end_logits.dense_1.weight torch.Size([1, 1024])\n",
      "end_logits.dense_1.bias torch.Size([1])\n",
      "answer_class.dense.weight torch.Size([2, 1024])\n",
      "answer_class.dense.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "model = AutoQuestionAnswering.from_pretrained('data/models/bert-large-cased-whole-word-masking')\n",
    "for k,v in model.state_dict().items():\n",
    "    print(k, v.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function _VariableFunctions.zeros_like>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'as_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-40c6acfd55b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'as_type'"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,2]).as_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBerta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForQuestionAnswering, RobertaTokenizer\n",
    "model_path = 'data/models/roberta-base/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_Roberta = RobertaTokenizer.from_pretrained(model_path)\n",
    "model_Roberta = RobertaForQuestionAnswering.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [('hello, i am 88899', 'who are you 88899?'),]\n",
    "text_str = 'hello, i am 88899, who are you 88899?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " ',',\n",
       " 'Ġi',\n",
       " 'Ġam',\n",
       " 'Ġ8',\n",
       " '88',\n",
       " '99',\n",
       " ',',\n",
       " 'Ġwho',\n",
       " 'Ġare',\n",
       " 'Ġyou',\n",
       " 'Ġ8',\n",
       " '88',\n",
       " '99',\n",
       " '?']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_Roberta.tokenize(text_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 20760,     6,   939,   524,   290,  4652,  2831,     2,     2,\n",
       "             54,    32,    47,   290,  4652,  2831,   116,     2]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_Roberta.batch_encode_plus(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of RobertaForQuestionAnswering(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_Roberta.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizerFast\n",
    "model_path = 'data/models/distilbert-base-uncased-distilled-squad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_Distil = DistilBertTokenizerFast.from_pretrained(model_path, local_files_only=True)\n",
    "model_Distil = DistilBertForQuestionAnswering.from_pretrained(model_path, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [('hello, i am 88899', 'who are you 88899?'),]\n",
    "text_str = 'hello, i am 88899, who are you 88899?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'hello',\n",
       " ',',\n",
       " 'i',\n",
       " 'am',\n",
       " '88',\n",
       " '##8',\n",
       " '##9',\n",
       " '##9',\n",
       " ',',\n",
       " 'who',\n",
       " 'are',\n",
       " 'you',\n",
       " '88',\n",
       " '##8',\n",
       " '##9',\n",
       " '##9',\n",
       " '?',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_Distil.tokenize(text_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7592, 1010, 1045, 2572, 6070, 2620, 2683, 2683,  102, 2040, 2024,\n",
       "          2017, 6070, 2620, 2683, 2683, 1029,  102]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_Distil.batch_encode_plus(text, return_tensors='pt', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_Distil.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "语言模型测试",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
