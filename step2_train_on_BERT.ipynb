{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#train-on-BERT\" data-toc-modified-id=\"train-on-BERT-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>train on BERT</a></span><ul class=\"toc-item\"><li><span><a href=\"#初始化\" data-toc-modified-id=\"初始化-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>初始化</a></span></li><li><span><a href=\"#HotpotQA_Dataset\" data-toc-modified-id=\"HotpotQA_Dataset-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>HotpotQA_Dataset</a></span></li></ul></li><li><span><a href=\"#训练\" data-toc-modified-id=\"训练-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>训练</a></span><ul class=\"toc-item\"><li><span><a href=\"#辅助函数\" data-toc-modified-id=\"辅助函数-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>辅助函数</a></span></li><li><span><a href=\"#超参数\" data-toc-modified-id=\"超参数-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>超参数</a></span></li><li><span><a href=\"#参数配置\" data-toc-modified-id=\"参数配置-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>参数配置</a></span></li><li><span><a href=\"#训练流程\" data-toc-modified-id=\"训练流程-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>训练流程</a></span><ul class=\"toc-item\"><li><span><a href=\"#训练辅助函数\" data-toc-modified-id=\"训练辅助函数-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>训练辅助函数</a></span></li><li><span><a href=\"#实例化\" data-toc-modified-id=\"实例化-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>实例化</a></span></li><li><span><a href=\"#训练\" data-toc-modified-id=\"训练-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>训练</a></span></li></ul></li></ul></li><li><span><a href=\"#草稿\" data-toc-modified-id=\"草稿-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>草稿</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "InVkq8XM-MSr"
   },
   "source": [
    "# train on BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rnlFxqcbu6Mp",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS_info: \\S\n",
      "Kernel \\r on an \\m\n",
      "\n",
      "cpu_type:      20  Intel(R) Core(TM) i9-9820X CPU @ 3.30GHz\n",
      "cpu_kernel:       1 physical id\t: 0\n",
      "      1 address sizes\t: 46 bits physical, 48 bits virtual\n",
      "      1 physical id\t: 0\n",
      "      1 address sizes\t: 46 bits physical, 48 bits virtual\n",
      "      1 physical id\t: 0\n",
      "      1 address sizes\t: 46 bits physical, 48 bits virtual\n",
      "      1 physical id\t: 0\n",
      "      1 address sizes\t: 46 bits physical, 48 bits virtual\n",
      "      1 physical id\t: 0\n",
      "      1 address sizes\t: 46 bits physical, 48 bits virtual\n",
      "      1 physical id\t: 0\n",
      "      1 address sizes\t: 46 bits physical, 48 bits virtual\n",
      "      1 physical id\t: 0\n",
      "      1 address sizes\t: 46 bits physical, 48 bits virtual\n",
      "      1 physical id\t: 0\n",
      "      1 address sizes\t: 46 bits physical, 48 bits virtual\n",
      "      1 physical id\t: 0\n",
      "      1 address sizes\t: 46 bits physical, 48 bits virtual\n",
      "      1 physical id\t: 0\n",
      "      1 address sizes\t: 46 bits physical, 48 bits virtual\n",
      "      1 physical id\t: 0\n",
      "      1 address sizes\t: 46 bits physical, 48 bits virtual\n",
      "      1 physical id\t: 0\n",
      "      1 address sizes\t: 46 bits physical, 48 bits virtual\n",
      "      1 physical id\t: 0\n",
      "      1 address sizes\t: 46 bits physical, 48 bits virtual\n",
      "      1 physical id\t: 0\n",
      "      1 address sizes\t: 46 bits physical, 48 bits virtual\n",
      "      1 physical id\t: 0\n",
      "      1 address sizes\t: 46 bits physical, 48 bits virtual\n",
      "      1 physical id\t: 0\n",
      "      1 address sizes\t: 46 bits physical, 48 bits virtual\n",
      "      1 physical id\t: 0\n",
      "      1 address sizes\t: 46 bits physical, 48 bits virtual\n",
      "      1 physical id\t: 0\n",
      "      1 address sizes\t: 46 bits physical, 48 bits virtual\n",
      "      1 physical id\t: 0\n",
      "      1 address sizes\t: 46 bits physical, 48 bits virtual\n",
      "      1 physical id\t: 0\n",
      "      1 address sizes\t: 46 bits physical, 48 bits virtual\n",
      "cpu_run_mode: 64\n",
      "cpu_support_mode: 20\n"
     ]
    }
   ],
   "source": [
    "print_cpu_info = True\n",
    "if print_cpu_info:\n",
    "    r_OS_info = !cat /etc/issue\n",
    "    r_cpu_type = !cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c\n",
    "    r_cpu_kernel = !cat /proc/cpuinfo | grep physical | uniq -c\n",
    "    r_cpu_run_mode = !getconf LONG_BIT\n",
    "    r_cpu_support_mode = !cat /proc/cpuinfo | grep flags | grep ' lm ' | wc -l\n",
    "\n",
    "    OS_info = '\\n'.join(r_OS_info)\n",
    "    cpu_type = '\\n'.join(r_cpu_type)\n",
    "    cpu_kernel = '\\n'.join(r_cpu_kernel)\n",
    "    cpu_run_mode = '\\n'.join(r_cpu_run_mode)\n",
    "    cpu_support_mode = '\\n'.join(r_cpu_support_mode)\n",
    "\n",
    "    print(f'OS_info: {OS_info}')\n",
    "    print(f'cpu_type: {cpu_type}')\n",
    "    print(f'cpu_kernel: {cpu_kernel}')\n",
    "    print(f'cpu_run_mode: {cpu_run_mode}')\n",
    "    print(f'cpu_support_mode: {cpu_support_mode}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "vYKqd1w_tJDy",
    "outputId": "b8965a28-92f6-4a32-aa9f-ec9647a38d19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar  8 11:33:37 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.36       Driver Version: 440.36       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:19:00.0 Off |                  N/A |\n",
      "| 27%   28C    P8     1W / 250W |      1MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 27%   32C    P8     3W / 250W |      1MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:67:00.0 Off |                  N/A |\n",
      "| 27%   34C    P8    11W / 250W |      1MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:68:00.0 Off |                  N/A |\n",
      "| 27%   35C    P8    20W / 250W |    140MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    3     12973      G   /usr/bin/X                                    80MiB |\n",
      "|    3     14954      G   /usr/bin/gnome-shell                          57MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print_gpu_info = True\n",
    "if print_gpu_info:\n",
    "    gpu_info = !nvidia-smi\n",
    "    gpu_info = '\\n'.join(gpu_info)\n",
    "    if gpu_info.find('failed') >= 0:\n",
    "        print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "        print('and then re-execute this cell.')\n",
    "    else:\n",
    "        print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3BQirFJctLUv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 33.2 gigabytes of available RAM\n",
      "\n",
      "You are using a high-RAM runtime!\n"
     ]
    }
   ],
   "source": [
    "print_ram_info = True\n",
    "if print_ram_info:\n",
    "    from psutil import virtual_memory\n",
    "    ram_gb = virtual_memory().total / 1e9\n",
    "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "    if ram_gb < 20:\n",
    "        print('To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"')\n",
    "        print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
    "        print('re-execute this cell.')\n",
    "    else:\n",
    "        print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "77xeXtImYPVV"
   },
   "source": [
    "## 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwo3xmF0YS6Q"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/folders/')\n",
    "    !pip install transformers\n",
    "    # !pip install -U spacy[cuda100]\n",
    "    # !wget -P /content/folders/My\\ Drive/download/ https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz\n",
    "    # !pip install /content/folders/My\\ Drive/download/en_core_web_lg-2.2.5.tar.gz\n",
    "    # !wget -P /content/folders/My\\ Drive/HotpotQA/ http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json\n",
    "    # json_train_path = '/content/folders/My Drive/HotpotQA/样例_hotpot_train_v1.1.json' # 5个例子\n",
    "    json_train_path = '/content/folders/My Drive/HotpotQA/hotpot_train_v1.1.json'\n",
    "    save_cache_path = '/content/folders/My Drive/save_cache/'\n",
    "    save_cache_path_linux = '/content/folders/My\\ Drive/save_cache/'\n",
    "    HotpotQA_path = '/content/folders/My Drive/HotpotQA'\n",
    "except:\n",
    "    json_train_path = r'./data/hotpot_train_v1.1.json'\n",
    "    HotpotQA_path = './'\n",
    "    save_cache_path = 'save_cache/'\n",
    "    use_proxy = False\n",
    "    proxies={\"http_proxy\": \"127.0.0.1:10802\",\n",
    "        \"https_proxy\": \"127.0.0.1:10802\"} if use_proxy else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "colab_type": "code",
    "id": "4jQk2iEEbRsz",
    "outputId": "1c838baa-7319-4d04-a0aa-ea02a0cf1961"
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from apex import amp\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,HotpotQA_path)\n",
    "\n",
    "from Classes import Node, Question_Paragraph, Adjacency_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q0DdQA-gYPVs"
   },
   "source": [
    "## HotpotQA_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r0zl6ewAYPVw"
   },
   "outputs": [],
   "source": [
    "class HotpotQA_Dataset(Dataset):\n",
    "    '''index只要获取一条一条的(q,p,label)即可.'''\n",
    "    def __init__(self, train_pair, val_pair, tokenizer):\n",
    "        \n",
    "        # features\n",
    "        self.train_pair = train_pair # [(sen1, sen2, label), ...]\n",
    "        self.train_size = len(self.train_pair)\n",
    "        \n",
    "        self.val_pair = val_pair\n",
    "        self.val_size = len(self.val_pair)\n",
    "\n",
    "        self._tokenizer = tokenizer\n",
    "        \n",
    "        # func\n",
    "        self._lookup_dict = {'train': (self.train_pair, self.train_size),\n",
    "                    'val': (self.val_pair, self.val_size)}\n",
    "\n",
    "        self.pair_type=None\n",
    "        self.set_split('train')\n",
    "\n",
    "        # parameters\n",
    "        self.max_length = 1024\n",
    "        self.pad_to_max_length = True\n",
    "        \n",
    "    @classmethod\n",
    "    def build_dataset(cls, hotpotQA_train_preprocess, pair_type='ques_sent', \\\n",
    "                      ratio_train=0.7, contain_title=False, \\\n",
    "                      tokenizer=None, seed=123):\n",
    "        '''Q_P_list是由Question-Paragraph对组成的list.\n",
    "        可以打平抽样. 否则难以提升batchsize.\n",
    "        '''\n",
    "        assert pair_type in ['ques_sent', 'ques_para']\n",
    "\n",
    "        _Q_P_list = [i['ques_para_list'] for i in hotpotQA_train_preprocess]\n",
    "        Q_P_list = []\n",
    "        for i in _Q_P_list: Q_P_list.extend(i)\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(Q_P_list)\n",
    "\n",
    "        sample_train_num = int(ratio_train * len(Q_P_list))\n",
    "\n",
    "        if pair_type == 'ques_sent':\n",
    "            train_pair = []\n",
    "            for Q_P in Q_P_list[:sample_train_num]:\n",
    "                train_pair.extend(Q_P.get_ques_sent_label_list())\n",
    "\n",
    "            val_pair = []\n",
    "            for Q_P in Q_P_list[sample_train_num:]:\n",
    "                val_pair.extend(Q_P.get_ques_sent_label_list())       \n",
    "        else:\n",
    "            train_pair = [Q_P.get_ques_para_label_tuple(contain_title) \\\n",
    "                          for Q_P in Q_P_list[:sample_train_num]]\n",
    "            val_pair = [Q_P.get_ques_para_label_tuple(contain_title) \\\n",
    "                        for Q_P in Q_P_list[sample_train_num:]]\n",
    "\n",
    "        _cls = cls(train_pair, val_pair, tokenizer)\n",
    "        _cls.pair_type = pair_type\n",
    "        return _cls\n",
    "\n",
    "    @staticmethod\n",
    "    def build_dataset_from_path(QA_preprocess_path, pair_type='ques_sent', \\\n",
    "                      ratio_train=0.7, contain_title=False, \\\n",
    "                      tokenizer=None, seed=123):\n",
    "        \n",
    "        with open(QA_preprocess_path,'rb')as fp:\n",
    "            hotpotQA_train_preprocess = pickle.load(fp)\n",
    "        \n",
    "        return HotpotQA_Dataset.build_dataset(hotpotQA_train_preprocess,\n",
    "                            pair_type,\n",
    "                            ratio_train,\n",
    "                            contain_title,\n",
    "                            tokenizer,\n",
    "                            seed,)\n",
    "\n",
    "    def set_parameters(self, max_length, pad_to_max_length):\n",
    "        self.max_length = max_length\n",
    "        self.pad_to_max_length = pad_to_max_length\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        assert split in ['train', 'val', 'test']\n",
    "        self._target_split = split\n",
    "        self._target_pair, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def get_one_item(self, index):\n",
    "        one_line = self.train_pair[index]\n",
    "        return {'ques_tokens': one_line[0],\n",
    "                'pair_tokens': one_line[1],\n",
    "                'label': one_line[2]}\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        return self._tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        one_line = self.train_pair[index]\n",
    "\n",
    "        model_input = self._tokenizer.encode_plus(\n",
    "                text = one_line[0],\n",
    "                text_pair = one_line[1],\n",
    "                add_special_tokens = True,\n",
    "                max_length = self.max_length,\n",
    "                truncation_strategy = 'only_second',\n",
    "                pad_to_max_length = self.pad_to_max_length,\n",
    "                return_tensors = 'pt'\n",
    "            )\n",
    "        label = one_line[2]\n",
    "        model_input['labels'] = torch.tensor([label], dtype=torch.long)\n",
    "\n",
    "        return model_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'HotpotQA Dataset. mode: {}. pair: {}. size: {}. max_seq: {}'.format\\\n",
    "            (self._target_split,self.pair_type,self.__len__(),self.max_length)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'): \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                    shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, _tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].squeeze(1).to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "whLjqpPaYPWM"
   },
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gxA2hmd-bFzx"
   },
   "source": [
    "## 辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w38KM2KSLhLC"
   },
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xA1rKeIHbJ-r"
   },
   "source": [
    "## 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o1zV4d0abNeY"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    # drive.mount('/content/folders/')\n",
    "    json_train_path = '/content/folders/My Drive/HotpotQA/hotpot_train_v1.1.json'\n",
    "    save_cache_path = '/content/folders/My Drive/save_cache'\n",
    "    HotpotQA_path = '/content/folders/My Drive/HotpotQA'\n",
    "except:\n",
    "    json_train_path = 'data/hotpot_train_v1.1.json'\n",
    "    save_cache_path = 'save_cache/'\n",
    "    HotpotQA_path = './'\n",
    "\n",
    "args = Namespace(\n",
    "    # Data and path information\n",
    "    json_train_path=json_train_path,\n",
    "    model_state_file = \"model_HotpotQA.pth\",\n",
    "    save_dir = save_cache_path,\n",
    "    HotpotQA_preprocess_file = 'hotpotQA_train_preprocess100.pkl',\n",
    "\n",
    "    # Model hyper parameter\n",
    "    use_proxy = False,\n",
    "    proxies={\"http_proxy\": \"127.0.0.1:10802\",\n",
    "             \"https_proxy\": \"127.0.0.1:10802\"} if use_proxy else None,\n",
    "    tokenizer_type = \"bert-large-cased-whole-word-masking\",\n",
    "    model_type = 'bert-base-uncased',\n",
    "\n",
    "    # Dataset parameter\n",
    "    max_seq = 512,\n",
    "    pad_to_max = True,\n",
    "\n",
    "    # Training hyper parameter\n",
    "    num_epochs=2,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=64,\n",
    "    seed=1337,\n",
    "    early_stopping_criteria=5,\n",
    "    freeze_layer_name = 'bert.encoder.layer.10',\n",
    "\n",
    "    # Runtime hyper parameter\n",
    "    cuda=True,\n",
    "    device=None,\n",
    "    tpu=False,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "km8Z8PM5nCTE"
   },
   "source": [
    "## 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zXhvL8qiAmu8",
    "outputId": "c61d89d5-84d1-4609-975b-4c9178efed32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check TPU\n",
    "if args.tpu:\n",
    "    try:\n",
    "        import torch_xla\n",
    "        import torch_xla.core.xla_model as xm\n",
    "        args.device = xm.xla_device()\n",
    "    except:\n",
    "        # 安装完可能要重启内核\n",
    "        VERSION = \"20200220\"\n",
    "        _=!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "        _=!python pytorch-xla-env-setup.py --version $VERSION\n",
    "        import torch_xla\n",
    "        import torch_xla.core.xla_model as xm\n",
    "        args.device = xm.xla_device()\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "if not args.device:\n",
    "    args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "print(\"Using: {}\".format(args.device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229,
     "referenced_widgets": [
      "690e1e3611ff45b38c448e96c1ea8f1e",
      "218632911683498396f60f7e1b202cb2",
      "bccadcd8149e4b029a512db16b1f922f",
      "68a9c9423eff4063a138f3cd878cc0ee",
      "3aea4476c92043e3b343b12dabfc8d0b",
      "77f70d07431e4a7bbe2a8de1f2316a3a",
      "77edb67c4d994fee9974b241a4331194",
      "b94e87ac89dc4c9cac7f453ab0405354",
      "2fd567cab6f04cadb38bd9a2802ad774",
      "d78771179ad646229807d6d6b1a093d2",
      "f721aac47c7b4e68b4c5089246f7148f",
      "b32ad1041a5541eda8da257fb7613f24",
      "2addf3c9976d4142b3c3fbe9ce0dbb09",
      "e07d73ba35534dfa90314fd774ee3099",
      "091a5c6fe3074acc87552ad1a5745e6b",
      "f4c181f262cd47c1852845cf1077bd77",
      "91eaf72aaf7f4c6c96486cabd28e675c",
      "6160f139e960417dbe744897c6853136",
      "9b530a68231e4cb995a63903fba2c6a9",
      "b07db3f069f3404abf66107df09f821c",
      "5e149740813b490a902a9792602f22f2",
      "8f231276459648cc94f90ea9fbf01296",
      "ec214db428fd49a3ad6ef5f3234a63f3",
      "ae56506c959e472ba8711851bc4492ce",
      "a607691af8ca44d8b4b149223dc8914a",
      "00e26a17c44c4e9b97819a88769407bd",
      "cfb86fbea41c418dbeb14ba82dd20a70",
      "aa6b256323314da48e40e57bda785995",
      "fa4d270d05724507aa429333ff2c68fb",
      "daf77371b38046f8926ec3c5d897938d",
      "e8f69a780fa941c89316c5faab9ecb69",
      "e944313259d44b61a3af3c70c755e801"
     ]
    },
    "colab_type": "code",
    "id": "1DDtP5qtLhN_",
    "outputId": "12996049-6d97-4fe8-f60e-84171cf2c288"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HotpotQA Dataset. mode: train. pair: ques_sent. size: 312. max_seq: 512\n"
     ]
    }
   ],
   "source": [
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.model_state_file = os.path.join(args.save_dir,args.model_state_file)\n",
    "    args.HotpotQA_preprocess_file = os.path.join(args.save_dir,args.HotpotQA_preprocess_file)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)\n",
    "\n",
    "# init pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_type,proxies=proxies)\n",
    "classifier = AutoModelForSequenceClassification.from_pretrained(args.model_type,proxies=proxies)\n",
    "\n",
    "if args.reload_from_files:\n",
    "    dataset = HotpotQA_Dataset.build_dataset_from_path(args.HotpotQA_preprocess_file, tokenizer=tokenizer)\n",
    "else:\n",
    "    dataset = HotpotQA_Dataset.build_dataset(hotpotQA_train_preprocess, tokenizer=tokenizer)\n",
    "\n",
    "dataset.set_parameters(args.max_seq,args.pad_to_max)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qasgxoZOq79l"
   },
   "source": [
    "## 训练流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MNQ_RTQFr-QZ"
   },
   "source": [
    "### 训练辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "M3DLKu_Lq7Rq"
   },
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "         \n",
    "        # If loss worsened\n",
    "        if loss_t >= loss_tm1:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(logits, y_target):\n",
    "    _, logits_indices = logits.max(dim=1)\n",
    "    n_correct = torch.eq(logits_indices, y_target).sum().item()\n",
    "    return n_correct / len(logits_indices) * 100\n",
    "\n",
    "# 冻结\n",
    "def freeze_to_layer(model, layer_name):\n",
    "    '''冻结层. 从0到layer_name.'''\n",
    "    index_start = -1\n",
    "    for index, (key, _value) in enumerate(model.state_dict().items()):\n",
    "        if key.startswith(layer_name): \n",
    "            index_start = index\n",
    "            break\n",
    "    \n",
    "    if index_start < 0:\n",
    "        print(f\"Don't find layer name: {layer_name}\")\n",
    "        return\n",
    "    \n",
    "    no_grad_nums = index_start + 1\n",
    "    grad_nums = 0\n",
    "\n",
    "    for index, i in enumerate(model.parameters()):\n",
    "        if index >= index_start:\n",
    "            i.requires_grad = True\n",
    "            grad_nums += 1\n",
    "        else:\n",
    "            i.requires_grad = False\n",
    "    \n",
    "    print(f\"freeze layers num: {no_grad_nums}, active layers num: {grad_nums}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a9qOwTrZsEXT"
   },
   "source": [
    "### 实例化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "bdabee3a4c914c63bb810604c742fb03",
      "c65b91a019dd4ba0b3656d0da173231a",
      "00c06e4ff1f44729af2ebf1213846727",
      "ab370d2ebbf542ffa57a1d64c64a11ae",
      "95b8d36adc9142aaa764c660ed66a2c5",
      "f752dc1723e84bffaff690d40782a6b6",
      "56d45ffc59e94cca8d3946e549eb300d",
      "dde2bcfea6a948828f5880a1deff5a59",
      "d0d0c87c39484deeb5d19d15500ce4a8",
      "980fb3dfb1fc4e13a613420b27456e4a",
      "07069d327d794f468eb26dda02fd19fa",
      "950e707552a843cbbb12a0d82c01f8a0",
      "dd4fa254465744fdbfdbc135ad730abc",
      "b3f3c29b42864a429341053978b91439",
      "452b98cf78f24d1e8f359176e075b068",
      "a22009fed4fe41a680d7c9339e5c5263",
      "3112ca264ade4e8c97ec0b50be9efa18",
      "458dcc0e272a45a9ab33f8a9cdddbd36",
      "4c52b320378b4246a7d2e82c055d0587",
      "551fb7c1e72f4c88a0cbd431c60b0d93",
      "1db48f410d06456e80490d9e192198e4",
      "b8233ac6e5c84e49ad08dd2129339fc4",
      "93e9d6c97a384ac6856bc7263390f98c",
      "9013b4878bad4f0ab88c2845c72b9b44"
     ]
    },
    "colab_type": "code",
    "id": "QeWemKdTLhQX",
    "outputId": "2172bc25-a3b7-4e08-856b-d78ad59a471e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freeze layers num: 166, active layers num: 36.\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64ee526898c413095aacb177e47e2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training routine', max=2, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55d7356f4364df2bc9b28e36618254f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='split=train', max=4, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141e8f589eb74f7180ec58922aa118a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='split=val', max=2, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "classifier.train()\n",
    "freeze_to_layer(classifier, args.freeze_layer_name)\n",
    "# dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "    \n",
    "# loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, classifier.parameters()),\n",
    "                      lr=args.learning_rate)\n",
    "\n",
    "classifier, optimizer = amp.initialize(classifier, optimizer)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\\\n",
    "                        mode='min', factor=0.5, patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='training routine',\n",
    "                total=args.num_epochs,\n",
    "                position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm_notebook(desc='split=train',\n",
    "                total=dataset.get_num_batches(args.batch_size), \n",
    "                position=1, \n",
    "                leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm_notebook(desc='split=val',\n",
    "                total=dataset.get_num_batches(args.batch_size), \n",
    "                position=1, \n",
    "                leave=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHkjI83zyLpp"
   },
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "dyYX8pYgLhU0",
    "outputId": "a17e6ac5-904e-415c-93b7-2cccf01c20e8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "        # print('F1')\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset,\n",
    "                            batch_size=args.batch_size, \n",
    "                            device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------    \n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_target = batch_dict.pop('labels')\n",
    "            # print(batch_dict)\n",
    "            res_tuple = classifier(**batch_dict)\n",
    "            logits = res_tuple[0]\n",
    "#             if type(logits) != torch.Tensor: print(logits)\n",
    "\n",
    "            # step 3. compute the loss\n",
    "\n",
    "            loss = loss_func(logits, y_target)\n",
    "\n",
    "            # 移动平均. 递归数列可推导通项公式,求的是整个batch的均值.\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(logits, y_target)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                            batch_size=args.batch_size, \n",
    "                            device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            with torch.no_grad():\n",
    "                y_target = batch_dict.pop('labels')\n",
    "                res_tuple = classifier(**batch_dict)\n",
    "                logits = res_tuple[0]\n",
    "\n",
    "                # step 3. compute the loss\n",
    "                loss = loss_func(logits, y_target)\n",
    "                running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # compute the accuracy\n",
    "                acc_t = compute_accuracy(logits, y_target)\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "                \n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier, train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOfGhBKQxiLM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KBGx5Edmxiiq"
   },
   "source": [
    "# 草稿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3q6evb-ybMM"
   },
   "outputs": [],
   "source": [
    "tokenizer_BERT = AutoTokenizer.from_pretrained(\"bert-large-cased-whole-word-masking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OUWOMDBMYPVj",
    "outputId": "8f0c5224-d1a5-44bd-b215-994781170cf5"
   },
   "outputs": [],
   "source": [
    "with open(f'{save_cache_path}/hotpotQA_train_preprocess100.pkl','rb')as fp:\n",
    "    hotpotQA_train_preprocess = pickle.load(fp)\n",
    "len(hotpotQA_train_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YxKsDyFAAwSK",
    "outputId": "6ac7e37d-0cef-4ab0-8281-4808724e5266"
   },
   "outputs": [],
   "source": [
    "hotpotQA_train_preprocess[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "s_eMGbE-YPVz",
    "outputId": "e1fe26e3-79c3-44c5-935f-a79963c333c4"
   },
   "outputs": [],
   "source": [
    "test_dataset = HotpotQA_Dataset.build_dataset(hotpotQA_train_preprocess,tokenizer=tokenizer_BERT)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ta9mKaMguzb-",
    "outputId": "17820b3e-7c29-4014-8c57-9b4072896d56"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0djC_i9JLAN"
   },
   "outputs": [],
   "source": [
    "test_dataset.set_parameters(512,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "c_7xbZZxIVk7",
    "outputId": "950eed1b-7588-4972-ce16-c497ba4ef0fb"
   },
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'): \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                    shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, _tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device).squeeze(1)\n",
    "        yield out_data_dict\n",
    "\n",
    "for i in generate_batches(dataset, 3, device=args.device):\n",
    "    test_one_batch = i\n",
    "    print(i)\n",
    "    for k,v in i.items(): print(v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "6c1FpJ9avMbG",
    "outputId": "7cbd22e5-25b2-44b1-be67-9b49b7578c80"
   },
   "outputs": [],
   "source": [
    "test_item = test_dataset.get_one_item(2)\n",
    "print(test_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aMQUh0dRxFCF"
   },
   "outputs": [],
   "source": [
    "test_one_input = test_dataset.__getitem__(2)\n",
    "# print(test_one_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D9PIE1hZYPV2"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AtxM7PXSF-0R"
   },
   "outputs": [],
   "source": [
    "# 自行从头开始训练BERT..\n",
    "my_config = BertConfig.from_pretrained('bert-base-uncased',\n",
    "                    max_position_embeddings = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uXly2R0lwkfV"
   },
   "outputs": [],
   "source": [
    "# load预训练模型 position embedding只有512.\n",
    "model_BERT_pre = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "_ = model_BERT_pre.to(args.device).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "VQhyQAP2YPWG",
    "outputId": "7c21ada8-8208-4a87-cc78-653a24991e91"
   },
   "outputs": [],
   "source": [
    "model_BERT_pre(**test_one_batch)\n",
    "# model_loss, model_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-k7r_05L7dlg"
   },
   "outputs": [],
   "source": [
    "test_one_batch.pop('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-VVSfff8EZK"
   },
   "outputs": [],
   "source": [
    "for k,v in test_one_batch.items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "OlsQAKhQxiJY",
    "outputId": "df3d10ac-af4c-41d4-9a74-edab25d7f171"
   },
   "outputs": [],
   "source": [
    "model_BERT_pre(**test_one_batch)\n",
    "# model_loss, model_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "yOzouvHpxiES",
    "outputId": "382b798b-65ed-4035-e4e8-122fef719035"
   },
   "outputs": [],
   "source": [
    "test_one_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z2v3jA4e7uxk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Axa2PScY7u2w"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RmOUTfoU7u0l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ByNHb1XtxiCL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "train_on_BERT.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "240.275px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "141e8f589eb74f7180ec58922aa118a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c0c0ee7e74ec4b1ba8a9ac362ea33119",
        "IPY_MODEL_d4b43277bc364439a7dfa005270daca8"
       ],
       "layout": "IPY_MODEL_8245a398c95d41f1beb08f9dd4921d94"
      }
     },
     "176a3c88c5f94a5a8bce2f10dc7d6738": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "description": "training routine: 100%",
       "layout": "IPY_MODEL_c7703152fc414c75994c584f8c33cad5",
       "max": 2,
       "style": "IPY_MODEL_8139912ecd974a129c1d9fe5327a30cc",
       "value": 2
      }
     },
     "3c0b328c318843938db2b5ebb605d31d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3ee3cf0ca53e45f8852b662986f09be5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "59c07ffef9914e6cb2eac11ea430da4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7893906724114f2c8d1f79aafdacdd53": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9e6d8b6619984aad9df9f4e8298b1013",
       "style": "IPY_MODEL_84192036848f4d7a8fc423c10480df52",
       "value": " 3/4 [00:06&lt;00:00,  1.53it/s, acc=93.4, epoch=1, loss=0.336]"
      }
     },
     "8139912ecd974a129c1d9fe5327a30cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "8245a398c95d41f1beb08f9dd4921d94": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "84192036848f4d7a8fc423c10480df52": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "96b54af013064e699da0b0ca7c3073c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "description": "split=train:  75%",
       "layout": "IPY_MODEL_9c591d95d15a44868d44efa907a31b2e",
       "max": 4,
       "style": "IPY_MODEL_f0b649d2f36b4712b043ac3c0bd7676b",
       "value": 3
      }
     },
     "96cddfbb56ea4c08bc32747c94185050": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "975f345d45294cfc853c44a1186b3a97": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9c591d95d15a44868d44efa907a31b2e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9e6d8b6619984aad9df9f4e8298b1013": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a64bab2d47414877a0ff074b86a15767": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b666fc8bf91a4fd7926a486a0de253d3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c0c0ee7e74ec4b1ba8a9ac362ea33119": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "description": "split=val:  50%",
       "layout": "IPY_MODEL_a64bab2d47414877a0ff074b86a15767",
       "max": 2,
       "style": "IPY_MODEL_ce35c8733bd5439c93161696f71c9abf",
       "value": 1
      }
     },
     "c7703152fc414c75994c584f8c33cad5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ce35c8733bd5439c93161696f71c9abf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "d4b43277bc364439a7dfa005270daca8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3ee3cf0ca53e45f8852b662986f09be5",
       "style": "IPY_MODEL_96cddfbb56ea4c08bc32747c94185050",
       "value": " 1/2 [00:07&lt;00:02,  2.24s/it, acc=90.6, epoch=1, loss=0.323]"
      }
     },
     "d64ee526898c413095aacb177e47e2a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_176a3c88c5f94a5a8bce2f10dc7d6738",
        "IPY_MODEL_e1e935f8a4664e74a17c270a8763ae0e"
       ],
       "layout": "IPY_MODEL_975f345d45294cfc853c44a1186b3a97"
      }
     },
     "e1e935f8a4664e74a17c270a8763ae0e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3c0b328c318843938db2b5ebb605d31d",
       "style": "IPY_MODEL_59c07ffef9914e6cb2eac11ea430da4d",
       "value": " 2/2 [00:09&lt;00:00,  4.48s/it]"
      }
     },
     "e55d7356f4364df2bc9b28e36618254f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_96b54af013064e699da0b0ca7c3073c6",
        "IPY_MODEL_7893906724114f2c8d1f79aafdacdd53"
       ],
       "layout": "IPY_MODEL_b666fc8bf91a4fd7926a486a0de253d3"
      }
     },
     "f0b649d2f36b4712b043ac3c0bd7676b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
