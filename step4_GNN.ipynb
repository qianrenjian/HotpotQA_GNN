{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle\n",
    "\n",
    "from Classes import Node, Adjacency_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "save_cache_path = 'save_cache/'\n",
    "\n",
    "with open(save_cache_path+'hotpotQA_train_preprocess100_feat_adj.pkl', 'rb') as fp:\n",
    "    matrix_adj_label = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'\n",
    "feat_matrix, Adj_sp, labels = matrix_adj_label[0]\n",
    "\n",
    "feat_matrix = feat_matrix.to(DEVICE)\n",
    "\n",
    "adj = Adj_sp.to_dense_symmetric()\n",
    "adj = torch.from_numpy(adj).to(DEVICE)\n",
    "\n",
    "labels = labels.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([118, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([118, 118])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([118, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_all(feat_matrix, adj, labels, max_num = 300, pad_value = 0):\n",
    "    # feat_matrix: [N, dim]\n",
    "    assert feat_matrix.shape[0] == adj.shape[0] == labels.shape[0]\n",
    "    node_len = feat_matrix.shape[0]\n",
    "    max_num = max(max_num, node_len)\n",
    "    node_dim = feat_matrix.shape[1]\n",
    "    \n",
    "    feat_matrix_p = torch.zeros([max_num, node_dim]).fill_(pad_value)\n",
    "    feat_matrix_p[:node_len,:] = feat_matrix\n",
    "    \n",
    "    adj_p = torch.zeros([max_num, max_num]).fill_(pad_value)\n",
    "    adj_p[:node_len,:node_len] = adj\n",
    "    \n",
    "    labels_p = torch.zeros([max_num, 1]).fill_(pad_value)\n",
    "    labels_p[:node_len,:] = labels   \n",
    "    \n",
    "    return feat_matrix_p, adj_p, labels_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_matrix_p, adj_p, labels_p = pad_all(feat_matrix, adj, labels)\n",
    "feat_matrix_p = feat_matrix_p.to(DEVICE)\n",
    "adj_p = adj_p.to(DEVICE)\n",
    "labels_p = labels_p.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        # nfeat=dim, nhid=8\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features))) # (dim, 8)\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1))) #(2*8,1)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # features (B, N, dim) , adj (B, N, N)\n",
    "        h = torch.matmul(input, self.W) # (B,N,8)\n",
    "        N = h.shape[-2] # N\n",
    "        B = input.shape[0]\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1, 1, N).view(B, N * N, -1), h.repeat(1, N, 1)], dim=-1)\\\n",
    "                                        .view(-1, N, N, 2 * self.out_features) # (B, N, N, 16)\n",
    "\n",
    "        # 节点聚合!! 后两维(N, 16)* (16, 1)表示对节点i,计算N个节点对(i,j): 进行线性变换后产生一个标量. 对应原文的e_ij\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(-1)) # (B, N, N, 16) * (16, 1) --> (B, N, N)\n",
    "        # e没有normalizaze?\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e) # (B, N, N)\n",
    "        attention = torch.where(adj > 0, e, zero_vec) # 都是[B, N, N]\n",
    "        attention = F.softmax(attention, dim = -1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.bmm(attention, h)  # (B, N, N)*(B, N ,8)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime) # 一种激活函数\n",
    "        else:\n",
    "            return h_prime # [B, N, 8]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Dense version of GAT.\"\"\"\n",
    "        # nfeat=1433, nhid=8, nclass=7, dropout=0.6, alpha=0.3, nheads=8\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        # nhid * nheads = 8*8, nclass= 7 \n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False) # (2708,7)\n",
    "\n",
    "    def forward(self, feat_matrix, adj, ):\n",
    "        # features (B, N, dim) , adj (B, N, N)\n",
    "        feat_matrix = F.dropout(feat_matrix, self.dropout, training=self.training)\n",
    "        # att --> (N,8)\n",
    "        feat_matrix = torch.cat([att(feat_matrix, adj) for att in self.attentions], dim=-1) # (N,8*heads)\n",
    "        feat_matrix = F.dropout(feat_matrix, self.dropout, training=self.training)\n",
    "        feat_matrix = F.elu(self.out_att(feat_matrix, adj))\n",
    "\n",
    "        return F.log_softmax(feat_matrix, dim=1)\n",
    "    \n",
    "g_model = GAT(nfeat=768, nhid=8, nclass=2, dropout=0.6, alpha=0.3, nheads=8).to(DEVICE)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "g_model(feat_matrix_p, adj_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feat_matrix_p = torch.randn([3,300,768], device = DEVICE)\n",
    "test_adj = torch.randint(0,8,[3, 300,300], device = DEVICE)\n",
    "test_labels = torch.randint(0,2,[3, 300,1], device = DEVICE)\n",
    "test_sent_mask = torch.randint(0,2,[3, 300,1], device = DEVICE)\n",
    "test_para_mask = torch.randint(0,2,[3, 300,1], device = DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logits = g_model(test_feat_matrix_p, test_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 300, 2])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选择句子的loss\n",
    "r1 = (test_logits * test_sent_mask).view(-1,2)\n",
    "l = (test_labels * test_sent_mask).view(-1)\n",
    "loss = nn.CrossEntropyLoss()(r1,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([900])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6126, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## graph-level 输出\n",
    "\n",
    "1. 判断答案类型: `yes/no/span`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "228.475px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
