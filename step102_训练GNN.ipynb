{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Train GNN<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#HotpotQA_Dataset_New\" data-toc-modified-id=\"HotpotQA_Dataset_New-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>HotpotQA_Dataset_New</a></span></li><li><span><a href=\"#GNN\" data-toc-modified-id=\"GNN-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>GNN</a></span></li><li><span><a href=\"#训练\" data-toc-modified-id=\"训练-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>训练</a></span><ul class=\"toc-item\"><li><span><a href=\"#超参数\" data-toc-modified-id=\"超参数-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>超参数</a></span></li><li><span><a href=\"#辅助函数\" data-toc-modified-id=\"辅助函数-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>辅助函数</a></span></li><li><span><a href=\"#实例化\" data-toc-modified-id=\"实例化-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>实例化</a></span></li><li><span><a href=\"#开始训练\" data-toc-modified-id=\"开始训练-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>开始训练</a></span></li></ul></li><li><span><a href=\"#保存与加载\" data-toc-modified-id=\"保存与加载-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>保存与加载</a></span></li><li><span><a href=\"#阶段2\" data-toc-modified-id=\"阶段2-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>阶段2</a></span><ul class=\"toc-item\"><li><span><a href=\"#find-top-K-sents\" data-toc-modified-id=\"find-top-K-sents-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>find top K sents</a></span></li><li><span><a href=\"#测试find-span\" data-toc-modified-id=\"测试find-span-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>测试find span</a></span></li><li><span><a href=\"#计算span\" data-toc-modified-id=\"计算span-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>计算span</a></span></li></ul></li><li><span><a href=\"#top-K-sent-acc\" data-toc-modified-id=\"top-K-sent-acc-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>top K sent acc</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/folders/')\n",
    "    !pip install transformers\n",
    "    # !pip install -U spacy[cuda100]\n",
    "    # !wget -P /content/folders/My\\ Drive/download/ https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz\n",
    "    # !pip install /content/folders/My\\ Drive/download/en_core_web_lg-2.2.5.tar.gz\n",
    "    # !wget -P /content/folders/My\\ Drive/HotpotQA/ http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json\n",
    "    # json_train_path = '/content/folders/My Drive/HotpotQA/样例_hotpot_train_v1.1.json' # 5个例子\n",
    "    json_train_path = '/content/folders/My Drive/HotpotQA/hotpot_train_v1.1.json'\n",
    "    save_cache_path = '/content/folders/My Drive/save_cache/'\n",
    "    save_cache_path_linux = '/content/folders/My\\ Drive/save_cache/'\n",
    "    HotpotQA_path = '/content/folders/My Drive/HotpotQA'\n",
    "except:\n",
    "    json_train_path = r'./data/hotpot_train_v1.1.json'\n",
    "    HotpotQA_path = './'\n",
    "    save_cache_path = 'save_cache/'\n",
    "    use_proxy = False\n",
    "    proxies={\"http_proxy\": \"127.0.0.1:10809\",\n",
    "        \"https_proxy\": \"127.0.0.1:10809\"} if use_proxy else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from argparse import Namespace\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import ujson as json\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from apex import amp\n",
    "\n",
    "sys.path.insert(0,HotpotQA_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HotpotQA_Dataset_New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import HotpotQA_GNN_Dataset, gen_GNN_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GNN import GAT_HotpotQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "        \n",
    "args = Namespace(\n",
    "    # Data and path information\n",
    "    json_train_path=r'./data/hotpot_train_v1.1.json',\n",
    "    json_train_mini_path=r'./data/hotpot_train_mini.json',\n",
    "    model_state_file = \"GNN_HotpotQA.pt\",\n",
    "    save_dir = 'save_cache',\n",
    "    hotpotQA_item_folder = 'save_preprocess_new',\n",
    "    model_path = '/g/data/models/xlnet-large-cased',\n",
    "    use_proxy = False,\n",
    "    proxies={\"http_proxy\": \"127.0.0.1:10809\",\n",
    "             \"https_proxy\": \"127.0.0.1:10809\"},\n",
    "\n",
    "    # Dataset parameter\n",
    "    pad_max_num = 450,\n",
    "    pad_value = 0,\n",
    "\n",
    "    # Training hyper parameter\n",
    "    chunk_size = 500,\n",
    "    num_epochs=5,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=20,\n",
    "    seed=1337,\n",
    "    early_stopping_criteria=5,\n",
    "    flush_secs=60,\n",
    "    \n",
    "    # Runtime hyper parameter\n",
    "    cuda=True,\n",
    "    device=None,\n",
    "    tpu=False,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     3
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n",
      "total items: 50000\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "if not args.device:\n",
    "    args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.model_state_file = os.path.join(args.save_dir,args.model_state_file)\n",
    "#     args.HotpotQA_preprocess_file = os.path.join(args.save_dir,args.HotpotQA_preprocess_file)\n",
    "\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "handle_dirs(args.save_dir)\n",
    "\n",
    "print(\"Using: {}\".format(args.device))\n",
    "\n",
    "total_items = !ls -l $args.hotpotQA_item_folder |grep \"^-\"| wc -l\n",
    "total_items = int(total_items[0])\n",
    "\n",
    "print(f\"total items: {total_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [
     0,
     24,
     67,
     72
    ]
   },
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            \n",
    "            'train_running_loss': [],\n",
    "            'train_running_recall_sent': [],\n",
    "            'train_running_recall_para': [],\n",
    "            'train_running_acc_Qtype': [],\n",
    "            \n",
    "            'val_running_loss': [],\n",
    "            'val_running_recall_sent': [],\n",
    "            'val_running_recall_para': [],\n",
    "            'val_running_acc_Qtype': [],\n",
    "\n",
    "            'test_running_loss': [],\n",
    "            'test_running_recall_sent': [],\n",
    "            'test_running_recall_para': [],\n",
    "            'test_running_acc_Qtype': [],\n",
    "            \n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, optimizer, train_state):\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        checkpoint = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'amp': amp.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, train_state['model_filename'])\n",
    "        \n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_running_loss'][-2:]\n",
    "         \n",
    "        # If loss worsened\n",
    "        if loss_t >= loss_tm1:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                \n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'amp': amp.state_dict()\n",
    "                }\n",
    "                torch.save(checkpoint, train_state['model_filename'])\n",
    "\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(logits, labels):\n",
    "    _, logits_indices = logits.max(dim=1)\n",
    "    n_correct = torch.eq(logits_indices, labels).sum().item()\n",
    "    return float(n_correct) / len(logits_indices)\n",
    "\n",
    "def compute_recall(logits, labels, mask):\n",
    "    '''only count the positive recall'''\n",
    "    _, logits_indices = logits.max(dim=1)\n",
    "    all_positive_predicts = (logits_indices * labels * mask).sum().item() # only positive\n",
    "    all_positive_labels = (labels * mask).sum().item()\n",
    "    return float(all_positive_predicts) / all_positive_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.tensor([\n",
    "                [0.1, 0.99],\n",
    "                [0.6, 0.4],\n",
    "                [0.1, 0.91],\n",
    "                [0.6, 0.4],\n",
    "                [0.1, 0.92]\n",
    "            ])\n",
    "logits_B = torch.tensor([[\n",
    "                [0.1, 0.99],\n",
    "                [0.6, 0.4],\n",
    "                [0.1, 0.91],\n",
    "                [0.6, 0.4],\n",
    "                [0.1, 0.92]\n",
    "            ],\n",
    "            [\n",
    "                [1.1, 0.99],\n",
    "                [1.6, 0.4],\n",
    "                [0.1, 0.91],\n",
    "                [0.6, 0.4],\n",
    "                [1.1, 0.92]\n",
    "            ]])\n",
    "labels = torch.tensor([[1 ,0, 0, 1, 0],[1 ,0, 0, 1, 0]])\n",
    "m0ask = torch.tensor([[1 ,1, 0, 0, 1],[1 ,1, 0, 0, 1]])\n",
    "p0redict = torch.tensor([[1 ,1, 1, 1, 0],[1 ,1, 1, 1, 0]])\n",
    "\n",
    "index_test = torch.tensor([[0, 4, 1, 3],[1, 2, 3, 4]])\n",
    "compute_recall(logits, labels, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9900, 0.6000, 0.9100, 0.6000, 0.9200],\n",
       "        [1.1000, 1.6000, 0.9100, 0.6000, 1.1000]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value, indexs = logits_B.max(dim=-1)\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9900, 0.6000, 0.0000, 0.0000, 0.9200],\n",
       "        [1.1000, 1.6000, 0.0000, 0.0000, 1.1000]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value*mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 4, 1, 3],\n",
       "        [1, 4, 0, 3]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values2,indexs2 = (value*mask).topk(4, dim=-1)\n",
    "indexs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(predict, -1, indexs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select((labels*mask), -1, indexs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 0, 0],\n",
       "        [0, 1, 0, 0]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(logits_B.max(dim=-1)[1], -1, index_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value, max_index = logits_B.max(dim=-1) # max_index is predict class.\n",
    "topN_sent_index_batch = (max_value * mask).topk(4, dim=-1)[1]\n",
    "topN_sent_predict = torch.gather(max_index, -1, topN_sent_index_batch)\n",
    "topN_sent_label = torch.gather(labels*mask, -1, topN_sent_index_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 0, 0],\n",
       "        [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topN_sent_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0],\n",
       "        [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topN_sent_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "classifier = GAT_HotpotQA(features=768, hidden=32, nclass=2, dropout=0.2, alpha=0.3, nheads=8, nodes_num=args.pad_max_num)\n",
    "classifier = classifier.to(args.device)\n",
    "classifier.train()\n",
    "\n",
    "class_weights_sent, class_weights_para, class_weights_Qtype = \\\n",
    "            HotpotQA_GNN_Dataset.get_weights(device=args.device)\n",
    "loss_func_sent = nn.CrossEntropyLoss(class_weights_sent,ignore_index=-100)\n",
    "loss_func_para = nn.CrossEntropyLoss(class_weights_para,ignore_index=-100)\n",
    "loss_func_Qtype = nn.CrossEntropyLoss(class_weights_Qtype)\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, classifier.parameters()),\n",
    "                      lr=args.learning_rate)\n",
    "\n",
    "# Initialization\n",
    "opt_level = 'O1'\n",
    "classifier, optimizer = amp.initialize(classifier, optimizer, opt_level=opt_level)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\\\n",
    "                        mode='min', factor=0.5, patience=3)\n",
    "\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "dataset.set_split('train')\n",
    "batch_generator = gen_batches(dataset,\n",
    "                    batch_size=args.batch_size, \n",
    "                    device=args.device)\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    print(batch_dict)\n",
    "\n",
    "    logits_sent, logits_para, logits_Qtype = \\\n",
    "                    classifier(batch_dict['feature_matrix'],batch_dict['adj'])\n",
    "\n",
    "    logits_sent = (batch_dict['sent_mask'] * logits_sent).view(-1,2)\n",
    "    labels_sent = (batch_dict['sent_mask'] * batch_dict['labels']).view(-1)\n",
    "\n",
    "    logits_para = (batch_dict['para_mask'] * logits_para).view(-1,2)\n",
    "    labels_para = (batch_dict['para_mask'] * batch_dict['labels']).view(-1)\n",
    "\n",
    "    loss_sent = loss_func_sent(logits_sent, labels_sent) # [B,2] [B]\n",
    "    loss_para = loss_func_para(logits_para, labels_para) # [B,2] [B]\n",
    "    loss_Qtype = loss_func_Qtype(logits_Qtype, batch_dict['answer_type']) # [B,2] [B]\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     191
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3072544a854249ed864ecbf5969868f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='loading 0~500', max=500.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872431256bcb4df78d3e635cdf2bd8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training routine', max=5.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c2fcd7892e40b0b72db0c278706d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=train', max=17.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d2c421398f449c8afad5c3dcd61f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=val', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf39a4590ba4b7fa3b6088f7a7b9696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='loading 500~1000', max=500.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fbbcda30f143bbb183b07cc4cd5709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training routine', max=5.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5d9ab918014eacb775892b6a28ad4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=train', max=17.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a543634c1e412e84c236491baa17d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=val', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting loop\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    writer = SummaryWriter(flush_secs=60)\n",
    "    for chunk_i in range(0, total_items, args.chunk_size):\n",
    "        dataset = HotpotQA_GNN_Dataset.build_dataset(hotpotQA_item_folder = args.hotpotQA_item_folder,\n",
    "                                                     i_from = chunk_i, \n",
    "                                                     i_to = chunk_i+args.chunk_size)\n",
    "        dataset.set_parameters(args.pad_max_num, args.pad_value)\n",
    "\n",
    "        epoch_bar = tqdm(desc='training routine',\n",
    "                        total=args.num_epochs,\n",
    "                        position=0)\n",
    "\n",
    "        dataset.set_split('train')\n",
    "        train_bar = tqdm(desc='split=train',\n",
    "                        total=dataset.get_num_batches(args.batch_size), \n",
    "                        position=1)\n",
    "\n",
    "        dataset.set_split('val')\n",
    "        val_bar = tqdm(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size), \n",
    "                        position=1)\n",
    "\n",
    "\n",
    "        for epoch_index in range(args.num_epochs):\n",
    "\n",
    "            train_state['epoch_index'] = epoch_index\n",
    "\n",
    "            dataset.set_split('train')\n",
    "            batch_generator = gen_GNN_batches(dataset,\n",
    "                                            batch_size=args.batch_size, \n",
    "                                            device=args.device)\n",
    "            running_loss = 0.0\n",
    "\n",
    "            running_recall_sent = 0.0\n",
    "            running_recall_para = 0.0\n",
    "            running_acc_Qtype = 0.0\n",
    "\n",
    "            classifier.train()\n",
    "\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                logits_sent, logits_para, logits_Qtype = \\\n",
    "                                classifier(batch_dict['feature_matrix'], batch_dict['adj'])\n",
    "\n",
    "                logits_sent = (batch_dict['sent_mask'] * logits_sent).view(-1,2)\n",
    "\n",
    "                labels_sent = (batch_dict['sent_mask']*batch_dict['labels'] + \\\n",
    "                               batch_dict['sent_mask'].eq(0)*-100).view(-1)\n",
    "\n",
    "                logits_para = (batch_dict['para_mask'] * logits_para).view(-1,2)\n",
    "\n",
    "                labels_para = (batch_dict['para_mask']*batch_dict['labels'] + \\\n",
    "                               batch_dict['para_mask'].eq(0)*-100).view(-1)\n",
    "\n",
    "\n",
    "                loss_sent = loss_func_sent(logits_sent, labels_sent) # [B,2] [B]\n",
    "                loss_para = loss_func_para(logits_para, labels_para) # [B,2] [B]\n",
    "                loss_Qtype = loss_func_Qtype(logits_Qtype.view(-1,2),\n",
    "                                             batch_dict['answer_type'].view(-1)) # [B,2] [B]\n",
    "\n",
    "                loss = loss_sent + loss_para + loss_Qtype\n",
    "                running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                # compute the recall\n",
    "                recall_t_sent = compute_recall(logits_sent.view(-1,2), \n",
    "                                               batch_dict['labels'].view(-1), \n",
    "                                               batch_dict['sent_mask'].view(-1))\n",
    "                running_recall_sent += (recall_t_sent - running_recall_sent) / (batch_index + 1)\n",
    "\n",
    "                recall_t_para = compute_recall(logits_para.view(-1,2), \n",
    "                                               batch_dict['labels'].view(-1), \n",
    "                                               batch_dict['para_mask'].view(-1))\n",
    "                running_recall_para += (recall_t_para - running_recall_para) / (batch_index + 1)\n",
    "\n",
    "                acc_t_Qtype = compute_accuracy(logits_Qtype.view(-1,2), \n",
    "                                               batch_dict['answer_type'].view(-1))\n",
    "                running_acc_Qtype += (acc_t_Qtype - running_acc_Qtype) / (batch_index + 1)\n",
    "\n",
    "                # update bar\n",
    "                train_bar.set_postfix(loss=running_loss,\n",
    "                                      recall_sent=running_recall_sent,\n",
    "                                      recall_para=running_recall_para,\n",
    "                                      acc_Qtype = running_acc_Qtype,\n",
    "                                      epoch=epoch_index)\n",
    "                train_bar.update()\n",
    "                \n",
    "                writer.add_scalar('running_loss/train', running_loss, batch_index)\n",
    "                writer.add_scalar('running_recall_sent/train', running_recall_sent, batch_index)\n",
    "                writer.add_scalar('running_recall_para/train', running_recall_para, batch_index)\n",
    "                writer.add_scalar('running_acc_Qtype/train', running_acc_Qtype, batch_index)\n",
    "\n",
    "            train_state['train_running_loss'].append(running_loss)\n",
    "            train_state['train_running_recall_sent'].append(running_recall_sent)\n",
    "            train_state['train_running_recall_para'].append(running_recall_para)\n",
    "            train_state['train_running_acc_Qtype'].append(running_acc_Qtype)\n",
    "\n",
    "            # Iterate over val dataset\n",
    "\n",
    "            # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "\n",
    "            dataset.set_split('val')\n",
    "            batch_generator = gen_GNN_batches(dataset,\n",
    "                                            batch_size=args.batch_size, \n",
    "                                            device=args.device)\n",
    "            running_loss = 0.0\n",
    "\n",
    "            running_recall_sent = 0.0\n",
    "            running_recall_para = 0.0\n",
    "            running_acc_Qtype = 0.0\n",
    "            classifier.eval()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                # compute the output\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    logits_sent, logits_para, logits_Qtype = \\\n",
    "                                    classifier(batch_dict['feature_matrix'], batch_dict['adj'])\n",
    "\n",
    "                    logits_sent = (batch_dict['sent_mask'] * logits_sent).view(-1,2)\n",
    "\n",
    "                    labels_sent = (batch_dict['sent_mask']*batch_dict['labels'] + \\\n",
    "                                   batch_dict['sent_mask'].eq(0)*-100).view(-1)\n",
    "\n",
    "                    logits_para = (batch_dict['para_mask'] * logits_para).view(-1,2)\n",
    "\n",
    "                    labels_para = (batch_dict['para_mask']*batch_dict['labels'] + \\\n",
    "                                   batch_dict['para_mask'].eq(0)*-100).view(-1)\n",
    "\n",
    "\n",
    "                    loss_sent = loss_func_sent(logits_sent, labels_sent) # [B,2] [B]\n",
    "                    loss_para = loss_func_para(logits_para, labels_para) # [B,2] [B]\n",
    "                    loss_Qtype = loss_func_Qtype(logits_Qtype.view(-1,2),\n",
    "                                                 batch_dict['answer_type'].view(-1)) # [B,2] [B]\n",
    "\n",
    "                    loss = loss_sent + loss_para + loss_Qtype\n",
    "                    running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "                    # compute the recall\n",
    "                    recall_t_sent = compute_recall(logits_sent.view(-1,2), \n",
    "                                                   batch_dict['labels'].view(-1), \n",
    "                                                   batch_dict['sent_mask'].view(-1))\n",
    "                    running_recall_sent += (recall_t_sent - running_recall_sent) / (batch_index + 1)\n",
    "\n",
    "                    recall_t_para = compute_recall(logits_para.view(-1,2), \n",
    "                                                   batch_dict['labels'].view(-1), \n",
    "                                                   batch_dict['para_mask'].view(-1))\n",
    "                    running_recall_para += (recall_t_para - running_recall_para) / (batch_index + 1)\n",
    "\n",
    "                    acc_t_Qtype = compute_accuracy(logits_Qtype.view(-1,2), \n",
    "                                                   batch_dict['answer_type'].view(-1))\n",
    "                    running_acc_Qtype += (acc_t_Qtype - running_acc_Qtype) / (batch_index + 1)\n",
    "\n",
    "\n",
    "                val_bar.set_postfix(loss=running_loss,\n",
    "                                      recall_sent=running_recall_sent,\n",
    "                                      recall_para=running_recall_para,\n",
    "                                      acc_Qtype = running_acc_Qtype,\n",
    "                                      epoch=epoch_index)\n",
    "                val_bar.update()\n",
    "                                \n",
    "                writer.add_scalar('running_loss/val', running_loss, epoch_index)\n",
    "                writer.add_scalar('running_recall_sent/val', running_recall_sent, epoch_index)\n",
    "                writer.add_scalar('running_recall_para/val', running_recall_para, epoch_index)\n",
    "                writer.add_scalar('running_acc_Qtype/val', running_acc_Qtype, epoch_index)\n",
    "\n",
    "            train_state['val_running_loss'].append(running_loss)\n",
    "            train_state['val_running_recall_sent'].append(running_recall_sent)\n",
    "            train_state['val_running_recall_para'].append(running_recall_para)\n",
    "            train_state['val_running_acc_Qtype'].append(running_acc_Qtype)\n",
    "\n",
    "            train_state = update_train_state(args=args, model=classifier, \n",
    "                                             optimizer = optimizer,\n",
    "                                             train_state=train_state)\n",
    "\n",
    "            scheduler.step(train_state['val_running_loss'][-1])\n",
    "\n",
    "            train_bar.n = 0\n",
    "            val_bar.n = 0\n",
    "            epoch_bar.update()\n",
    "\n",
    "            if train_state['stop_early']:\n",
    "                break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存与加载\n",
    "\n",
    "上文已经自动保存模型.\n",
    "\n",
    "```python\n",
    "# Save checkpoint\n",
    "checkpoint = {\n",
    "    'model': classifier.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'amp': amp.state_dict()\n",
    "}\n",
    "torch.save(checkpoint, 'GNN_HotpotQA_amp.pt')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from traceback import print_exc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7d84a8d5e03e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuestionNode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParagraphTitleNode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSentenceNode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEntityNode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdjacency_sp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_reload_Node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m# from class_simple import *\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'classes'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from argparse import Namespace\n",
    "import os\n",
    "import ujson as json\n",
    "import time\n",
    "\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "from classes import QuestionNode, ParagraphTitleNode, SentenceNode, EntityNode, Adjacency_sp, auto_reload_Node\n",
    "# from class_simple import *\n",
    "\n",
    "from apex import amp\n",
    "\n",
    "import sys\n",
    "\n",
    "HotpotQA_path = './'\n",
    "sys.path.insert(0,HotpotQA_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Restore\n",
    "classifier = GAT_HotpotQA(features=768, hidden=32, nclass=2, dropout=0.2, alpha=0.3, nheads=8, nodes_num=450)\n",
    "classifier.to('cuda')\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, classifier.parameters()),\n",
    "                      lr=1e-3)\n",
    "checkpoint = torch.load('save_cache/GNN_HotpotQA.pt')\n",
    "\n",
    "opt_level = 'O1'\n",
    "classifier, optimizer = amp.initialize(classifier, optimizer, opt_level=opt_level)\n",
    "\n",
    "classifier.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "amp.load_state_dict(checkpoint['amp'])\n",
    "\n",
    "# Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 阶段2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find top K sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61cbfc9999b4d8c9512f2972c9b328b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='loading 0~10', max=10.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = HotpotQA_Dataset_New.build_dataset(i_from = 0,i_to = 10)\n",
    "dataset.set_parameters(450, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for batch_dict in gen_batches(dataset,\n",
    "                                batch_size=7, \n",
    "                                device='cuda'):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['feature_matrix', 'adj', 'sent_mask', 'para_mask', 'labels', 'answer_type', 'ans_yes_no', 'ques_tokens', 'answer_tokens', 'sent_tokens'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_dict.keys()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "logits_sent, logits_para, logits_Qtype = \\\n",
    "                                classifier(batch_dict['feature_matrix'], batch_dict['adj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def find_ans_spans(target, tokens, offets_type = 'position', top_num = None):\n",
    "    assert offets_type in ['position', 'range'] and \\\n",
    "        type(target) == type(tokens) == list\n",
    "    len_x1 = len(target)\n",
    "    len_x2 = len(tokens)\n",
    "    if len_x1 == 0 or len_x2 == 0 or len_x1 > len_x2:\n",
    "        return [0,0]\n",
    "    \n",
    "    i1=0\n",
    "    i2=0\n",
    "    i2_current = 0\n",
    "    spans = []\n",
    "    while i2 <= len_x2 - len_x1:\n",
    "        if top_num and len(spans) == top_num:\n",
    "            break\n",
    "        i2_current = i2\n",
    "        while i1 < len_x1:\n",
    "            if target[i1] != tokens[i2]: \n",
    "                i1 = 0\n",
    "                i2 = i2_current + 1\n",
    "                break\n",
    "            else:\n",
    "                i1 += 1\n",
    "                i2 += 1\n",
    "                \n",
    "        if not i1 < len_x1:\n",
    "            i1 = 0\n",
    "            if offets_type == 'position':  \n",
    "                spans.append([i2_current, i2_current+len_x1-1])\n",
    "            else:\n",
    "                spans.append([i2_current, i2_current+len_x1])\n",
    "                \n",
    "    return spans[:top_num+1] if top_num else spans[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def convert_batch_to_LM(classifier, batch_dict, topN = 4,\n",
    "                        sep_token = '[SEP]', cls_token = '[CLS]'):\n",
    "    '''convert batch from GNN to LM model(QA fine-tuning)'''\n",
    "    batch_dict_for_LM = []\n",
    "    logits_sent, _, __ = classifier(batch_dict['feature_matrix'], batch_dict['adj'])\n",
    "    topN_sents = []\n",
    "    for batch_index in range(batch_dict['sent_mask'].shape[0]):\n",
    "        sent_index = torch.nonzero(batch_dict['sent_mask'][batch_index].view(-1,).long()).flatten()\n",
    "        sent_logits = torch.index_select(logits_sent[batch_index], 0, sent_index)\n",
    "        sent_logits_soft = F.softmax(sent_logits, dim = -1).cpu()\n",
    "        value, _index = torch.max(sent_logits_soft, dim=-1)\n",
    "        sent_scores = value.tolist()\n",
    "        sent_score_index = [(score, index) for score, index in zip(sent_scores, range(len(sent_scores)))]\n",
    "        sent_score_index_sort = sorted(sent_score_index,key=lambda x:x[0], reverse=True)\n",
    "        sent_indexes = [x[-1] for x in sent_score_index_sort[:topN]]\n",
    "        topN_sents.append(sent_indexes)\n",
    "        \n",
    "    batch_sent_tokens = []\n",
    "    for batch_index, sents in enumerate(topN_sents):\n",
    "        temp = []\n",
    "        for i_sent in sents: temp.extend(batch_dict['sent_tokens'][batch_index][i_sent])\n",
    "        batch_sent_tokens.append(temp)\n",
    "\n",
    "    batch_ques_sent_tokens = [[sep_token] + batch_dict['ques_tokens'] + sep_token + \\\n",
    "                              sent_tokens + cls_token for sent_tokens in batch_sent_tokens]\n",
    "    \n",
    "\n",
    "topN_sents_one_batch(batch_dict,logits_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "sent_index = torch.nonzero(batch_dict['sent_mask'][0].view(-1,).long()).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2,  10,  14,  22,  31,  33,  36,  43,  49,  54,  58,  66,  72,  74,\n",
       "         77,  79,  81,  85,  89,  93,  97, 101, 105, 106, 108, 112, 117, 121,\n",
       "        123, 127, 130, 134, 138, 143, 148, 151, 157])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_index.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([37])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_dict['sent_tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([37, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_logits = torch.index_select(logits_sent[0], 0, sent_index)\n",
    "sent_logits_soft = F.softmax(sent_logits, dim = -1).cpu()\n",
    "sent_logits_soft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value, _index = torch.max(sent_logits_soft, dim=-1)\n",
    "sent_scores = value.cpu().tolist()\n",
    "len(sent_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "sent_score_index = [(score, index) for score, index in zip(sent_scores, range(len(sent_scores)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 12, 16]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_score_index_sort = sorted(sent_score_index,key=lambda x:x[0],reverse=True)\n",
    "sent_indexes = [x[-1] for x in sent_score_index_sort[:4]]\n",
    "sent_indexes\n",
    "# 获取batch中一个元素的前4句."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## 测试find span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁It', '▁was', '▁founded', '▁in', '▁Pine', '▁Bluff', ',', '▁Arkansas', '▁in', '▁1986', '.', '▁Cardinal', '▁Health', '▁also', '▁owns', '▁the', '▁franchise', '▁chain', '▁The', '▁Medicine', '▁Shop', 'pe', '.', '▁Leader', '▁Drug', '▁Store', 's', '▁is', '▁a', '▁network', '▁of', '▁over', '▁3', ',', '100', '▁independently', '▁owned', '▁and', '▁operated', '▁pharmacies', '.', '▁Gray', '▁Drug', '▁was', '▁a', '▁drug', 'store', '▁chain', '▁in', '▁Cleveland', ',', '▁Ohio', '.']\n"
     ]
    }
   ],
   "source": [
    "sent_tokens = batch_dict['sent_tokens']\n",
    "\n",
    "sent_final_tokens = []\n",
    "for i in sent_indexes: sent_final_tokens.extend(sent_tokens[0][i])\n",
    "print(sent_final_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "answer_tokens_test =  ['▁network', '▁of', '▁over']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_tokens_test in sent_final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁network', '▁of', '▁over']\n",
      "['▁network', '▁of']\n"
     ]
    }
   ],
   "source": [
    "print(answer_tokens_test)\n",
    "print(sent_final_tokens[29:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def find_ans_spans(target, tokens, offets_type = 'position', top_num = None):\n",
    "    assert offets_type in ['position', 'range'] and \\\n",
    "        type(target) == type(tokens) == list\n",
    "    len_x1 = len(target)\n",
    "    len_x2 = len(tokens)\n",
    "    if len_x1 == 0 or len_x2 == 0 or len_x1 > len_x2:\n",
    "        return [0,0]\n",
    "    \n",
    "    i1=0\n",
    "    i2=0\n",
    "    i2_current = 0\n",
    "    spans = []\n",
    "    while i2 <= len_x2 - len_x1:\n",
    "        if top_num and len(spans) == top_num:\n",
    "            break\n",
    "        i2_current = i2\n",
    "        while i1 < len_x1:\n",
    "            if target[i1] != tokens[i2]: \n",
    "                i1 = 0\n",
    "                i2 = i2_current + 1\n",
    "                break\n",
    "            else:\n",
    "                i1 += 1\n",
    "                i2 += 1\n",
    "                \n",
    "        if not i1 < len_x1:\n",
    "            i1 = 0\n",
    "            if offets_type == 'position':  \n",
    "                spans.append([i2_current, i2_current+len_x1-1])\n",
    "            else:\n",
    "                spans.append([i2_current, i2_current+len_x1])\n",
    "                \n",
    "    return spans[:top_num+1] if top_num else spans[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[29, 31]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_ans_spans(answer_tokens_test, sent_final_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## 计算span\n",
    "\n",
    "将`ques_feature`和`content_features`加上`[SEP] [CLS]`拼接在一起."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "answer_tokens_test2 = [ 'Australian', 'GT', 'Championship',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "sent_tokens_combine = []\n",
    "for s_index in sent_indexes: sent_tokens_combine.extend(sent_tokens[0][s_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tokens = ['<x1>','[ques]','<x2>'] + sent_tokens_combine + ['<cls>']\n",
    "ans_spans = find_ans_spans(answer_tokens_test2, final_tokens, top_num = 2)\n",
    "ans_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top K sent acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([[[0.2, 0.8],\n",
    "                      [0.3,0.7],\n",
    "                      [0.9,0.1]],\n",
    "                      [[0.2, 0.8],\n",
    "                      [0.6,0.4],\n",
    "                      [0.99,0.01]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "value, indexs = logits.max(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "values2,indexs2 = value[0].topk(2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = torch.tensor([[0,1,1],[1,0,0]])\n",
    "torch.index_select(predict[0], 0, indexs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Train GNN",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "218.275px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
